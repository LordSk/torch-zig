// THIS FILE IS AUTOMATICALLY GENERATED, DO NOT EDIT BY HAND! ";
const __c = @cImport({
    @cInclude("stddef.h");
    @cInclude("stdbool.h");
    @cInclude("torch_api.h");
    @cInclude("torch_api_generated.h");
});
const std = @import("std");
const torch = @import("torch.zig");
const TchError = torch.TchError;
const TensorOptions = torch.TensorOptions;
const Device = torch.Device;
const Kind = torch.Kind;
const Scalar = torch.Scalar;
const Layout = torch.Layout;
const C_tensor = __c.tensor;
const _global_allocator = torch.global_allocator;

fn ptrList(l: []?Tensor) []*C_tensor {
    _ = l;
    // l.iter().map(|x| x.as_ref().map_or(std::ptr::null_mut(), |x| \
    // x.borrow().c_tensor)).collect()";
}
fn ptrListOpt(l: []Tensor) []*C_tensor {
    _ = l;
}

pub const Tensor = struct {
    c_tensor: C_tensor,
    pub fn new() Tensor {
        const ret = __c.at_new_tensor();
        torch.readAndCleanError();
        return Tensor{ .c_tensor = ret };
    }

    pub fn fromPtr(c_tensor: C_tensor) Tensor {
        return Tensor{ .c_tensor = c_tensor };
    }

    pub fn cloneFromPtr(c_tensor: C_tensor) Tensor {
        const tensor = __c.at_shallow_clone(c_tensor);
        torch.readAndCleanError();
        return Tensor{ .c_tensor = tensor };
    }

    pub fn asPtr(self: Tensor) C_tensor {
        return self.c_tensor;
    }

    pub fn dim(self: *Tensor) usize {
        const ret = __c.at_dim(self.c_tensor);
        torch.readAndCleanError();
        return ret;
    }

    pub fn size(self: *Tensor) ![]i64 {
        const dim_ = __c.at_dim(self.c_tensor);
        torch.readAndCleanError();
        var sz = std.ArrayList(i64).init(_global_allocator);
        try sz.resize(dim_);
        __c.at_shape(self.c_tensor, sz.items);
        torch.readAndCleanError();
        return sz.toOwnedSlice();
    }

    pub fn sizeDims(comptime dims: usize, self: *Tensor) ![dims]i64 {
        const size_ = self.size();
        if (size_.len != dims) {
            std.log.err("expected {} dims, got {}", .{ dims, size_.len });
            return error.UnexpectedDimension;
        }
        return size_[0..dims];
    }

    pub fn stride(self: *Tensor) ![]i64 {
        const dim_ = self.dim();
        var sz = std.ArrayList(i64).init(_global_allocator);
        try sz.resize(dim_);
        __c.at_stride(self.c_tensor, sz.items);
        torch.readAndCleanError();
        return sz.toOwnedSlice();
    }

    pub fn strideDims(comptime dims: usize, self: *Tensor) ![dims]i64 {
        const stride_ = self.stride();
        if (stride_.len != dims) {
            std.log.err("expected one dim, got {}", .{stride_.len});
            return error.UnexpectedDimension;
        }
        return stride_[0..dims];
    }

    pub fn kind(self: *Tensor) Kind {
        const kind_ = __c.at_scalar_type(self.c_tensor);
        torch.readAndCleanError();
        return Kind.fromCInt(kind_);
    }

    pub fn device(self: *Tensor) Device {
        const device_ = __c.at_device(self.c_tensor);
        torch.readAndCleanError();
        return Device.fromCInt(device_);
    }

    pub fn print(self: *Tensor) void {
        __c.at_print(self.c_tensor);
        torch.readAndCleanError();
    }

    pub fn doubleValue(self: *Tensor, idx: []i64) f64 {
        const ret = __c.at_double_value_at_indexes(self.c_tensor, idx.items, idx.len);
        torch.readAndCleanError();
        return ret;
    }

    pub fn int64Value(self: *Tensor, idx: []i64) i64 {
        const ret = __c.at_int64_value_at_indexes(self.c_tensor, idx.items, idx.len);
        torch.readAndCleanError();
        return ret;
    }

    pub fn requiresGrad(self: *Tensor) bool {
        const ret = __c.at_requires_grad(self.c_tensor);
        torch.readAndCleanError();
        return ret;
    }

    // pub fn dataPtr(self: *Tensor) *c_void {
    //     const ret = __c.at_data_ptr(self.c_tensor);
    //     torch.readAndCleanError();
    //     return ret;
    // }

    pub fn defined(self: *Tensor) bool {
        const ret = __c.at_defined(self.c_tensor);
        torch.readAndCleanError();
        return ret;
    }

    pub fn isMkldnn(self: *Tensor) bool {
        const ret = __c.at_is_mkldnn(self.c_tensor);
        torch.readAndCleanError();
        return ret;
    }

    pub fn isSparse(self: *Tensor) bool {
        const ret = __c.at_is_sparse(self.c_tensor);
        torch.readAndCleanError();
        return ret;
    }

    pub fn isContiguous(self: *Tensor) bool {
        const ret = __c.at_is_contiguous(self.c_tensor);
        torch.readAndCleanError();
        return ret;
    }

    pub fn zeroGrad(self: *Tensor) void {
        var grad_ = self.grad();
        if (grad_.defined()) {
            _ = grad_.detach().zero();
        }
    }

    pub fn free(self: *Tensor) void {
        __c.at_free(self.c_tensor);
        torch.readAndCleanError();
    }
};
