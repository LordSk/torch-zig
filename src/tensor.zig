// THIS FILE IS AUTOMATICALLY GENERATED, DO NOT EDIT BY HAND! ";
const __c = @cImport({
    @cInclude("stddef.h");
    @cInclude("stdbool.h");
    @cInclude("torch_api.h");
    @cInclude("torch_api_generated.h");
});
const std = @import("std");
const torch = @import("torch.zig");
const TchError = torch.TchError;
const TensorOptions = torch.TensorOptions;
const Device = torch.Device;
const Kind = torch.Kind;
const Scalar = torch.Scalar;
const Layout = torch.Layout;
const C_tensor = __c.tensor;

pub const Reduction = enum {
    None,
    Mean,
    Sum,

    pub fn toInt(self: Reduction) i64 {
        return switch (self) {
            .None => 0,
            .Mean => 1,
            .Sum => 2,
        };
    }
};

fn ptrList(l: []?Tensor) []*C_tensor {
    var ret = std.ArrayList(*C_tensor).init(torch.global_allocator);
    for (l) |x| {
        ret.append(x.c_tensor);
    }
    return ret.toOwnedSlice();
}
fn ptrListOpt(l: []Tensor) []*C_tensor {
    return ptrList(l);
}

pub const TensorIndexer = union(enum) {
    Select: i64,
    Narrow: struct {
        start: ?i64,
        end: ?i64,
    },
    IndexSelect: *const Tensor,
    InsertNewAxis: void,
};

pub const NewAxis = struct {};

pub const Tensor = struct {
    c_tensor: C_tensor,
    pub fn new() Tensor {
        const ret = __c.at_new_tensor();
        torch.readAndCleanError();
        return Tensor{ .c_tensor = ret };
    }
    // TODO: implement this for formatted printing, for now we can just use the default print
    // pub fn format(self: Tensor, comptime fmt: []const u8, options: std.fmt.FormatOptions, writer: anytype,) !void {
    // }

    pub fn i(self: *const Tensor, index_spec: anytype) Tensor {
        // TODO: add case for just a single int index
        var specs = std.ArrayList(TensorIndexer).init(torch.global_allocator);
        defer specs.deinit();
        inline for (index_spec) |spec| {
            const spec_ = switch (@TypeOf(spec)) {
                NewAxis => TensorIndexer.InsertNewAxis,
                i64, comptime_int => TensorIndexer{ .Select = spec },
                struct { start: ?i64, end: ?i64 } => TensorIndexer{ .Narrow = spec },
                []i64 => blk_i64: {
                    const index_tensor = Tensor.fromSlice(i64, spec);
                    break :blk_i64 TensorIndexer{ .IndexSelect = index_tensor };
                },
                Tensor => TensorIndexer{ .IndexSelect = spec.shallowClone() },
                else => {
                    std.log.err("unsupported index spec type: {any}", .{@TypeOf(spec)});
                    unreachable;
                },
            };
            specs.append(spec_) catch unreachable;
        }
        return self.indexer(specs.items);
    }

    fn indexer(self: *const Tensor, index_spec: []TensorIndexer) Tensor {
        var n_newaxis: usize = 0;
        for (index_spec) |spec| {
            if (spec == TensorIndexer.InsertNewAxis) {
                n_newaxis += 1;
            }
        }
        const dim_ = self.dim();
        if (index_spec.len > dim_ + n_newaxis) {
            std.log.err("too many indices for tensor of dimension {d}", .{dim_});
            unreachable;
        }

        for (index_spec) |spec| {
            switch (spec) {
                .IndexSelect => |tensor| {
                    if (dim_ != 1) {
                        std.log.err("expected 1-d tensor, got {}", .{dim_});
                        unreachable;
                    }

                    switch (tensor.kind()) {
                        .Int64, .Int16, .Int8, .Int => {},
                        else => {
                            std.log.err("expected int tensor for indices, got {}", .{tensor.kind()});
                            unreachable;
                        },
                    }
                },
                else => {},
            }
        }

        var curr_tensor = self.shallowClone();
        var curr_idx: usize = 0;

        for (index_spec) |spec| {
            switch (spec) {
                TensorIndexer.InsertNewAxis => {
                    curr_tensor = curr_tensor.unsqueeze(@intCast(curr_idx));
                    curr_idx += 1;
                },
                TensorIndexer.Select => |idx| {
                    curr_tensor = curr_tensor.select(@intCast(curr_idx), @intCast(idx));
                },
                TensorIndexer.Narrow => |narrow_| {
                    const size_ = self.size();
                    const start = narrow_.start orelse 0;
                    const end = narrow_.end orelse size_[curr_idx];
                    if (start < 0 or end < start or end > size_[curr_idx]) {
                        std.log.err("invalid start/end for narrow: start={}, end={}, shape={}", .{ start, end, size_[curr_idx] });
                        unreachable;
                    }
                    const length = end - start;
                    curr_tensor = curr_tensor.narrow(@intCast(curr_idx), start, length);
                    curr_idx += 1;
                },
                TensorIndexer.IndexSelect => |index_tensor| {
                    curr_tensor = curr_tensor.indexSelect(@intCast(curr_idx), index_tensor);
                    curr_idx += 1;
                },
            }
        }

        return curr_tensor;
    }

    pub fn options(self: *const Tensor) TensorOptions {
        return TensorOptions{ .kind = self.kind(), .device = self.device() };
    }

    pub fn fromPtr(c_tensor: C_tensor) Tensor {
        return Tensor{ .c_tensor = c_tensor };
    }

    pub fn cloneFromPtr(c_tensor: C_tensor) Tensor {
        const tensor = __c.at_shallow_clone(c_tensor);
        torch.readAndCleanError();
        return Tensor{ .c_tensor = tensor };
    }

    pub fn asPtr(self: *const Tensor) C_tensor {
        return self.c_tensor;
    }

    pub fn dim(self: *const Tensor) usize {
        const ret = __c.at_dim(self.c_tensor);
        torch.readAndCleanError();
        return ret;
    }

    pub fn size(self: *const Tensor) []i64 {
        const dim_ = self.dim();
        var buffer: [10]i64 = undefined;
        __c.at_shape(self.c_tensor, buffer[0..dim_].ptr);
        torch.readAndCleanError();
        return buffer[0..dim_];
    }

    pub fn sizeDims(self: *const Tensor, comptime dims: usize) ![dims]i64 {
        const size_ = self.size();
        if (size_.len != dims) {
            std.log.err("expected {} dims, got {}", .{ dims, size_.len });
            return error.UnexpectedDimension;
        }
        return size_[0..dims];
    }

    pub fn stride(self: *const Tensor) ![]i64 {
        const dim_ = self.dim();
        var sz = std.ArrayList(i64).init(torch.global_allocator);
        try sz.resize(dim_);
        __c.at_stride(self.c_tensor, sz.items);
        torch.readAndCleanError();
        return sz.toOwnedSlice();
    }

    pub fn strideDims(self: *const Tensor, comptime dims: usize) ![dims]i64 {
        const stride_ = self.stride();
        if (stride_.len != dims) {
            std.log.err("expected one dim, got {}", .{stride_.len});
            return error.UnexpectedDimension;
        }
        return stride_[0..dims];
    }

    pub fn kind(self: *const Tensor) Kind {
        const kind_ = __c.at_scalar_type(self.c_tensor);
        torch.readAndCleanError();
        return Kind.fromCInt(kind_);
    }

    pub fn device(self: *const Tensor) Device {
        const device_ = __c.at_device(self.c_tensor);
        torch.readAndCleanError();
        return Device.fromCInt(device_);
    }

    pub fn print(self: *const Tensor) void {
        __c.at_print(self.c_tensor);
        torch.readAndCleanError();
    }

    pub fn doubleValue(self: *const Tensor, idx: []i64) f64 {
        const ret = __c.at_double_value_at_indexes(self.c_tensor, idx.items, idx.len);
        torch.readAndCleanError();
        return ret;
    }

    pub fn int64Value(self: *const Tensor, idx: []i64) i64 {
        const ret = __c.at_int64_value_at_indexes(self.c_tensor, idx.items, idx.len);
        torch.readAndCleanError();
        return ret;
    }

    pub fn requiresGrad(self: *const Tensor) bool {
        const ret = __c.at_requires_grad(self.c_tensor);
        torch.readAndCleanError();
        return ret;
    }

    // pub fn dataPtr(self: *Tensor) *c_void {
    //     const ret = __c.at_data_ptr(self.c_tensor);
    //     torch.readAndCleanError();
    //     return ret;
    // }

    pub fn defined(self: *const Tensor) bool {
        const ret = __c.at_defined(self.c_tensor);
        torch.readAndCleanError();
        return if (ret != 0) true else false;
    }

    pub fn isMkldnn(self: *const Tensor) bool {
        const ret = __c.at_is_mkldnn(self.c_tensor);
        torch.readAndCleanError();
        return ret;
    }

    pub fn isSparse(self: *const Tensor) bool {
        const ret = __c.at_is_sparse(self.c_tensor);
        torch.readAndCleanError();
        return ret;
    }

    pub fn isContiguous(self: *const Tensor) bool {
        const ret = __c.at_is_contiguous(self.c_tensor);
        torch.readAndCleanError();
        return ret;
    }

    pub fn zeroGrad(self: *Tensor) void {
        var grad_ = self.grad();
        if (grad_.defined()) {
            _ = grad_.detach().zero();
        }
    }

    pub fn backward(self: *Tensor) void {
        __c.at_backward(self.c_tensor, 0, 0);
        torch.readAndCleanError();
    }

    pub fn runBackward(tensors: []Tensor, inputs: []Tensor, keep_graph: bool, create_graph: bool) !std.ArrayList(Tensor) {
        var outputs = std.ArrayList(C_tensor).init(torch.global_allocator);
        defer outputs.deinit();
        try outputs.resize(inputs.len);
        var inputs_ = ptrList(inputs);
        var tensors_ = ptrList(tensors);

        __c.at_run_backward(&tensors_, @intCast(tensors_.len), &inputs_, @intCast(inputs_.len), outputs.items, keep_graph, create_graph);
        torch.readAndCleanError();
        var res = std.ArrayList(Tensor).init(torch.global_allocator);
        for (outputs.items) |output| {
            res.append(Tensor{ .c_tensor = output });
        }
        return res;
    }

    pub fn copyDataU8(self: *Tensor, dst: []u8, numel_: usize) !void {
        const elt_size_in_bytes = self.kind().eltSizeInBytes();
        if (dst.len < numel_ * elt_size_in_bytes) {
            std.log.err("expected buffer of size {}, got {}", .{ numel_ * elt_size_in_bytes, dst.len });
            return error.BufferTooSmall;
        }
        __c.at_copy_data(self.c_tensor, dst.ptr, numel_, elt_size_in_bytes);
        torch.readAndCleanError();
    }

    pub fn internalAmpNonFiniteCheckAndUnscale(self: *Tensor, found_inf: *Tensor, inv_scale: *const Tensor) void {
        __c.at__amp_non_finite_check_and_unscale(self.c_tensor, found_inf.c_tensor, inv_scale.c_tensor);
        torch.readAndCleanError();
    }

    pub fn copyData(self: *const Tensor, dst: []*Tensor, numel_: usize) !void {
        // TODO: Fix this function
        if (self.kind() != dst.kind()) {
            std.log.err("expected same kind, got {any} and {any}", .{ self.kind(), dst.kind() });
            return error.UnexpectedKind;
        }

        if (dst.len < numel_) {
            std.log.err("expected buffer of size {}, got {}", .{ numel_, dst.len });
            return error.BufferTooSmall;
        }

        __c.at_copy_data(self.c_tensor, dst.c_tensor, numel_);
        torch.readAndCleanError();
    }

    pub fn numel(self: *const Tensor) usize {
        const size_ = self.size();
        var ret: usize = 1;
        for (size_) |s| {
            ret *= @intCast(s);
        }
        return ret;
    }

    pub fn fromSlice(comptime T: type, data_: []T) Tensor {
        var size_ = [_]i64{@intCast(data_.len)};
        const kind_ = torch.elementKind(T);
        const c_tensor = __c.at_tensor_of_data(data_.ptr, &size_, 1, kind_.eltSizeInBytes(), kind_.cInt());
        torch.readAndCleanError();
        return Tensor{ .c_tensor = c_tensor };
    }

    pub fn free(self: *Tensor) void {
        __c.at_free(self.c_tensor);
        torch.readAndCleanError();
    }

    // TODO: finish rest of the functions

    pub fn shallowClone(self: *const Tensor) Tensor {
        const c_tensor = __c.at_shallow_clone(self.c_tensor);
        torch.readAndCleanError();
        return Tensor{ .c_tensor = c_tensor };
    }

    pub fn get(self: *const Tensor, idx: i64) Tensor {
        const c_tensor = __c.at_get(self.c_tensor, idx);
        torch.readAndCleanError();
        return Tensor{ .c_tensor = c_tensor };
    }

    pub fn copy(self: *const Tensor, src: *const Tensor) void {
        _ = __c.at_copy_(self.c_tensor, src.c_tensor);
        torch.readAndCleanError();
    }

    pub fn load(path: []const u8) Tensor {
        const c_path = torch.global_allocator.dupeZ(path);
        defer torch.global_allocator.free(c_path);
        const c_tensor = __c.at_load(c_path);
        torch.readAndCleanError();
        return Tensor{ .c_tensor = c_tensor };
    }

    pub fn save(self: *const Tensor, path: []const u8) void {
        const c_path = torch.global_allocator.dupeZ(path);
        defer torch.global_allocator.free(c_path);
        __c.at_save(self.c_tensor, c_path);
        torch.readAndCleanError();
    }

    pub fn toString(self: *const Tensor, lw: i64) []const u8 {
        const s = __c.at_to_string(self.c_tensor, @intCast(lw));
        torch.readAndCleanError();
        return s;
    }

    // Generated code starts here

    pub fn internalAnd_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg___and__(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalAndTensor_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg___and__tensor_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalIand_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg___iand__(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalIandTensor_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg___iand__tensor_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalIlshift_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg___ilshift__(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalIlshiftTensor_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg___ilshift__tensor_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalIor_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg___ior__(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalIorTensor_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg___ior__tensor_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalIrshift_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg___irshift__(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalIrshiftTensor_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg___irshift__tensor_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalIxor_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg___ixor__(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalIxorTensor_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg___ixor__tensor_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalLshift_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg___lshift__(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalLshiftScalarOut_(
        self: *const Tensor, out: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg___lshift__scalar_out_(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalLshiftTensor_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg___lshift__tensor_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalLshiftTensorOut_(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg___lshift__tensor_out_(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalOr_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg___or__(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalOrTensor_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg___or__tensor_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalRshift_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg___rshift__(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalRshiftScalarOut_(
        self: *const Tensor, out: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg___rshift__scalar_out_(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalRshiftTensor_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg___rshift__tensor_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalRshiftTensorOut_(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg___rshift__tensor_out_(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalXor_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg___xor__(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalXorTensor_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg___xor__tensor_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalAdaptiveAvgPool2d(
        self: *const Tensor, output_size: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__adaptive_avg_pool2d(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalAdaptiveAvgPool2dBackward(
        self: *const Tensor, grad_output: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__adaptive_avg_pool2d_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalAdaptiveAvgPool2dBackwardOut(
        self: *const Tensor, out: *const Tensor, grad_output: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__adaptive_avg_pool2d_backward_out(@ptrCast(&c_tensors), out.c_tensor,
                grad_output.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalAdaptiveAvgPool2dOut(
        self: *const Tensor, out: *const Tensor, output_size: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__adaptive_avg_pool2d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                output_size.ptr, @intCast(output_size.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalAdaptiveAvgPool3d(
        self: *const Tensor, output_size: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__adaptive_avg_pool3d(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalAdaptiveAvgPool3dBackward(
        self: *const Tensor, grad_output: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__adaptive_avg_pool3d_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalAdaptiveAvgPool3dBackwardOut(
        self: *const Tensor, out: *const Tensor, grad_output: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__adaptive_avg_pool3d_backward_out(@ptrCast(&c_tensors), out.c_tensor,
                grad_output.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalAdaptiveAvgPool3dOut(
        self: *const Tensor, out: *const Tensor, output_size: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__adaptive_avg_pool3d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                output_size.ptr, @intCast(output_size.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalAddBatchDim(
        self: *const Tensor, batch_dim: i64, level: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__add_batch_dim(@ptrCast(&c_tensors), self.c_tensor,
                batch_dim,
                level);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalAddRelu(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__add_relu(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalAddRelu_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__add_relu_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalAddReluOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__add_relu_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalAddReluScalar(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__add_relu_scalar(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalAddReluScalar_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__add_relu_scalar_(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalAddReluScalarOut(
        self: *const Tensor, out: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__add_relu_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalAddmmActivation(
        self: *const Tensor, mat1: *const Tensor, mat2: *const Tensor, use_gelu: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__addmm_activation(@ptrCast(&c_tensors), self.c_tensor,
                mat1.c_tensor,
                mat2.c_tensor,
                if (use_gelu)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalAddmmActivationOut(
        self: *const Tensor, out: *const Tensor, mat1: *const Tensor, mat2: *const Tensor, use_gelu: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__addmm_activation_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                mat1.c_tensor,
                mat2.c_tensor,
                if (use_gelu)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalAminmax(
        self: *const Tensor, 
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__aminmax(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalAminmaxDim(
        self: *const Tensor, dim_: i64, keepdim: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__aminmax_dim(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalAminmaxDimOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, dim_: i64, keepdim: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__aminmax_dim_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                self.c_tensor,
                dim_,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalAminmaxOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__aminmax_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalAmpUpdateScale(
        self: *const Tensor, growth_tracker: *const Tensor, found_inf: *const Tensor, scale_growth_factor: f64, scale_backoff_factor: f64, growth_interval: i64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__amp_update_scale(@ptrCast(&c_tensors), self.c_tensor,
                growth_tracker.c_tensor,
                found_inf.c_tensor,
                scale_growth_factor,
                scale_backoff_factor,
                growth_interval);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalAmpUpdateScale_(
        self: *Tensor, growth_tracker: *const Tensor, found_inf: *const Tensor, scale_growth_factor: f64, scale_backoff_factor: f64, growth_interval: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__amp_update_scale_(@ptrCast(&c_tensors), self.c_tensor,
                growth_tracker.c_tensor,
                found_inf.c_tensor,
                scale_growth_factor,
                scale_backoff_factor,
                growth_interval);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalAmpUpdateScaleOut(
        self: *const Tensor, out: *const Tensor, growth_tracker: *const Tensor, found_inf: *const Tensor, scale_growth_factor: f64, scale_backoff_factor: f64, growth_interval: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__amp_update_scale_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                growth_tracker.c_tensor,
                found_inf.c_tensor,
                scale_growth_factor,
                scale_backoff_factor,
                growth_interval);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalAssertScalar(
        self_scalar: Scalar, assert_msg: []const u8
    ) void {
        __c.atg__assert_scalar(self_scalar.into().c_scalar,
                assert_msg.ptr, assert_msg.len);
        torch.readAndCleanError();
        return;
    }

    pub fn internalAssertTensorMetadata(
        a: *const Tensor, size_: ?[]i64, stride_: ?[]i64, dtype: ?Kind
    ) void {
        __c.atg__assert_tensor_metadata(a.c_tensor,
                size_.ptr, @intCast(size_.len),
                stride_.ptr, @intCast(stride_.len),
                dtype orelse -1);
        torch.readAndCleanError();
        return;
    }

    pub fn internalAutocastToFullPrecision(
        self: *const Tensor, cuda_enabled: bool, cpu_enabled: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__autocast_to_full_precision(@ptrCast(&c_tensors), self.c_tensor,
                if (cuda_enabled)  1  else  0,
                if (cpu_enabled)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalAutocastToReducedPrecision(
        self: *const Tensor, cuda_enabled: bool, cpu_enabled: bool, cuda_dtype: Kind, cpu_dtype: Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__autocast_to_reduced_precision(@ptrCast(&c_tensors), self.c_tensor,
                if (cuda_enabled)  1  else  0,
                if (cpu_enabled)  1  else  0,
                cuda_dtype.cInt(),
                cpu_dtype.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalCastByte(
        self: *const Tensor, non_blocking: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__cast_byte(@ptrCast(&c_tensors), self.c_tensor,
                if (non_blocking)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalCastChar(
        self: *const Tensor, non_blocking: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__cast_char(@ptrCast(&c_tensors), self.c_tensor,
                if (non_blocking)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalCastDouble(
        self: *const Tensor, non_blocking: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__cast_double(@ptrCast(&c_tensors), self.c_tensor,
                if (non_blocking)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalCastFloat(
        self: *const Tensor, non_blocking: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__cast_float(@ptrCast(&c_tensors), self.c_tensor,
                if (non_blocking)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalCastHalf(
        self: *const Tensor, non_blocking: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__cast_half(@ptrCast(&c_tensors), self.c_tensor,
                if (non_blocking)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalCastInt(
        self: *const Tensor, non_blocking: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__cast_int(@ptrCast(&c_tensors), self.c_tensor,
                if (non_blocking)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalCastLong(
        self: *const Tensor, non_blocking: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__cast_long(@ptrCast(&c_tensors), self.c_tensor,
                if (non_blocking)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalCastShort(
        self: *const Tensor, non_blocking: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__cast_short(@ptrCast(&c_tensors), self.c_tensor,
                if (non_blocking)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalCdistBackward(
        gradient: *const Tensor, x1: *const Tensor, x2: *const Tensor, p: f64, cdist_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__cdist_backward(@ptrCast(&c_tensors), gradient.c_tensor,
                x1.c_tensor,
                x2.c_tensor,
                p,
                cdist_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalCdistBackwardOut(
        out: *const Tensor, gradient: *const Tensor, x1: *const Tensor, x2: *const Tensor, p: f64, cdist_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__cdist_backward_out(@ptrCast(&c_tensors), out.c_tensor,
                gradient.c_tensor,
                x1.c_tensor,
                x2.c_tensor,
                p,
                cdist_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalCholeskySolveHelper(
        self: *const Tensor, a: *const Tensor, upper: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__cholesky_solve_helper(@ptrCast(&c_tensors), self.c_tensor,
                a.c_tensor,
                if (upper)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalCholeskySolveHelperOut(
        self: *const Tensor, out: *const Tensor, a: *const Tensor, upper: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__cholesky_solve_helper_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                a.c_tensor,
                if (upper)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalChunkCat(
        tensors: []*const Tensor, dim_: i64, num_chunks: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__chunk_cat(@ptrCast(&c_tensors), ptrList(tensors).ptr, @intCast(tensors.len),
                dim_,
                num_chunks);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalChunkCatOut(
        out: *const Tensor, tensors: []*const Tensor, dim_: i64, num_chunks: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__chunk_cat_out(@ptrCast(&c_tensors), out.c_tensor,
                ptrList(tensors).ptr, @intCast(tensors.len),
                dim_,
                num_chunks);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalCoalesce(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__coalesce(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalCoalesceOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__coalesce_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalCoalesced(
        self: *const Tensor, coalesced: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__coalesced(@ptrCast(&c_tensors), self.c_tensor,
                if (coalesced)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalCoalesced_(
        self: *Tensor, coalesced: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__coalesced_(@ptrCast(&c_tensors), self.c_tensor,
                if (coalesced)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalCoalescedOut(
        self: *const Tensor, out: *const Tensor, coalesced: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__coalesced_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                if (coalesced)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalComputeLinearCombination(
        self: *const Tensor, coefficients: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__compute_linear_combination(@ptrCast(&c_tensors), self.c_tensor,
                coefficients.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalComputeLinearCombinationOut(
        self: *const Tensor, out: *const Tensor, coefficients: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__compute_linear_combination_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                coefficients.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalConj(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__conj(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalConjCopy(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__conj_copy(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalConjCopyOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__conj_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalConjPhysical(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__conj_physical(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalConjPhysicalOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__conj_physical_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalConvDepthwise2d(
        self: *const Tensor, weight: *const Tensor, kernel_size: []i64, bias: ?*const Tensor, stride_: []i64, padding: []i64, dilation: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__conv_depthwise2d(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalConvDepthwise2dOut(
        self: *const Tensor, out: *const Tensor, weight: *const Tensor, kernel_size: []i64, bias: ?*const Tensor, stride_: []i64, padding: []i64, dilation: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__conv_depthwise2d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                weight.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalConvertIndicesFromCooToCsr(
        self: *const Tensor, size_: i64, out_int32: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__convert_indices_from_coo_to_csr(@ptrCast(&c_tensors), self.c_tensor,
                size_,
                if (out_int32)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalConvertIndicesFromCooToCsrOut(
        self: *const Tensor, out: *const Tensor, size_: i64, out_int32: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__convert_indices_from_coo_to_csr_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                size_,
                if (out_int32)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalConvertIndicesFromCsrToCoo(
        crow_indices_: *const Tensor, col_indices_: *const Tensor, out_int32: bool, transpose_: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__convert_indices_from_csr_to_coo(@ptrCast(&c_tensors), crow_indices_.c_tensor,
                col_indices_.c_tensor,
                if (out_int32)  1  else  0,
                if (transpose_)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalConvertIndicesFromCsrToCooOut(
        out: *const Tensor, crow_indices_: *const Tensor, col_indices_: *const Tensor, out_int32: bool, transpose_: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__convert_indices_from_csr_to_coo_out(@ptrCast(&c_tensors), out.c_tensor,
                crow_indices_.c_tensor,
                col_indices_.c_tensor,
                if (out_int32)  1  else  0,
                if (transpose_)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalConvertWeightToInt4pack(
        self: *const Tensor, innerktiles: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__convert_weight_to_int4pack(@ptrCast(&c_tensors), self.c_tensor,
                innerktiles);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalConvolution(
        self: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, stride_: []i64, padding: []i64, dilation: []i64, transposed: bool, output_padding: []i64, groups: i64, benchmark: bool, deterministic: bool, cudnn_enabled: bool, allow_tf32: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__convolution(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (transposed)  1  else  0,
                output_padding.ptr, @intCast(output_padding.len),
                groups,
                if (benchmark)  1  else  0,
                if (deterministic)  1  else  0,
                if (cudnn_enabled)  1  else  0,
                if (allow_tf32)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalConvolutionDeprecated(
        self: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, stride_: []i64, padding: []i64, dilation: []i64, transposed: bool, output_padding: []i64, groups: i64, benchmark: bool, deterministic: bool, cudnn_enabled: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__convolution_deprecated(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (transposed)  1  else  0,
                output_padding.ptr, @intCast(output_padding.len),
                groups,
                if (benchmark)  1  else  0,
                if (deterministic)  1  else  0,
                if (cudnn_enabled)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalConvolutionMode(
        self: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, stride_: []i64, padding: []const u8, dilation: []i64, groups: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__convolution_mode(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, padding.len,
                dilation.ptr, @intCast(dilation.len),
                groups);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalConvolutionOut(
        self: *const Tensor, out: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, stride_: []i64, padding: []i64, dilation: []i64, transposed: bool, output_padding: []i64, groups: i64, benchmark: bool, deterministic: bool, cudnn_enabled: bool, allow_tf32: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__convolution_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (transposed)  1  else  0,
                output_padding.ptr, @intCast(output_padding.len),
                groups,
                if (benchmark)  1  else  0,
                if (deterministic)  1  else  0,
                if (cudnn_enabled)  1  else  0,
                if (allow_tf32)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalCopyFrom(
        self: *const Tensor, dst: *const Tensor, non_blocking: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__copy_from(@ptrCast(&c_tensors), self.c_tensor,
                dst.c_tensor,
                if (non_blocking)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalCopyFromAndResize(
        self: *const Tensor, dst: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__copy_from_and_resize(@ptrCast(&c_tensors), self.c_tensor,
                dst.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalCopyFromAndResizeOut(
        self: *const Tensor, out: *const Tensor, dst: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__copy_from_and_resize_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dst.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalCopyFromOut(
        self: *const Tensor, out: *const Tensor, dst: *const Tensor, non_blocking: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__copy_from_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dst.c_tensor,
                if (non_blocking)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalCsltCompress(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__cslt_compress(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalCsltSparseMm(
        compressed_a: *const Tensor, dense_b: *const Tensor, bias: ?*const Tensor, alpha: ?*const Tensor, out_dtype: ?Kind, transpose_result: bool, alg_id: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__cslt_sparse_mm(@ptrCast(&c_tensors), compressed_a.c_tensor,
                dense_b.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                if (alpha != null) alpha.?.c_tensor else null,
                out_dtype orelse -1,
                if (transpose_result)  1  else  0,
                alg_id);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalCsltSparseMmSearch(
        compressed_a: *const Tensor, dense_b: *const Tensor, bias: ?*const Tensor, alpha: ?*const Tensor, out_dtype: ?Kind, transpose_result: bool
    ) i64 {
        const return_ = __c.atg__cslt_sparse_mm_search(compressed_a.c_tensor,
                dense_b.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                if (alpha != null) alpha.?.c_tensor else null,
                out_dtype orelse -1,
                if (transpose_result)  1  else  0);
       torch.readAndCleanError();
        return return_;
    }

    pub fn internalCtcLoss(
        log_probs: *const Tensor, targets: *const Tensor, input_lengths: []i64, target_lengths: []i64, blank: i64, zero_infinity: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__ctc_loss(@ptrCast(&c_tensors), log_probs.c_tensor,
                targets.c_tensor,
                input_lengths.ptr, @intCast(input_lengths.len),
                target_lengths.ptr, @intCast(target_lengths.len),
                blank,
                if (zero_infinity)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalCtcLossBackward(
        gradient: *const Tensor, log_probs: *const Tensor, targets: *const Tensor, input_lengths: []i64, target_lengths: []i64, neg_log_likelihood: *const Tensor, log_alpha: *const Tensor, blank: i64, zero_infinity: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__ctc_loss_backward(@ptrCast(&c_tensors), gradient.c_tensor,
                log_probs.c_tensor,
                targets.c_tensor,
                input_lengths.ptr, @intCast(input_lengths.len),
                target_lengths.ptr, @intCast(target_lengths.len),
                neg_log_likelihood.c_tensor,
                log_alpha.c_tensor,
                blank,
                if (zero_infinity)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalCtcLossBackwardOut(
        out: *const Tensor, gradient: *const Tensor, log_probs: *const Tensor, targets: *const Tensor, input_lengths: []i64, target_lengths: []i64, neg_log_likelihood: *const Tensor, log_alpha: *const Tensor, blank: i64, zero_infinity: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__ctc_loss_backward_out(@ptrCast(&c_tensors), out.c_tensor,
                gradient.c_tensor,
                log_probs.c_tensor,
                targets.c_tensor,
                input_lengths.ptr, @intCast(input_lengths.len),
                target_lengths.ptr, @intCast(target_lengths.len),
                neg_log_likelihood.c_tensor,
                log_alpha.c_tensor,
                blank,
                if (zero_infinity)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalCtcLossBackwardTensor(
        gradient: *const Tensor, log_probs: *const Tensor, targets: *const Tensor, input_lengths: *const Tensor, target_lengths: *const Tensor, neg_log_likelihood: *const Tensor, log_alpha: *const Tensor, blank: i64, zero_infinity: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__ctc_loss_backward_tensor(@ptrCast(&c_tensors), gradient.c_tensor,
                log_probs.c_tensor,
                targets.c_tensor,
                input_lengths.c_tensor,
                target_lengths.c_tensor,
                neg_log_likelihood.c_tensor,
                log_alpha.c_tensor,
                blank,
                if (zero_infinity)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalCtcLossOut(
        out0: *const Tensor, out1: *const Tensor, log_probs: *const Tensor, targets: *const Tensor, input_lengths: []i64, target_lengths: []i64, blank: i64, zero_infinity: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__ctc_loss_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                log_probs.c_tensor,
                targets.c_tensor,
                input_lengths.ptr, @intCast(input_lengths.len),
                target_lengths.ptr, @intCast(target_lengths.len),
                blank,
                if (zero_infinity)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalCtcLossTensor(
        log_probs: *const Tensor, targets: *const Tensor, input_lengths: *const Tensor, target_lengths: *const Tensor, blank: i64, zero_infinity: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__ctc_loss_tensor(@ptrCast(&c_tensors), log_probs.c_tensor,
                targets.c_tensor,
                input_lengths.c_tensor,
                target_lengths.c_tensor,
                blank,
                if (zero_infinity)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalCtcLossTensorOut(
        out0: *const Tensor, out1: *const Tensor, log_probs: *const Tensor, targets: *const Tensor, input_lengths: *const Tensor, target_lengths: *const Tensor, blank: i64, zero_infinity: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__ctc_loss_tensor_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                log_probs.c_tensor,
                targets.c_tensor,
                input_lengths.c_tensor,
                target_lengths.c_tensor,
                blank,
                if (zero_infinity)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalCudnnCtcLoss(
        log_probs: *const Tensor, targets: *const Tensor, input_lengths: []i64, target_lengths: []i64, blank: i64, deterministic: bool, zero_infinity: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__cudnn_ctc_loss(@ptrCast(&c_tensors), log_probs.c_tensor,
                targets.c_tensor,
                input_lengths.ptr, @intCast(input_lengths.len),
                target_lengths.ptr, @intCast(target_lengths.len),
                blank,
                if (deterministic)  1  else  0,
                if (zero_infinity)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalCudnnCtcLossOut(
        out0: *const Tensor, out1: *const Tensor, log_probs: *const Tensor, targets: *const Tensor, input_lengths: []i64, target_lengths: []i64, blank: i64, deterministic: bool, zero_infinity: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__cudnn_ctc_loss_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                log_probs.c_tensor,
                targets.c_tensor,
                input_lengths.ptr, @intCast(input_lengths.len),
                target_lengths.ptr, @intCast(target_lengths.len),
                blank,
                if (deterministic)  1  else  0,
                if (zero_infinity)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalCudnnCtcLossTensor(
        log_probs: *const Tensor, targets: *const Tensor, input_lengths: *const Tensor, target_lengths: *const Tensor, blank: i64, deterministic: bool, zero_infinity: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__cudnn_ctc_loss_tensor(@ptrCast(&c_tensors), log_probs.c_tensor,
                targets.c_tensor,
                input_lengths.c_tensor,
                target_lengths.c_tensor,
                blank,
                if (deterministic)  1  else  0,
                if (zero_infinity)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalCudnnInitDropoutState(
        dropout_: f64, train: bool, dropout_seed: i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__cudnn_init_dropout_state(@ptrCast(&c_tensors), dropout_,
                if (train)  1  else  0,
                dropout_seed,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalCudnnInitDropoutStateOut(
        out: *const Tensor, dropout_: f64, train: bool, dropout_seed: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__cudnn_init_dropout_state_out(@ptrCast(&c_tensors), out.c_tensor,
                dropout_,
                if (train)  1  else  0,
                dropout_seed);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalCudnnRnn(
        self: *const Tensor, weight: []*const Tensor, weight_stride0: i64, weight_buf: ?*const Tensor, hx: *const Tensor, cx: ?*const Tensor, mode_: i64, hidden_size: i64, proj_size: i64, num_layers: i64, batch_first: bool, dropout_: f64, train: bool, bidirectional: bool, batch_sizes: []i64, dropout_state: ?*const Tensor
    ) [5]Tensor {
        var c_tensors = [_]C_tensor{null} ** 5;
        __c.atg__cudnn_rnn(@ptrCast(&c_tensors), self.c_tensor,
                ptrList(weight).ptr, @intCast(weight.len),
                weight_stride0,
                if (weight_buf != null) weight_buf.?.c_tensor else null,
                hx.c_tensor,
                if (cx != null) cx.?.c_tensor else null,
                mode_,
                hidden_size,
                proj_size,
                num_layers,
                if (batch_first)  1  else  0,
                dropout_,
                if (train)  1  else  0,
                if (bidirectional)  1  else  0,
                batch_sizes.ptr, @intCast(batch_sizes.len),
                if (dropout_state != null) dropout_state.?.c_tensor else null);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }, Tensor { .c_tensor = c_tensors[3] }, Tensor { .c_tensor = c_tensors[4] }};
    }

    pub fn internalCudnnRnnFlattenWeight(
        weight_arr: []*const Tensor, weight_stride0: i64, input_size: i64, mode_: i64, hidden_size: i64, proj_size: i64, num_layers: i64, batch_first: bool, bidirectional: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__cudnn_rnn_flatten_weight(@ptrCast(&c_tensors), ptrList(weight_arr).ptr, @intCast(weight_arr.len),
                weight_stride0,
                input_size,
                mode_,
                hidden_size,
                proj_size,
                num_layers,
                if (batch_first)  1  else  0,
                if (bidirectional)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalCudnnRnnFlattenWeightOut(
        out: *const Tensor, weight_arr: []*const Tensor, weight_stride0: i64, input_size: i64, mode_: i64, hidden_size: i64, proj_size: i64, num_layers: i64, batch_first: bool, bidirectional: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__cudnn_rnn_flatten_weight_out(@ptrCast(&c_tensors), out.c_tensor,
                ptrList(weight_arr).ptr, @intCast(weight_arr.len),
                weight_stride0,
                input_size,
                mode_,
                hidden_size,
                proj_size,
                num_layers,
                if (batch_first)  1  else  0,
                if (bidirectional)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalCudnnRnnOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, out2: *const Tensor, out3: *const Tensor, out4: *const Tensor, weight: []*const Tensor, weight_stride0: i64, weight_buf: ?*const Tensor, hx: *const Tensor, cx: ?*const Tensor, mode_: i64, hidden_size: i64, proj_size: i64, num_layers: i64, batch_first: bool, dropout_: f64, train: bool, bidirectional: bool, batch_sizes: []i64, dropout_state: ?*const Tensor
    ) [5]Tensor {
        var c_tensors = [_]C_tensor{null} ** 5;
        __c.atg__cudnn_rnn_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                out2.c_tensor,
                out3.c_tensor,
                out4.c_tensor,
                self.c_tensor,
                ptrList(weight).ptr, @intCast(weight.len),
                weight_stride0,
                if (weight_buf != null) weight_buf.?.c_tensor else null,
                hx.c_tensor,
                if (cx != null) cx.?.c_tensor else null,
                mode_,
                hidden_size,
                proj_size,
                num_layers,
                if (batch_first)  1  else  0,
                dropout_,
                if (train)  1  else  0,
                if (bidirectional)  1  else  0,
                batch_sizes.ptr, @intCast(batch_sizes.len),
                if (dropout_state != null) dropout_state.?.c_tensor else null);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }, Tensor { .c_tensor = c_tensors[3] }, Tensor { .c_tensor = c_tensors[4] }};
    }

    pub fn internalDebugHasInternalOverlap(
        self: *const Tensor, 
    ) i64 {
        const return_ = __c.atg__debug_has_internal_overlap(self.c_tensor);
       torch.readAndCleanError();
        return return_;
    }

    pub fn internalDimArange(
        like: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__dim_arange(@ptrCast(&c_tensors), like.c_tensor,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalDimi(
        self: *const Tensor, 
    ) i64 {
        const return_ = __c.atg__dimi(self.c_tensor);
       torch.readAndCleanError();
        return return_;
    }

    pub fn internalDimv(
        self: *const Tensor, 
    ) i64 {
        const return_ = __c.atg__dimv(self.c_tensor);
       torch.readAndCleanError();
        return return_;
    }

    pub fn internalDirichletGrad(
        x: *const Tensor, alpha: *const Tensor, total: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__dirichlet_grad(@ptrCast(&c_tensors), x.c_tensor,
                alpha.c_tensor,
                total.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalDirichletGradOut(
        out: *const Tensor, x: *const Tensor, alpha: *const Tensor, total: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__dirichlet_grad_out(@ptrCast(&c_tensors), out.c_tensor,
                x.c_tensor,
                alpha.c_tensor,
                total.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalEfficientAttentionBackward(
        grad_out_: *const Tensor, query: *const Tensor, key: *const Tensor, value: *const Tensor, bias: ?*const Tensor, out: *const Tensor, cu_seqlens_q: ?*const Tensor, cu_seqlens_k: ?*const Tensor, max_seqlen_q: i64, max_seqlen_k: i64, logsumexp_: *const Tensor, dropout_p: f64, philox_seed: *const Tensor, philox_offset: *const Tensor, custom_mask_type: i64, bias_requires_grad: bool, scale: ?f64, num_splits_key: ?i64
    ) [4]Tensor {
        var c_tensors = [_]C_tensor{null} ** 4;
        __c.atg__efficient_attention_backward(@ptrCast(&c_tensors), grad_out_.c_tensor,
                query.c_tensor,
                key.c_tensor,
                value.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                out.c_tensor,
                if (cu_seqlens_q != null) cu_seqlens_q.?.c_tensor else null,
                if (cu_seqlens_k != null) cu_seqlens_k.?.c_tensor else null,
                max_seqlen_q,
                max_seqlen_k,
                logsumexp_.c_tensor,
                dropout_p,
                philox_seed.c_tensor,
                philox_offset.c_tensor,
                custom_mask_type,
                if (bias_requires_grad)  1  else  0,
                scale orelse std.math.nan, (scale == null),
                num_splits_key orelse 0, (num_splits_key == null));
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }, Tensor { .c_tensor = c_tensors[3] }};
    }

    pub fn internalEfficientzerotensor(
        size_: []i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__efficientzerotensor(@ptrCast(&c_tensors), size_.ptr, @intCast(size_.len),
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalEfficientzerotensorOut(
        out: *const Tensor, size_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__efficientzerotensor_out(@ptrCast(&c_tensors), out.c_tensor,
                size_.ptr, @intCast(size_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalEmbeddingBag(
        weight: *const Tensor, indices_: *const Tensor, offsets: *const Tensor, scale_grad_by_freq: bool, mode_: i64, sparse: bool, per_sample_weights: ?*const Tensor, include_last_offset: bool, padding_idx: i64
    ) [4]Tensor {
        var c_tensors = [_]C_tensor{null} ** 4;
        __c.atg__embedding_bag(@ptrCast(&c_tensors), weight.c_tensor,
                indices_.c_tensor,
                offsets.c_tensor,
                if (scale_grad_by_freq)  1  else  0,
                mode_,
                if (sparse)  1  else  0,
                if (per_sample_weights != null) per_sample_weights.?.c_tensor else null,
                if (include_last_offset)  1  else  0,
                padding_idx);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }, Tensor { .c_tensor = c_tensors[3] }};
    }

    pub fn internalEmbeddingBagBackward(
        gradient: *const Tensor, indices_: *const Tensor, offsets: *const Tensor, offset2bag: *const Tensor, bag_size: *const Tensor, maximum_indices: *const Tensor, num_weights: i64, scale_grad_by_freq: bool, mode_: i64, sparse: bool, per_sample_weights: ?*const Tensor, padding_idx: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__embedding_bag_backward(@ptrCast(&c_tensors), gradient.c_tensor,
                indices_.c_tensor,
                offsets.c_tensor,
                offset2bag.c_tensor,
                bag_size.c_tensor,
                maximum_indices.c_tensor,
                num_weights,
                if (scale_grad_by_freq)  1  else  0,
                mode_,
                if (sparse)  1  else  0,
                if (per_sample_weights != null) per_sample_weights.?.c_tensor else null,
                padding_idx);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalEmbeddingBagDenseBackward(
        gradient: *const Tensor, indices_: *const Tensor, offset2bag: *const Tensor, bag_size: *const Tensor, maximum_indices: *const Tensor, num_weights: i64, scale_grad_by_freq: bool, mode_: i64, per_sample_weights: ?*const Tensor, padding_idx: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__embedding_bag_dense_backward(@ptrCast(&c_tensors), gradient.c_tensor,
                indices_.c_tensor,
                offset2bag.c_tensor,
                bag_size.c_tensor,
                maximum_indices.c_tensor,
                num_weights,
                if (scale_grad_by_freq)  1  else  0,
                mode_,
                if (per_sample_weights != null) per_sample_weights.?.c_tensor else null,
                padding_idx);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalEmbeddingBagDenseBackwardOut(
        out: *const Tensor, gradient: *const Tensor, indices_: *const Tensor, offset2bag: *const Tensor, bag_size: *const Tensor, maximum_indices: *const Tensor, num_weights: i64, scale_grad_by_freq: bool, mode_: i64, per_sample_weights: ?*const Tensor, padding_idx: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__embedding_bag_dense_backward_out(@ptrCast(&c_tensors), out.c_tensor,
                gradient.c_tensor,
                indices_.c_tensor,
                offset2bag.c_tensor,
                bag_size.c_tensor,
                maximum_indices.c_tensor,
                num_weights,
                if (scale_grad_by_freq)  1  else  0,
                mode_,
                if (per_sample_weights != null) per_sample_weights.?.c_tensor else null,
                padding_idx);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalEmbeddingBagForwardOnly(
        weight: *const Tensor, indices_: *const Tensor, offsets: *const Tensor, scale_grad_by_freq: bool, mode_: i64, sparse: bool, per_sample_weights: ?*const Tensor, include_last_offset: bool, padding_idx: i64
    ) [4]Tensor {
        var c_tensors = [_]C_tensor{null} ** 4;
        __c.atg__embedding_bag_forward_only(@ptrCast(&c_tensors), weight.c_tensor,
                indices_.c_tensor,
                offsets.c_tensor,
                if (scale_grad_by_freq)  1  else  0,
                mode_,
                if (sparse)  1  else  0,
                if (per_sample_weights != null) per_sample_weights.?.c_tensor else null,
                if (include_last_offset)  1  else  0,
                padding_idx);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }, Tensor { .c_tensor = c_tensors[3] }};
    }

    pub fn internalEmbeddingBagForwardOnlyOut(
        out0: *const Tensor, out1: *const Tensor, out2: *const Tensor, out3: *const Tensor, weight: *const Tensor, indices_: *const Tensor, offsets: *const Tensor, scale_grad_by_freq: bool, mode_: i64, sparse: bool, per_sample_weights: ?*const Tensor, include_last_offset: bool, padding_idx: i64
    ) [4]Tensor {
        var c_tensors = [_]C_tensor{null} ** 4;
        __c.atg__embedding_bag_forward_only_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                out2.c_tensor,
                out3.c_tensor,
                weight.c_tensor,
                indices_.c_tensor,
                offsets.c_tensor,
                if (scale_grad_by_freq)  1  else  0,
                mode_,
                if (sparse)  1  else  0,
                if (per_sample_weights != null) per_sample_weights.?.c_tensor else null,
                if (include_last_offset)  1  else  0,
                padding_idx);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }, Tensor { .c_tensor = c_tensors[3] }};
    }

    pub fn internalEmbeddingBagOut(
        out0: *const Tensor, out1: *const Tensor, out2: *const Tensor, out3: *const Tensor, weight: *const Tensor, indices_: *const Tensor, offsets: *const Tensor, scale_grad_by_freq: bool, mode_: i64, sparse: bool, per_sample_weights: ?*const Tensor, include_last_offset: bool, padding_idx: i64
    ) [4]Tensor {
        var c_tensors = [_]C_tensor{null} ** 4;
        __c.atg__embedding_bag_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                out2.c_tensor,
                out3.c_tensor,
                weight.c_tensor,
                indices_.c_tensor,
                offsets.c_tensor,
                if (scale_grad_by_freq)  1  else  0,
                mode_,
                if (sparse)  1  else  0,
                if (per_sample_weights != null) per_sample_weights.?.c_tensor else null,
                if (include_last_offset)  1  else  0,
                padding_idx);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }, Tensor { .c_tensor = c_tensors[3] }};
    }

    pub fn internalEmbeddingBagPerSampleWeightsBackward(
        gradient: *const Tensor, weight: *const Tensor, indices_: *const Tensor, offsets: *const Tensor, offset2bag: *const Tensor, mode_: i64, padding_idx: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__embedding_bag_per_sample_weights_backward(@ptrCast(&c_tensors), gradient.c_tensor,
                weight.c_tensor,
                indices_.c_tensor,
                offsets.c_tensor,
                offset2bag.c_tensor,
                mode_,
                padding_idx);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalEmbeddingBagPerSampleWeightsBackwardOut(
        out: *const Tensor, gradient: *const Tensor, weight: *const Tensor, indices_: *const Tensor, offsets: *const Tensor, offset2bag: *const Tensor, mode_: i64, padding_idx: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__embedding_bag_per_sample_weights_backward_out(@ptrCast(&c_tensors), out.c_tensor,
                gradient.c_tensor,
                weight.c_tensor,
                indices_.c_tensor,
                offsets.c_tensor,
                offset2bag.c_tensor,
                mode_,
                padding_idx);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalEmbeddingBagSparseBackward(
        gradient: *const Tensor, indices_: *const Tensor, offsets: *const Tensor, offset2bag: *const Tensor, bag_size: *const Tensor, num_weights: i64, scale_grad_by_freq: bool, mode_: i64, per_sample_weights: ?*const Tensor, padding_idx: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__embedding_bag_sparse_backward(@ptrCast(&c_tensors), gradient.c_tensor,
                indices_.c_tensor,
                offsets.c_tensor,
                offset2bag.c_tensor,
                bag_size.c_tensor,
                num_weights,
                if (scale_grad_by_freq)  1  else  0,
                mode_,
                if (per_sample_weights != null) per_sample_weights.?.c_tensor else null,
                padding_idx);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalEmptyAffineQuantized(
        size_: []i64, options_: TensorOptions, scale: f64, zero_point: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__empty_affine_quantized(@ptrCast(&c_tensors), size_.ptr, @intCast(size_.len),
                options_.kind.cInt(), options_.device.cInt(),
                scale,
                zero_point);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalEmptyAffineQuantizedOut(
        out: *const Tensor, size_: []i64, scale: f64, zero_point: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__empty_affine_quantized_out(@ptrCast(&c_tensors), out.c_tensor,
                size_.ptr, @intCast(size_.len),
                scale,
                zero_point);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalEmptyPerChannelAffineQuantized(
        size_: []i64, scales: *const Tensor, zero_points: *const Tensor, axis: i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__empty_per_channel_affine_quantized(@ptrCast(&c_tensors), size_.ptr, @intCast(size_.len),
                scales.c_tensor,
                zero_points.c_tensor,
                axis,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalEmptyPerChannelAffineQuantizedOut(
        out: *const Tensor, size_: []i64, scales: *const Tensor, zero_points: *const Tensor, axis: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__empty_per_channel_affine_quantized_out(@ptrCast(&c_tensors), out.c_tensor,
                size_.ptr, @intCast(size_.len),
                scales.c_tensor,
                zero_points.c_tensor,
                axis);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalEuclideanDist(
        x1: *const Tensor, x2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__euclidean_dist(@ptrCast(&c_tensors), x1.c_tensor,
                x2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalEuclideanDistOut(
        out: *const Tensor, x1: *const Tensor, x2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__euclidean_dist_out(@ptrCast(&c_tensors), out.c_tensor,
                x1.c_tensor,
                x2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalFakeQuantizeLearnablePerChannelAffine(
        self: *const Tensor, scale: *const Tensor, zero_point: *const Tensor, axis: i64, quant_min: i64, quant_max: i64, grad_factor: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__fake_quantize_learnable_per_channel_affine(@ptrCast(&c_tensors), self.c_tensor,
                scale.c_tensor,
                zero_point.c_tensor,
                axis,
                quant_min,
                quant_max,
                grad_factor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalFakeQuantizeLearnablePerChannelAffineBackward(
        self: *const Tensor, gradient: *const Tensor, scale: *const Tensor, zero_point: *const Tensor, axis: i64, quant_min: i64, quant_max: i64, grad_factor: f64
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg__fake_quantize_learnable_per_channel_affine_backward(@ptrCast(&c_tensors), gradient.c_tensor,
                self.c_tensor,
                scale.c_tensor,
                zero_point.c_tensor,
                axis,
                quant_min,
                quant_max,
                grad_factor);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn internalFakeQuantizeLearnablePerChannelAffineOut(
        self: *const Tensor, out: *const Tensor, scale: *const Tensor, zero_point: *const Tensor, axis: i64, quant_min: i64, quant_max: i64, grad_factor: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__fake_quantize_learnable_per_channel_affine_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                scale.c_tensor,
                zero_point.c_tensor,
                axis,
                quant_min,
                quant_max,
                grad_factor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalFakeQuantizeLearnablePerTensorAffine(
        self: *const Tensor, scale: *const Tensor, zero_point: *const Tensor, quant_min: i64, quant_max: i64, grad_factor: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__fake_quantize_learnable_per_tensor_affine(@ptrCast(&c_tensors), self.c_tensor,
                scale.c_tensor,
                zero_point.c_tensor,
                quant_min,
                quant_max,
                grad_factor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalFakeQuantizeLearnablePerTensorAffineBackward(
        self: *const Tensor, gradient: *const Tensor, scale: *const Tensor, zero_point: *const Tensor, quant_min: i64, quant_max: i64, grad_factor: f64
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg__fake_quantize_learnable_per_tensor_affine_backward(@ptrCast(&c_tensors), gradient.c_tensor,
                self.c_tensor,
                scale.c_tensor,
                zero_point.c_tensor,
                quant_min,
                quant_max,
                grad_factor);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn internalFakeQuantizeLearnablePerTensorAffineOut(
        self: *const Tensor, out: *const Tensor, scale: *const Tensor, zero_point: *const Tensor, quant_min: i64, quant_max: i64, grad_factor: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__fake_quantize_learnable_per_tensor_affine_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                scale.c_tensor,
                zero_point.c_tensor,
                quant_min,
                quant_max,
                grad_factor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalFakeQuantizePerTensorAffineCachemaskTensorQparams(
        self: *const Tensor, scale: *const Tensor, zero_point: *const Tensor, fake_quant_enabled: *const Tensor, quant_min: i64, quant_max: i64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__fake_quantize_per_tensor_affine_cachemask_tensor_qparams(@ptrCast(&c_tensors), self.c_tensor,
                scale.c_tensor,
                zero_point.c_tensor,
                fake_quant_enabled.c_tensor,
                quant_min,
                quant_max);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalFakeQuantizePerTensorAffineCachemaskTensorQparamsOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, scale: *const Tensor, zero_point: *const Tensor, fake_quant_enabled: *const Tensor, quant_min: i64, quant_max: i64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__fake_quantize_per_tensor_affine_cachemask_tensor_qparams_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                self.c_tensor,
                scale.c_tensor,
                zero_point.c_tensor,
                fake_quant_enabled.c_tensor,
                quant_min,
                quant_max);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalFftC2c(
        self: *const Tensor, dim_: []i64, normalization: i64, forward: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__fft_c2c(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                normalization,
                if (forward)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalFftC2cOut(
        self: *const Tensor, out: *const Tensor, dim_: []i64, normalization: i64, forward: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__fft_c2c_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                normalization,
                if (forward)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalFftC2r(
        self: *const Tensor, dim_: []i64, normalization: i64, last_dim_size: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__fft_c2r(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                normalization,
                last_dim_size);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalFftC2rOut(
        self: *const Tensor, out: *const Tensor, dim_: []i64, normalization: i64, last_dim_size: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__fft_c2r_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                normalization,
                last_dim_size);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalFftR2c(
        self: *const Tensor, dim_: []i64, normalization: i64, onesided: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__fft_r2c(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                normalization,
                if (onesided)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalFftR2cOut(
        self: *const Tensor, out: *const Tensor, dim_: []i64, normalization: i64, onesided: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__fft_r2c_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                normalization,
                if (onesided)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalFillMemEffDropoutMask_(
        self: *Tensor, dropout_p: f64, seed: i64, offset: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__fill_mem_eff_dropout_mask_(@ptrCast(&c_tensors), self.c_tensor,
                dropout_p,
                seed,
                offset);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalFlashAttentionBackward(
        grad_out: *const Tensor, query: *const Tensor, key: *const Tensor, value: *const Tensor, out: *const Tensor, logsumexp_: *const Tensor, cum_seq_q: *const Tensor, cum_seq_k: *const Tensor, max_q: i64, max_k: i64, dropout_p: f64, is_causal: bool, philox_seed: *const Tensor, philox_offset: *const Tensor, scale: ?f64
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg__flash_attention_backward(@ptrCast(&c_tensors), grad_out.c_tensor,
                query.c_tensor,
                key.c_tensor,
                value.c_tensor,
                out.c_tensor,
                logsumexp_.c_tensor,
                cum_seq_q.c_tensor,
                cum_seq_k.c_tensor,
                max_q,
                max_k,
                dropout_p,
                if (is_causal)  1  else  0,
                philox_seed.c_tensor,
                philox_offset.c_tensor,
                scale orelse std.math.nan, (scale == null));
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn internalFoobar(
        self: *const Tensor, arg1: bool, arg2: bool, arg3: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__foobar(@ptrCast(&c_tensors), self.c_tensor,
                if (arg1)  1  else  0,
                if (arg2)  1  else  0,
                if (arg3)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalFoobarOut(
        self: *const Tensor, out: *const Tensor, arg1: bool, arg2: bool, arg3: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__foobar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                if (arg1)  1  else  0,
                if (arg2)  1  else  0,
                if (arg3)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalFunctionalAssertAsync(
        self: *const Tensor, assert_msg: []const u8, dep_token: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__functional_assert_async(@ptrCast(&c_tensors), self.c_tensor,
                assert_msg.ptr, assert_msg.len,
                dep_token.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalFunctionalAssertScalar(
        self_scalar: Scalar, assert_msg: []const u8, dep_token: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__functional_assert_scalar(@ptrCast(&c_tensors), self_scalar.into().c_scalar,
                assert_msg.ptr, assert_msg.len,
                dep_token.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalFunctionalSymConstrainRange(
        size_: Scalar, min_: ?i64, max_: ?i64, dep_token: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__functional_sym_constrain_range(@ptrCast(&c_tensors), size_.into().c_scalar,
                min_ orelse 0, (min_ == null),
                max_ orelse 0, (max_ == null),
                dep_token.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalFunctionalSymConstrainRangeForSize(
        size_: Scalar, min_: ?i64, max_: ?i64, dep_token: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__functional_sym_constrain_range_for_size(@ptrCast(&c_tensors), size_.into().c_scalar,
                min_ orelse 0, (min_ == null),
                max_ orelse 0, (max_ == null),
                dep_token.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalFusedDropout(
        self: *const Tensor, p: f64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__fused_dropout(@ptrCast(&c_tensors), self.c_tensor,
                p);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalFusedDropoutOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, p: f64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__fused_dropout_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                self.c_tensor,
                p);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalFusedMovingAvgObsFqHelper(
        self: *const Tensor, observer_on: *const Tensor, fake_quant_on: *const Tensor, running_min: *const Tensor, running_max: *const Tensor, scale: *const Tensor, zero_point: *const Tensor, averaging_const: f64, quant_min: i64, quant_max: i64, ch_axis: i64, per_row_fake_quant: bool, symmetric_quant: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__fused_moving_avg_obs_fq_helper(@ptrCast(&c_tensors), self.c_tensor,
                observer_on.c_tensor,
                fake_quant_on.c_tensor,
                running_min.c_tensor,
                running_max.c_tensor,
                scale.c_tensor,
                zero_point.c_tensor,
                averaging_const,
                quant_min,
                quant_max,
                ch_axis,
                if (per_row_fake_quant)  1  else  0,
                if (symmetric_quant)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalFusedMovingAvgObsFqHelperFunctional(
        self: *const Tensor, observer_on: *const Tensor, fake_quant_on: *const Tensor, running_min: *const Tensor, running_max: *const Tensor, scale: *const Tensor, zero_point: *const Tensor, averaging_const: f64, quant_min: i64, quant_max: i64, ch_axis: i64, per_row_fake_quant: bool, symmetric_quant: bool
    ) [6]Tensor {
        var c_tensors = [_]C_tensor{null} ** 6;
        __c.atg__fused_moving_avg_obs_fq_helper_functional(@ptrCast(&c_tensors), self.c_tensor,
                observer_on.c_tensor,
                fake_quant_on.c_tensor,
                running_min.c_tensor,
                running_max.c_tensor,
                scale.c_tensor,
                zero_point.c_tensor,
                averaging_const,
                quant_min,
                quant_max,
                ch_axis,
                if (per_row_fake_quant)  1  else  0,
                if (symmetric_quant)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }, Tensor { .c_tensor = c_tensors[3] }, Tensor { .c_tensor = c_tensors[4] }, Tensor { .c_tensor = c_tensors[5] }};
    }

    pub fn internalFusedMovingAvgObsFqHelperOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, observer_on: *const Tensor, fake_quant_on: *const Tensor, running_min: *const Tensor, running_max: *const Tensor, scale: *const Tensor, zero_point: *const Tensor, averaging_const: f64, quant_min: i64, quant_max: i64, ch_axis: i64, per_row_fake_quant: bool, symmetric_quant: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__fused_moving_avg_obs_fq_helper_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                self.c_tensor,
                observer_on.c_tensor,
                fake_quant_on.c_tensor,
                running_min.c_tensor,
                running_max.c_tensor,
                scale.c_tensor,
                zero_point.c_tensor,
                averaging_const,
                quant_min,
                quant_max,
                ch_axis,
                if (per_row_fake_quant)  1  else  0,
                if (symmetric_quant)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalFusedSdpChoice(
        query: *const Tensor, key: *const Tensor, value: *const Tensor, attn_mask: ?*const Tensor, dropout_p: f64, is_causal: bool, scale: ?f64
    ) i64 {
        const return_ = __c.atg__fused_sdp_choice(query.c_tensor,
                key.c_tensor,
                value.c_tensor,
                if (attn_mask != null) attn_mask.?.c_tensor else null,
                dropout_p,
                if (is_causal)  1  else  0,
                scale orelse std.math.nan, (scale == null));
       torch.readAndCleanError();
        return return_;
    }

    pub fn internalFwPrimal(
        self: *const Tensor, level: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__fw_primal(@ptrCast(&c_tensors), self.c_tensor,
                level);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalFwPrimalCopy(
        self: *const Tensor, level: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__fw_primal_copy(@ptrCast(&c_tensors), self.c_tensor,
                level);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalFwPrimalCopyOut(
        self: *const Tensor, out: *const Tensor, level: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__fw_primal_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                level);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalGatherSparseBackward(
        self: *const Tensor, dim_: i64, index_: *const Tensor, gradient: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__gather_sparse_backward(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                index_.c_tensor,
                gradient.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalGridSampler2dCpuFallback(
        self: *const Tensor, grid: *const Tensor, interpolation_mode: i64, padding_mode: i64, align_corners: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__grid_sampler_2d_cpu_fallback(@ptrCast(&c_tensors), self.c_tensor,
                grid.c_tensor,
                interpolation_mode,
                padding_mode,
                if (align_corners)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalGridSampler2dCpuFallbackBackward(
        self: *const Tensor, grad_output: *const Tensor, grid: *const Tensor, interpolation_mode: i64, padding_mode: i64, align_corners: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__grid_sampler_2d_cpu_fallback_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                grid.c_tensor,
                interpolation_mode,
                padding_mode,
                if (align_corners)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalGridSampler2dCpuFallbackOut(
        self: *const Tensor, out: *const Tensor, grid: *const Tensor, interpolation_mode: i64, padding_mode: i64, align_corners: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__grid_sampler_2d_cpu_fallback_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                grid.c_tensor,
                interpolation_mode,
                padding_mode,
                if (align_corners)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalHasCompatibleShallowCopyType(
        self: *const Tensor, from: *const Tensor
    ) bool {
        const return_ = __c.atg__has_compatible_shallow_copy_type(self.c_tensor,
                from.c_tensor);
       torch.readAndCleanError();
        return return_ != 0;
    }

    pub fn internalHasSameStorageNumel(
        self: *const Tensor, other: *const Tensor
    ) bool {
        const return_ = __c.atg__has_same_storage_numel(self.c_tensor,
                other.c_tensor);
       torch.readAndCleanError();
        return return_ != 0;
    }

    pub fn internalHistogramddBinEdges(
        self: *const Tensor, bins: []i64, range_: []f64, weight: ?*const Tensor, density: bool
    ) []Tensor {
        const c_tensors = __c.atg__histogramdd_bin_edges(self.c_tensor,
                bins.ptr, @intCast(bins.len),
                range_.ptr, @intCast(range_.len),
                if (weight != null) weight.?.c_tensor else null,
                if (density)  1  else  0);
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn internalHistogramddBinEdgesOut(
        self: *const Tensor, out: []*const Tensor, bins: []i64, range_: []f64, weight: ?*const Tensor, density: bool
    ) void {
        __c.atg__histogramdd_bin_edges_out(ptrList(out).ptr, @intCast(out.len),
                self.c_tensor,
                bins.ptr, @intCast(bins.len),
                range_.ptr, @intCast(range_.len),
                if (weight != null) weight.?.c_tensor else null,
                if (density)  1  else  0);
        torch.readAndCleanError();
        return;
    }

    pub fn internalHistogramddFromBinCts(
        self: *const Tensor, bins: []i64, range_: []f64, weight: ?*const Tensor, density: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__histogramdd_from_bin_cts(@ptrCast(&c_tensors), self.c_tensor,
                bins.ptr, @intCast(bins.len),
                range_.ptr, @intCast(range_.len),
                if (weight != null) weight.?.c_tensor else null,
                if (density)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalHistogramddFromBinCtsOut(
        self: *const Tensor, out: *const Tensor, bins: []i64, range_: []f64, weight: ?*const Tensor, density: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__histogramdd_from_bin_cts_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                bins.ptr, @intCast(bins.len),
                range_.ptr, @intCast(range_.len),
                if (weight != null) weight.?.c_tensor else null,
                if (density)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalHistogramddFromBinTensors(
        self: *const Tensor, bins: []*const Tensor, weight: ?*const Tensor, density: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__histogramdd_from_bin_tensors(@ptrCast(&c_tensors), self.c_tensor,
                ptrList(bins).ptr, @intCast(bins.len),
                if (weight != null) weight.?.c_tensor else null,
                if (density)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalHistogramddFromBinTensorsOut(
        self: *const Tensor, out: *const Tensor, bins: []*const Tensor, weight: ?*const Tensor, density: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__histogramdd_from_bin_tensors_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                ptrList(bins).ptr, @intCast(bins.len),
                if (weight != null) weight.?.c_tensor else null,
                if (density)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalIndexPutImpl(
        self: *const Tensor, indices_: []?*const Tensor, values_: *const Tensor, accumulate: bool, unsafe: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__index_put_impl(@ptrCast(&c_tensors), self.c_tensor,
                ptrListOpt(indices_).ptr, @intCast(indices_.len),
                values_.c_tensor,
                if (accumulate)  1  else  0,
                if (unsafe)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalIndexPutImpl_(
        self: *Tensor, indices_: []?*const Tensor, values_: *const Tensor, accumulate: bool, unsafe: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__index_put_impl_(@ptrCast(&c_tensors), self.c_tensor,
                ptrListOpt(indices_).ptr, @intCast(indices_.len),
                values_.c_tensor,
                if (accumulate)  1  else  0,
                if (unsafe)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalIndexPutImplOut(
        self: *const Tensor, out: *const Tensor, indices_: []?*const Tensor, values_: *const Tensor, accumulate: bool, unsafe: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__index_put_impl_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                ptrListOpt(indices_).ptr, @intCast(indices_.len),
                values_.c_tensor,
                if (accumulate)  1  else  0,
                if (unsafe)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalIndices(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__indices(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalIndicesCopy(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__indices_copy(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalIndicesCopyOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__indices_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalIntMm(
        self: *const Tensor, mat2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__int_mm(@ptrCast(&c_tensors), self.c_tensor,
                mat2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalIntMmOut(
        self: *const Tensor, out: *const Tensor, mat2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__int_mm_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                mat2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalIsAllTrue(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__is_all_true(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalIsAnyTrue(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__is_any_true(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalIsZerotensor(
        self: *const Tensor, 
    ) bool {
        const return_ = __c.atg__is_zerotensor(self.c_tensor);
       torch.readAndCleanError();
        return return_ != 0;
    }

    pub fn internalLazyClone(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__lazy_clone(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalLinalgCheckErrors(
        info: *const Tensor, api_name: []const u8, is_matrix: bool
    ) void {
        __c.atg__linalg_check_errors(info.c_tensor,
                api_name.ptr, api_name.len,
                if (is_matrix)  1  else  0);
        torch.readAndCleanError();
        return;
    }

    pub fn internalLinalgDet(
        a: *const Tensor
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg__linalg_det(@ptrCast(&c_tensors), a.c_tensor);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn internalLinalgDetResult(
        result: *const Tensor, lu: *const Tensor, pivots: *const Tensor, a: *const Tensor
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg__linalg_det_result(@ptrCast(&c_tensors), result.c_tensor,
                lu.c_tensor,
                pivots.c_tensor,
                a.c_tensor);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn internalLinalgEigh(
        a: *const Tensor, uplo: []const u8, compute_v: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__linalg_eigh(@ptrCast(&c_tensors), a.c_tensor,
                uplo.ptr, uplo.len,
                if (compute_v)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalLinalgEighEigenvalues(
        eigenvalues: *const Tensor, eigenvectors: *const Tensor, a: *const Tensor, uplo: []const u8, compute_v: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__linalg_eigh_eigenvalues(@ptrCast(&c_tensors), eigenvalues.c_tensor,
                eigenvectors.c_tensor,
                a.c_tensor,
                uplo.ptr, uplo.len,
                if (compute_v)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalLinalgEigvals(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__linalg_eigvals(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalLinalgSlogdet(
        a: *const Tensor
    ) [4]Tensor {
        var c_tensors = [_]C_tensor{null} ** 4;
        __c.atg__linalg_slogdet(@ptrCast(&c_tensors), a.c_tensor);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }, Tensor { .c_tensor = c_tensors[3] }};
    }

    pub fn internalLinalgSlogdetSign(
        sign_: *const Tensor, logabsdet: *const Tensor, lu: *const Tensor, pivots: *const Tensor, a: *const Tensor
    ) [4]Tensor {
        var c_tensors = [_]C_tensor{null} ** 4;
        __c.atg__linalg_slogdet_sign(@ptrCast(&c_tensors), sign_.c_tensor,
                logabsdet.c_tensor,
                lu.c_tensor,
                pivots.c_tensor,
                a.c_tensor);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }, Tensor { .c_tensor = c_tensors[3] }};
    }

    pub fn internalLinalgSolveEx(
        a: *const Tensor, b: *const Tensor, left: bool, check_errors: bool
    ) [4]Tensor {
        var c_tensors = [_]C_tensor{null} ** 4;
        __c.atg__linalg_solve_ex(@ptrCast(&c_tensors), a.c_tensor,
                b.c_tensor,
                if (left)  1  else  0,
                if (check_errors)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }, Tensor { .c_tensor = c_tensors[3] }};
    }

    pub fn internalLinalgSolveExResult(
        result: *const Tensor, lu: *const Tensor, pivots: *const Tensor, info: *const Tensor, a: *const Tensor, b: *const Tensor, left: bool, check_errors: bool
    ) [4]Tensor {
        var c_tensors = [_]C_tensor{null} ** 4;
        __c.atg__linalg_solve_ex_result(@ptrCast(&c_tensors), result.c_tensor,
                lu.c_tensor,
                pivots.c_tensor,
                info.c_tensor,
                a.c_tensor,
                b.c_tensor,
                if (left)  1  else  0,
                if (check_errors)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }, Tensor { .c_tensor = c_tensors[3] }};
    }

    pub fn internalLinalgSvd(
        a: *const Tensor, full_matrices: bool, compute_uv: bool, driver: []const u8
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg__linalg_svd(@ptrCast(&c_tensors), a.c_tensor,
                if (full_matrices)  1  else  0,
                if (compute_uv)  1  else  0,
                driver.ptr, driver.len);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn internalLinalgSvdU(
        u: *const Tensor, s: *const Tensor, vh: *const Tensor, a: *const Tensor, full_matrices: bool, compute_uv: bool, driver: []const u8
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg__linalg_svd_u(@ptrCast(&c_tensors), u.c_tensor,
                s.c_tensor,
                vh.c_tensor,
                a.c_tensor,
                if (full_matrices)  1  else  0,
                if (compute_uv)  1  else  0,
                driver.ptr, driver.len);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn internalLogSoftmax(
        self: *const Tensor, dim_: i64, half_to_float: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__log_softmax(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                if (half_to_float)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalLogSoftmaxBackwardData(
        grad_output: *const Tensor, output: *const Tensor, dim_: i64, input_dtype: Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__log_softmax_backward_data(@ptrCast(&c_tensors), grad_output.c_tensor,
                output.c_tensor,
                dim_,
                input_dtype.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalLogSoftmaxBackwardDataOut(
        out: *const Tensor, grad_output: *const Tensor, output: *const Tensor, dim_: i64, input_dtype: Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__log_softmax_backward_data_out(@ptrCast(&c_tensors), out.c_tensor,
                grad_output.c_tensor,
                output.c_tensor,
                dim_,
                input_dtype.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalLogSoftmaxOut(
        self: *const Tensor, out: *const Tensor, dim_: i64, half_to_float: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__log_softmax_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_,
                if (half_to_float)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalLogcumsumexp(
        self: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__logcumsumexp(@ptrCast(&c_tensors), self.c_tensor,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalLogcumsumexpOut(
        self: *const Tensor, out: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__logcumsumexp_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalLstmMps(
        self: *const Tensor, hx: []*const Tensor, params: []*const Tensor, has_biases: bool, num_layers: i64, dropout_: f64, train: bool, bidirectional: bool, batch_first: bool
    ) [6]Tensor {
        var c_tensors = [_]C_tensor{null} ** 6;
        __c.atg__lstm_mps(@ptrCast(&c_tensors), self.c_tensor,
                ptrList(hx).ptr, @intCast(hx.len),
                ptrList(params).ptr, @intCast(params.len),
                if (has_biases)  1  else  0,
                num_layers,
                dropout_,
                if (train)  1  else  0,
                if (bidirectional)  1  else  0,
                if (batch_first)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }, Tensor { .c_tensor = c_tensors[3] }, Tensor { .c_tensor = c_tensors[4] }, Tensor { .c_tensor = c_tensors[5] }};
    }

    pub fn internalLstmMpsOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, out2: *const Tensor, out3: *const Tensor, out4: *const Tensor, out5: *const Tensor, hx: []*const Tensor, params: []*const Tensor, has_biases: bool, num_layers: i64, dropout_: f64, train: bool, bidirectional: bool, batch_first: bool
    ) [6]Tensor {
        var c_tensors = [_]C_tensor{null} ** 6;
        __c.atg__lstm_mps_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                out2.c_tensor,
                out3.c_tensor,
                out4.c_tensor,
                out5.c_tensor,
                self.c_tensor,
                ptrList(hx).ptr, @intCast(hx.len),
                ptrList(params).ptr, @intCast(params.len),
                if (has_biases)  1  else  0,
                num_layers,
                dropout_,
                if (train)  1  else  0,
                if (bidirectional)  1  else  0,
                if (batch_first)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }, Tensor { .c_tensor = c_tensors[3] }, Tensor { .c_tensor = c_tensors[4] }, Tensor { .c_tensor = c_tensors[5] }};
    }

    pub fn internalLuWithInfo(
        self: *const Tensor, pivot: bool, check_errors: bool
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg__lu_with_info(@ptrCast(&c_tensors), self.c_tensor,
                if (pivot)  1  else  0,
                if (check_errors)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn internalMakeDepToken(
        options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__make_dep_token(@ptrCast(&c_tensors), options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalMakeDual(
        primal: *const Tensor, tangent: *const Tensor, level: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__make_dual(@ptrCast(&c_tensors), primal.c_tensor,
                tangent.c_tensor,
                level);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalMakeDualCopy(
        primal: *const Tensor, tangent: *const Tensor, level: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__make_dual_copy(@ptrCast(&c_tensors), primal.c_tensor,
                tangent.c_tensor,
                level);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalMakeDualCopyOut(
        out: *const Tensor, primal: *const Tensor, tangent: *const Tensor, level: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__make_dual_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                primal.c_tensor,
                tangent.c_tensor,
                level);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalMakePerChannelQuantizedTensor(
        self: *const Tensor, scale: *const Tensor, zero_point: *const Tensor, axis: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__make_per_channel_quantized_tensor(@ptrCast(&c_tensors), self.c_tensor,
                scale.c_tensor,
                zero_point.c_tensor,
                axis);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalMakePerChannelQuantizedTensorOut(
        self: *const Tensor, out: *const Tensor, scale: *const Tensor, zero_point: *const Tensor, axis: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__make_per_channel_quantized_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                scale.c_tensor,
                zero_point.c_tensor,
                axis);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalMakePerTensorQuantizedTensor(
        self: *const Tensor, scale: f64, zero_point: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__make_per_tensor_quantized_tensor(@ptrCast(&c_tensors), self.c_tensor,
                scale,
                zero_point);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalMakePerTensorQuantizedTensorOut(
        self: *const Tensor, out: *const Tensor, scale: f64, zero_point: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__make_per_tensor_quantized_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                scale,
                zero_point);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalMaskedScale(
        self: *const Tensor, mask: *const Tensor, scale: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__masked_scale(@ptrCast(&c_tensors), self.c_tensor,
                mask.c_tensor,
                scale);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalMaskedScaleOut(
        self: *const Tensor, out: *const Tensor, mask: *const Tensor, scale: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__masked_scale_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                mask.c_tensor,
                scale);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalMaskedSoftmax(
        self: *const Tensor, mask: *const Tensor, dim_: ?i64, mask_type: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__masked_softmax(@ptrCast(&c_tensors), self.c_tensor,
                mask.c_tensor,
                dim_ orelse 0, (dim_ == null),
                mask_type orelse 0, (mask_type == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalMaskedSoftmaxBackward(
        grad_output: *const Tensor, output: *const Tensor, mask: *const Tensor, dim_: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__masked_softmax_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                output.c_tensor,
                mask.c_tensor,
                dim_ orelse 0, (dim_ == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalMaskedSoftmaxBackwardOut(
        out: *const Tensor, grad_output: *const Tensor, output: *const Tensor, mask: *const Tensor, dim_: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__masked_softmax_backward_out(@ptrCast(&c_tensors), out.c_tensor,
                grad_output.c_tensor,
                output.c_tensor,
                mask.c_tensor,
                dim_ orelse 0, (dim_ == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalMaskedSoftmaxOut(
        self: *const Tensor, out: *const Tensor, mask: *const Tensor, dim_: ?i64, mask_type: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__masked_softmax_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                mask.c_tensor,
                dim_ orelse 0, (dim_ == null),
                mask_type orelse 0, (mask_type == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalMixedDtypesLinear(
        self: *const Tensor, weight: *const Tensor, scale: *const Tensor, bias: ?*const Tensor, activation: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__mixed_dtypes_linear(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                scale.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                activation.ptr, activation.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalMkldnnReshape(
        self: *const Tensor, shape: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__mkldnn_reshape(@ptrCast(&c_tensors), self.c_tensor,
                shape.ptr, @intCast(shape.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalMkldnnReshapeOut(
        self: *const Tensor, out: *const Tensor, shape: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__mkldnn_reshape_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                shape.ptr, @intCast(shape.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalMkldnnTranspose(
        self: *const Tensor, dim0: i64, dim1: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__mkldnn_transpose(@ptrCast(&c_tensors), self.c_tensor,
                dim0,
                dim1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalMkldnnTranspose_(
        self: *Tensor, dim0: i64, dim1: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__mkldnn_transpose_(@ptrCast(&c_tensors), self.c_tensor,
                dim0,
                dim1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalMkldnnTransposeOut(
        self: *const Tensor, out: *const Tensor, dim0: i64, dim1: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__mkldnn_transpose_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim0,
                dim1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalMpsConvolution(
        self: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, padding: []i64, stride_: []i64, dilation: []i64, groups: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__mps_convolution(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                padding.ptr, @intCast(padding.len),
                stride_.ptr, @intCast(stride_.len),
                dilation.ptr, @intCast(dilation.len),
                groups);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalMpsConvolutionOut(
        self: *const Tensor, out: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, padding: []i64, stride_: []i64, dilation: []i64, groups: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__mps_convolution_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                padding.ptr, @intCast(padding.len),
                stride_.ptr, @intCast(stride_.len),
                dilation.ptr, @intCast(dilation.len),
                groups);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalMpsConvolutionTranspose(
        self: *const Tensor, weight: *const Tensor, padding: []i64, output_padding: []i64, stride_: []i64, dilation: []i64, groups: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__mps_convolution_transpose(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                padding.ptr, @intCast(padding.len),
                output_padding.ptr, @intCast(output_padding.len),
                stride_.ptr, @intCast(stride_.len),
                dilation.ptr, @intCast(dilation.len),
                groups);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalMpsConvolutionTransposeOut(
        self: *const Tensor, out: *const Tensor, weight: *const Tensor, padding: []i64, output_padding: []i64, stride_: []i64, dilation: []i64, groups: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__mps_convolution_transpose_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                weight.c_tensor,
                padding.ptr, @intCast(padding.len),
                output_padding.ptr, @intCast(output_padding.len),
                stride_.ptr, @intCast(stride_.len),
                dilation.ptr, @intCast(dilation.len),
                groups);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalNativeBatchNormLegit(
        self: *const Tensor, weight: ?*const Tensor, bias: ?*const Tensor, running_mean: *const Tensor, running_var: *const Tensor, training: bool, momentum: f64, eps: f64
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg__native_batch_norm_legit(@ptrCast(&c_tensors), self.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                if (bias != null) bias.?.c_tensor else null,
                running_mean.c_tensor,
                running_var.c_tensor,
                if (training)  1  else  0,
                momentum,
                eps);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn internalNativeBatchNormLegitFunctional(
        self: *const Tensor, weight: ?*const Tensor, bias: ?*const Tensor, running_mean: *const Tensor, running_var: *const Tensor, training: bool, momentum: f64, eps: f64
    ) [5]Tensor {
        var c_tensors = [_]C_tensor{null} ** 5;
        __c.atg__native_batch_norm_legit_functional(@ptrCast(&c_tensors), self.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                if (bias != null) bias.?.c_tensor else null,
                running_mean.c_tensor,
                running_var.c_tensor,
                if (training)  1  else  0,
                momentum,
                eps);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }, Tensor { .c_tensor = c_tensors[3] }, Tensor { .c_tensor = c_tensors[4] }};
    }

    pub fn internalNativeBatchNormLegitNoStats(
        self: *const Tensor, weight: ?*const Tensor, bias: ?*const Tensor, training: bool, momentum: f64, eps: f64
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg__native_batch_norm_legit_no_stats(@ptrCast(&c_tensors), self.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                if (bias != null) bias.?.c_tensor else null,
                if (training)  1  else  0,
                momentum,
                eps);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn internalNativeBatchNormLegitNoStatsOut(
        self: *const Tensor, out: *const Tensor, save_mean: *const Tensor, save_invstd: *const Tensor, weight: ?*const Tensor, bias: ?*const Tensor, training: bool, momentum: f64, eps: f64
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg__native_batch_norm_legit_no_stats_out(@ptrCast(&c_tensors), out.c_tensor,
                save_mean.c_tensor,
                save_invstd.c_tensor,
                self.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                if (bias != null) bias.?.c_tensor else null,
                if (training)  1  else  0,
                momentum,
                eps);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn internalNativeBatchNormLegitNoTraining(
        self: *const Tensor, weight: ?*const Tensor, bias: ?*const Tensor, running_mean: *const Tensor, running_var: *const Tensor, momentum: f64, eps: f64
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg__native_batch_norm_legit_no_training(@ptrCast(&c_tensors), self.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                if (bias != null) bias.?.c_tensor else null,
                running_mean.c_tensor,
                running_var.c_tensor,
                momentum,
                eps);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn internalNativeBatchNormLegitNoTrainingOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, out2: *const Tensor, weight: ?*const Tensor, bias: ?*const Tensor, running_mean: *const Tensor, running_var: *const Tensor, momentum: f64, eps: f64
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg__native_batch_norm_legit_no_training_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                out2.c_tensor,
                self.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                if (bias != null) bias.?.c_tensor else null,
                running_mean.c_tensor,
                running_var.c_tensor,
                momentum,
                eps);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn internalNativeBatchNormLegitOut(
        self: *const Tensor, out: *const Tensor, save_mean: *const Tensor, save_invstd: *const Tensor, weight: ?*const Tensor, bias: ?*const Tensor, running_mean: *const Tensor, running_var: *const Tensor, training: bool, momentum: f64, eps: f64
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg__native_batch_norm_legit_out(@ptrCast(&c_tensors), out.c_tensor,
                save_mean.c_tensor,
                save_invstd.c_tensor,
                self.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                if (bias != null) bias.?.c_tensor else null,
                running_mean.c_tensor,
                running_var.c_tensor,
                if (training)  1  else  0,
                momentum,
                eps);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn internalNativeMultiHeadAttention(
        query: *const Tensor, key: *const Tensor, value: *const Tensor, embed_dim: i64, num_head: i64, qkv_weight: *const Tensor, qkv_bias: *const Tensor, proj_weight: *const Tensor, proj_bias: *const Tensor, mask: ?*const Tensor, need_weights: bool, average_attn_weights: bool, mask_type: ?i64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__native_multi_head_attention(@ptrCast(&c_tensors), query.c_tensor,
                key.c_tensor,
                value.c_tensor,
                embed_dim,
                num_head,
                qkv_weight.c_tensor,
                qkv_bias.c_tensor,
                proj_weight.c_tensor,
                proj_bias.c_tensor,
                if (mask != null) mask.?.c_tensor else null,
                if (need_weights)  1  else  0,
                if (average_attn_weights)  1  else  0,
                mask_type orelse 0, (mask_type == null));
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalNativeMultiHeadAttentionOut(
        out0: *const Tensor, out1: *const Tensor, query: *const Tensor, key: *const Tensor, value: *const Tensor, embed_dim: i64, num_head: i64, qkv_weight: *const Tensor, qkv_bias: *const Tensor, proj_weight: *const Tensor, proj_bias: *const Tensor, mask: ?*const Tensor, need_weights: bool, average_attn_weights: bool, mask_type: ?i64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__native_multi_head_attention_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                query.c_tensor,
                key.c_tensor,
                value.c_tensor,
                embed_dim,
                num_head,
                qkv_weight.c_tensor,
                qkv_bias.c_tensor,
                proj_weight.c_tensor,
                proj_bias.c_tensor,
                if (mask != null) mask.?.c_tensor else null,
                if (need_weights)  1  else  0,
                if (average_attn_weights)  1  else  0,
                mask_type orelse 0, (mask_type == null));
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalNegView(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__neg_view(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalNegViewCopy(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__neg_view_copy(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalNegViewCopyOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__neg_view_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalNestedFromPadded(
        padded: *const Tensor, cpu_nested_shape_example: *const Tensor, fuse_transform_0213: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__nested_from_padded(@ptrCast(&c_tensors), padded.c_tensor,
                cpu_nested_shape_example.c_tensor,
                if (fuse_transform_0213)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalNestedFromPaddedAndNestedExample(
        padded: *const Tensor, nt_example: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__nested_from_padded_and_nested_example(@ptrCast(&c_tensors), padded.c_tensor,
                nt_example.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalNestedFromPaddedAndNestedExampleOut(
        out: *const Tensor, padded: *const Tensor, nt_example: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__nested_from_padded_and_nested_example_out(@ptrCast(&c_tensors), out.c_tensor,
                padded.c_tensor,
                nt_example.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalNestedFromPaddedOut(
        out: *const Tensor, padded: *const Tensor, cpu_nested_shape_example: *const Tensor, fuse_transform_0213: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__nested_from_padded_out(@ptrCast(&c_tensors), out.c_tensor,
                padded.c_tensor,
                cpu_nested_shape_example.c_tensor,
                if (fuse_transform_0213)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalNestedGetJaggedDummy(
        any_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__nested_get_jagged_dummy(@ptrCast(&c_tensors), any_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalNestedGetLengths(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__nested_get_lengths(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalNestedGetOffsets(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__nested_get_offsets(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalNestedGetRaggedIdx(
        self: *const Tensor, 
    ) i64 {
        const return_ = __c.atg__nested_get_ragged_idx(self.c_tensor);
       torch.readAndCleanError();
        return return_;
    }

    pub fn internalNestedGetValues(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__nested_get_values(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalNestedGetValuesCopy(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__nested_get_values_copy(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalNestedGetValuesCopyOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__nested_get_values_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalNestedSelectBackward(
        self: *const Tensor, grad_output: *const Tensor, dim_: i64, index_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__nested_select_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                dim_,
                index_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalNestedSumBackward(
        self: *const Tensor, gradient: *const Tensor, dim_: ?[]i64, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__nested_sum_backward(@ptrCast(&c_tensors), gradient.c_tensor,
                self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalNestedViewFromBuffer(
        self: *const Tensor, nested_size: *const Tensor, nested_strides: *const Tensor, offsets: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__nested_view_from_buffer(@ptrCast(&c_tensors), self.c_tensor,
                nested_size.c_tensor,
                nested_strides.c_tensor,
                offsets.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalNestedViewFromBufferCopy(
        self: *const Tensor, nested_size: *const Tensor, nested_strides: *const Tensor, offsets: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__nested_view_from_buffer_copy(@ptrCast(&c_tensors), self.c_tensor,
                nested_size.c_tensor,
                nested_strides.c_tensor,
                offsets.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalNestedViewFromBufferCopyOut(
        self: *const Tensor, out: *const Tensor, nested_size: *const Tensor, nested_strides: *const Tensor, offsets: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__nested_view_from_buffer_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                nested_size.c_tensor,
                nested_strides.c_tensor,
                offsets.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalNestedViewFromJagged(
        self: *const Tensor, offsets: *const Tensor, dummy: *const Tensor, lengths: ?*const Tensor, ragged_idx: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__nested_view_from_jagged(@ptrCast(&c_tensors), self.c_tensor,
                offsets.c_tensor,
                dummy.c_tensor,
                if (lengths != null) lengths.?.c_tensor else null,
                ragged_idx);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalNestedViewFromJaggedCopy(
        self: *const Tensor, offsets: *const Tensor, dummy: *const Tensor, lengths: ?*const Tensor, ragged_idx: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__nested_view_from_jagged_copy(@ptrCast(&c_tensors), self.c_tensor,
                offsets.c_tensor,
                dummy.c_tensor,
                if (lengths != null) lengths.?.c_tensor else null,
                ragged_idx);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalNestedViewFromJaggedCopyOut(
        self: *const Tensor, out: *const Tensor, offsets: *const Tensor, dummy: *const Tensor, lengths: ?*const Tensor, ragged_idx: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__nested_view_from_jagged_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                offsets.c_tensor,
                dummy.c_tensor,
                if (lengths != null) lengths.?.c_tensor else null,
                ragged_idx);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalNewZerosWithSameFeatureMeta(
        self: *const Tensor, other: *const Tensor, self_num_batch_dims: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__new_zeros_with_same_feature_meta(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor,
                self_num_batch_dims);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalNewZerosWithSameFeatureMetaOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor, self_num_batch_dims: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__new_zeros_with_same_feature_meta_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor,
                self_num_batch_dims);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalNnpackAvailable(
        
    ) bool {
        const return_ = __c.atg__nnpack_available();
       torch.readAndCleanError();
        return return_ != 0;
    }

    pub fn internalNnpackSpatialConvolution(
        self: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, padding: []i64, stride_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__nnpack_spatial_convolution(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                padding.ptr, @intCast(padding.len),
                stride_.ptr, @intCast(stride_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalNnpackSpatialConvolutionOut(
        self: *const Tensor, out: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, padding: []i64, stride_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__nnpack_spatial_convolution_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                padding.ptr, @intCast(padding.len),
                stride_.ptr, @intCast(stride_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalNnz(
        self: *const Tensor, 
    ) i64 {
        const return_ = __c.atg__nnz(self.c_tensor);
       torch.readAndCleanError();
        return return_;
    }

    pub fn internalPackPaddedSequence(
        self: *const Tensor, lengths: *const Tensor, batch_first: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__pack_padded_sequence(@ptrCast(&c_tensors), self.c_tensor,
                lengths.c_tensor,
                if (batch_first)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalPackPaddedSequenceBackward(
        gradient: *const Tensor, input_size: []i64, batch_sizes: *const Tensor, batch_first: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__pack_padded_sequence_backward(@ptrCast(&c_tensors), gradient.c_tensor,
                input_size.ptr, @intCast(input_size.len),
                batch_sizes.c_tensor,
                if (batch_first)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalPackPaddedSequenceOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, lengths: *const Tensor, batch_first: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__pack_padded_sequence_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                self.c_tensor,
                lengths.c_tensor,
                if (batch_first)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalPadCircular(
        self: *const Tensor, padding: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__pad_circular(@ptrCast(&c_tensors), self.c_tensor,
                padding.ptr, @intCast(padding.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalPadEnum(
        self: *const Tensor, padding: []i64, mode_: i64, value: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__pad_enum(@ptrCast(&c_tensors), self.c_tensor,
                padding.ptr, @intCast(padding.len),
                mode_,
                value orelse std.math.nan, (value == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalPadPackedSequence(
        data_: *const Tensor, batch_sizes: *const Tensor, batch_first: bool, padding_value: Scalar, total_length: i64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__pad_packed_sequence(@ptrCast(&c_tensors), data_.c_tensor,
                batch_sizes.c_tensor,
                if (batch_first)  1  else  0,
                padding_value.into().c_scalar,
                total_length);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalPdistBackward(
        self: *const Tensor, gradient: *const Tensor, p: f64, pdist_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__pdist_backward(@ptrCast(&c_tensors), gradient.c_tensor,
                self.c_tensor,
                p,
                pdist_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalPdistBackwardOut(
        self: *const Tensor, out: *const Tensor, gradient: *const Tensor, p: f64, pdist_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__pdist_backward_out(@ptrCast(&c_tensors), out.c_tensor,
                gradient.c_tensor,
                self.c_tensor,
                p,
                pdist_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalPinMemory(
        self: *const Tensor, device_: Device
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__pin_memory(@ptrCast(&c_tensors), self.c_tensor,
                device_.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalPinMemoryOut(
        self: *const Tensor, out: *const Tensor, device_: Device
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__pin_memory_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                device_.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalPreluKernel(
        self: *const Tensor, weight: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__prelu_kernel(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalPreluKernelBackward(
        self: *const Tensor, grad_output: *const Tensor, weight: *const Tensor
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__prelu_kernel_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                weight.c_tensor);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalPrint(
        s: []const u8
    ) void {
        __c.atg__print(s.ptr, s.len);
        torch.readAndCleanError();
        return;
    }

    pub fn internalPropagateXlaData(
        self: *const Tensor, output: *const Tensor
    ) void {
        __c.atg__propagate_xla_data(self.c_tensor,
                output.c_tensor);
        torch.readAndCleanError();
        return;
    }

    pub fn internalRemoveBatchDim(
        self: *const Tensor, level: i64, batch_size: i64, out_dim: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__remove_batch_dim(@ptrCast(&c_tensors), self.c_tensor,
                level,
                batch_size,
                out_dim);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalReshapeAlias(
        self: *const Tensor, size_: []i64, stride_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__reshape_alias(@ptrCast(&c_tensors), self.c_tensor,
                size_.ptr, @intCast(size_.len),
                stride_.ptr, @intCast(stride_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalReshapeAliasCopy(
        self: *const Tensor, size_: []i64, stride_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__reshape_alias_copy(@ptrCast(&c_tensors), self.c_tensor,
                size_.ptr, @intCast(size_.len),
                stride_.ptr, @intCast(stride_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalReshapeAliasCopyOut(
        self: *const Tensor, out: *const Tensor, size_: []i64, stride_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__reshape_alias_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                size_.ptr, @intCast(size_.len),
                stride_.ptr, @intCast(stride_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalReshapeCopy(
        self: *const Tensor, size_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__reshape_copy(@ptrCast(&c_tensors), self.c_tensor,
                size_.ptr, @intCast(size_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalReshapeFromTensor(
        self: *const Tensor, shape: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__reshape_from_tensor(@ptrCast(&c_tensors), self.c_tensor,
                shape.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalResizeOutput(
        self: *const Tensor, size_: []i64, device_: Device
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__resize_output(@ptrCast(&c_tensors), self.c_tensor,
                size_.ptr, @intCast(size_.len),
                device_.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalResizeOutput_(
        self: *Tensor, size_: []i64, device_: Device
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__resize_output_(@ptrCast(&c_tensors), self.c_tensor,
                size_.ptr, @intCast(size_.len),
                device_.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalResizeOutputOut(
        self: *const Tensor, out: *const Tensor, size_: []i64, device_: Device
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__resize_output_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                size_.ptr, @intCast(size_.len),
                device_.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalRowwisePrune(
        weight: *const Tensor, mask: *const Tensor, compressed_indices_dtype: Kind
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__rowwise_prune(@ptrCast(&c_tensors), weight.c_tensor,
                mask.c_tensor,
                compressed_indices_dtype.cInt());
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalSampleDirichlet(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sample_dirichlet(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSampleDirichletOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sample_dirichlet_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSaturateWeightToFp16(
        weight: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__saturate_weight_to_fp16(@ptrCast(&c_tensors), weight.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalScaledDotProductAttentionMath(
        query: *const Tensor, key: *const Tensor, value: *const Tensor, attn_mask: ?*const Tensor, dropout_p: f64, is_causal: bool, dropout_mask: ?*const Tensor, scale: ?f64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__scaled_dot_product_attention_math(@ptrCast(&c_tensors), query.c_tensor,
                key.c_tensor,
                value.c_tensor,
                if (attn_mask != null) attn_mask.?.c_tensor else null,
                dropout_p,
                if (is_causal)  1  else  0,
                if (dropout_mask != null) dropout_mask.?.c_tensor else null,
                scale orelse std.math.nan, (scale == null));
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalScaledDotProductCudnnAttention(
        query: *const Tensor, key: *const Tensor, value: *const Tensor, dropout_p: f64, is_causal: bool, return_debug_mask: bool, scale: ?f64
    ) [4]Tensor {
        var c_tensors = [_]C_tensor{null} ** 4;
        __c.atg__scaled_dot_product_cudnn_attention(@ptrCast(&c_tensors), query.c_tensor,
                key.c_tensor,
                value.c_tensor,
                dropout_p,
                if (is_causal)  1  else  0,
                if (return_debug_mask)  1  else  0,
                scale orelse std.math.nan, (scale == null));
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }, Tensor { .c_tensor = c_tensors[3] }};
    }

    pub fn internalScaledDotProductEfficientAttention(
        query: *const Tensor, key: *const Tensor, value: *const Tensor, attn_bias: ?*const Tensor, compute_log_sumexp: bool, dropout_p: f64, is_causal: bool, scale: ?f64
    ) [4]Tensor {
        var c_tensors = [_]C_tensor{null} ** 4;
        __c.atg__scaled_dot_product_efficient_attention(@ptrCast(&c_tensors), query.c_tensor,
                key.c_tensor,
                value.c_tensor,
                if (attn_bias != null) attn_bias.?.c_tensor else null,
                if (compute_log_sumexp)  1  else  0,
                dropout_p,
                if (is_causal)  1  else  0,
                scale orelse std.math.nan, (scale == null));
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }, Tensor { .c_tensor = c_tensors[3] }};
    }

    pub fn internalScaledDotProductFlashAttentionBackward(
        grad_out: *const Tensor, query: *const Tensor, key: *const Tensor, value: *const Tensor, out: *const Tensor, logsumexp_: *const Tensor, cum_seq_q: *const Tensor, cum_seq_k: *const Tensor, max_q: i64, max_k: i64, dropout_p: f64, is_causal: bool, philox_seed: *const Tensor, philox_offset: *const Tensor, scale: ?f64
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg__scaled_dot_product_flash_attention_backward(@ptrCast(&c_tensors), grad_out.c_tensor,
                query.c_tensor,
                key.c_tensor,
                value.c_tensor,
                out.c_tensor,
                logsumexp_.c_tensor,
                cum_seq_q.c_tensor,
                cum_seq_k.c_tensor,
                max_q,
                max_k,
                dropout_p,
                if (is_causal)  1  else  0,
                philox_seed.c_tensor,
                philox_offset.c_tensor,
                scale orelse std.math.nan, (scale == null));
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn internalScaledDotProductFlashAttentionForCpu(
        query: *const Tensor, key: *const Tensor, value: *const Tensor, dropout_p: f64, is_causal: bool, attn_mask: ?*const Tensor, scale: ?f64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__scaled_dot_product_flash_attention_for_cpu(@ptrCast(&c_tensors), query.c_tensor,
                key.c_tensor,
                value.c_tensor,
                dropout_p,
                if (is_causal)  1  else  0,
                if (attn_mask != null) attn_mask.?.c_tensor else null,
                scale orelse std.math.nan, (scale == null));
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalScaledDotProductFlashAttentionForCpuBackward(
        grad_out: *const Tensor, query: *const Tensor, key: *const Tensor, value: *const Tensor, out: *const Tensor, logsumexp_: *const Tensor, dropout_p: f64, is_causal: bool, attn_mask: ?*const Tensor, scale: ?f64
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg__scaled_dot_product_flash_attention_for_cpu_backward(@ptrCast(&c_tensors), grad_out.c_tensor,
                query.c_tensor,
                key.c_tensor,
                value.c_tensor,
                out.c_tensor,
                logsumexp_.c_tensor,
                dropout_p,
                if (is_causal)  1  else  0,
                if (attn_mask != null) attn_mask.?.c_tensor else null,
                scale orelse std.math.nan, (scale == null));
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn internalScaledMm(
        self: *const Tensor, mat2: *const Tensor, bias: ?*const Tensor, out_dtype: ?Kind, scale_a: ?*const Tensor, scale_b: ?*const Tensor, scale_result: ?*const Tensor, use_fast_accum: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__scaled_mm(@ptrCast(&c_tensors), self.c_tensor,
                mat2.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                out_dtype orelse -1,
                if (scale_a != null) scale_a.?.c_tensor else null,
                if (scale_b != null) scale_b.?.c_tensor else null,
                if (scale_result != null) scale_result.?.c_tensor else null,
                if (use_fast_accum)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalScaledMmOut(
        self: *const Tensor, out: *const Tensor, out_amax: *const Tensor, mat2: *const Tensor, bias: ?*const Tensor, out_dtype: ?Kind, scale_a: ?*const Tensor, scale_b: ?*const Tensor, scale_result: ?*const Tensor, use_fast_accum: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__scaled_mm_out(@ptrCast(&c_tensors), out.c_tensor,
                out_amax.c_tensor,
                self.c_tensor,
                mat2.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                out_dtype orelse -1,
                if (scale_a != null) scale_a.?.c_tensor else null,
                if (scale_b != null) scale_b.?.c_tensor else null,
                if (scale_result != null) scale_result.?.c_tensor else null,
                if (use_fast_accum)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalScatterReduce(
        self: *const Tensor, dim_: i64, index_: *const Tensor, src: *const Tensor, reduce: []const u8, include_self: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__scatter_reduce(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                index_.c_tensor,
                src.c_tensor,
                reduce.ptr, reduce.len,
                if (include_self)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalScatterReduce_(
        self: *Tensor, dim_: i64, index_: *const Tensor, src: *const Tensor, reduce: []const u8, include_self: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__scatter_reduce_(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                index_.c_tensor,
                src.c_tensor,
                reduce.ptr, reduce.len,
                if (include_self)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalScatterReduceTwoOut(
        self: *const Tensor, out: *const Tensor, dim_: i64, index_: *const Tensor, src: *const Tensor, reduce: []const u8, include_self: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__scatter_reduce_two_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_,
                index_.c_tensor,
                src.c_tensor,
                reduce.ptr, reduce.len,
                if (include_self)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSegmentReduceBackward(
        gradient: *const Tensor, output: *const Tensor, data_: *const Tensor, reduce: []const u8, lengths: ?*const Tensor, offsets: ?*const Tensor, axis: i64, initial: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__segment_reduce_backward(@ptrCast(&c_tensors), gradient.c_tensor,
                output.c_tensor,
                data_.c_tensor,
                reduce.ptr, reduce.len,
                if (lengths != null) lengths.?.c_tensor else null,
                if (offsets != null) offsets.?.c_tensor else null,
                axis,
                initial.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSegmentReduceBackwardOut(
        out: *const Tensor, gradient: *const Tensor, output: *const Tensor, data_: *const Tensor, reduce: []const u8, lengths: ?*const Tensor, offsets: ?*const Tensor, axis: i64, initial: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__segment_reduce_backward_out(@ptrCast(&c_tensors), out.c_tensor,
                gradient.c_tensor,
                output.c_tensor,
                data_.c_tensor,
                reduce.ptr, reduce.len,
                if (lengths != null) lengths.?.c_tensor else null,
                if (offsets != null) offsets.?.c_tensor else null,
                axis,
                initial.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalShapeAsTensor(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__shape_as_tensor(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSlowConv2dBackward(
        self: *const Tensor, grad_input: *const Tensor, grad_weight: *const Tensor, grad_bias: *const Tensor, grad_output: *const Tensor, weight: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg__slow_conv2d_backward(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_weight.c_tensor,
                grad_bias.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                weight.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len));
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn internalSobolEngineDraw(
        quasi: *const Tensor, n: i64, sobolstate: *const Tensor, dimension: i64, num_generated: i64, dtype: ?Kind
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__sobol_engine_draw(@ptrCast(&c_tensors), quasi.c_tensor,
                n,
                sobolstate.c_tensor,
                dimension,
                num_generated,
                dtype orelse -1);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalSobolEngineFf_(
        self: *Tensor, n: i64, sobolstate: *const Tensor, dimension: i64, num_generated: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sobol_engine_ff_(@ptrCast(&c_tensors), self.c_tensor,
                n,
                sobolstate.c_tensor,
                dimension,
                num_generated);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSobolEngineInitializeState_(
        self: *Tensor, dimension: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sobol_engine_initialize_state_(@ptrCast(&c_tensors), self.c_tensor,
                dimension);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSobolEngineScramble_(
        self: *Tensor, ltm: *const Tensor, dimension: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sobol_engine_scramble_(@ptrCast(&c_tensors), self.c_tensor,
                ltm.c_tensor,
                dimension);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSoftmax(
        self: *const Tensor, dim_: i64, half_to_float: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__softmax(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                if (half_to_float)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSoftmaxBackwardData(
        grad_output: *const Tensor, output: *const Tensor, dim_: i64, input_dtype: Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__softmax_backward_data(@ptrCast(&c_tensors), grad_output.c_tensor,
                output.c_tensor,
                dim_,
                input_dtype.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSoftmaxBackwardDataOut(
        grad_input: *const Tensor, grad_output: *const Tensor, output: *const Tensor, dim_: i64, input_dtype: Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__softmax_backward_data_out(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                output.c_tensor,
                dim_,
                input_dtype.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSoftmaxOut(
        self: *const Tensor, out: *const Tensor, dim_: i64, half_to_float: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__softmax_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_,
                if (half_to_float)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseAddmm(
        self: *const Tensor, mat1: *const Tensor, mat2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_addmm(@ptrCast(&c_tensors), self.c_tensor,
                mat1.c_tensor,
                mat2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseAddmmOut(
        self: *const Tensor, out: *const Tensor, mat1: *const Tensor, mat2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_addmm_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                mat1.c_tensor,
                mat2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseBroadcastTo(
        self: *const Tensor, size_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_broadcast_to(@ptrCast(&c_tensors), self.c_tensor,
                size_.ptr, @intCast(size_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseBroadcastToCopy(
        self: *const Tensor, size_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_broadcast_to_copy(@ptrCast(&c_tensors), self.c_tensor,
                size_.ptr, @intCast(size_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseBroadcastToCopyOut(
        self: *const Tensor, out: *const Tensor, size_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_broadcast_to_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                size_.ptr, @intCast(size_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseBscTensorUnsafe(
        ccol_indices_: *const Tensor, row_indices_: *const Tensor, values_: *const Tensor, size_: []i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_bsc_tensor_unsafe(@ptrCast(&c_tensors), ccol_indices_.c_tensor,
                row_indices_.c_tensor,
                values_.c_tensor,
                size_.ptr, @intCast(size_.len),
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseBsrTensorUnsafe(
        crow_indices_: *const Tensor, col_indices_: *const Tensor, values_: *const Tensor, size_: []i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_bsr_tensor_unsafe(@ptrCast(&c_tensors), crow_indices_.c_tensor,
                col_indices_.c_tensor,
                values_.c_tensor,
                size_.ptr, @intCast(size_.len),
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseCompressedTensorUnsafe(
        compressed_indices: *const Tensor, plain_indices: *const Tensor, values_: *const Tensor, size_: []i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_compressed_tensor_unsafe(@ptrCast(&c_tensors), compressed_indices.c_tensor,
                plain_indices.c_tensor,
                values_.c_tensor,
                size_.ptr, @intCast(size_.len),
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseCooTensorUnsafe(
        indices_: *const Tensor, values_: *const Tensor, size_: []i64, options_: TensorOptions, is_coalesced_: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_coo_tensor_unsafe(@ptrCast(&c_tensors), indices_.c_tensor,
                values_.c_tensor,
                size_.ptr, @intCast(size_.len),
                options_.kind.cInt(), options_.device.cInt(),
                if (is_coalesced_)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseCooTensorWithDims(
        sparse_dim_: i64, dense_dim_: i64, size_: []i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_coo_tensor_with_dims(@ptrCast(&c_tensors), sparse_dim_,
                dense_dim_,
                size_.ptr, @intCast(size_.len),
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseCooTensorWithDimsAndTensors(
        sparse_dim_: i64, dense_dim_: i64, size_: []i64, indices_: *const Tensor, values_: *const Tensor, options_: TensorOptions, is_coalesced_: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_coo_tensor_with_dims_and_tensors(@ptrCast(&c_tensors), sparse_dim_,
                dense_dim_,
                size_.ptr, @intCast(size_.len),
                indices_.c_tensor,
                values_.c_tensor,
                options_.kind.cInt(), options_.device.cInt(),
                if (is_coalesced_)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseCooTensorWithDimsAndTensorsOut(
        out: *const Tensor, sparse_dim_: i64, dense_dim_: i64, size_: []i64, indices_: *const Tensor, values_: *const Tensor, is_coalesced_: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_coo_tensor_with_dims_and_tensors_out(@ptrCast(&c_tensors), out.c_tensor,
                sparse_dim_,
                dense_dim_,
                size_.ptr, @intCast(size_.len),
                indices_.c_tensor,
                values_.c_tensor,
                if (is_coalesced_)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseCooTensorWithDimsOut(
        out: *const Tensor, sparse_dim_: i64, dense_dim_: i64, size_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_coo_tensor_with_dims_out(@ptrCast(&c_tensors), out.c_tensor,
                sparse_dim_,
                dense_dim_,
                size_.ptr, @intCast(size_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseCscTensorUnsafe(
        ccol_indices_: *const Tensor, row_indices_: *const Tensor, values_: *const Tensor, size_: []i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_csc_tensor_unsafe(@ptrCast(&c_tensors), ccol_indices_.c_tensor,
                row_indices_.c_tensor,
                values_.c_tensor,
                size_.ptr, @intCast(size_.len),
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseCsrProd(
        self: *const Tensor, dim_: []i64, keepdim: bool, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_csr_prod(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseCsrProdDimDtypeOut(
        self: *const Tensor, out: *const Tensor, dim_: []i64, keepdim: bool, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_csr_prod_dim_dtype_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseCsrSum(
        self: *const Tensor, dim_: []i64, keepdim: bool, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_csr_sum(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseCsrSumDimDtypeOut(
        self: *const Tensor, out: *const Tensor, dim_: []i64, keepdim: bool, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_csr_sum_dim_dtype_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseCsrTensorUnsafe(
        crow_indices_: *const Tensor, col_indices_: *const Tensor, values_: *const Tensor, size_: []i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_csr_tensor_unsafe(@ptrCast(&c_tensors), crow_indices_.c_tensor,
                col_indices_.c_tensor,
                values_.c_tensor,
                size_.ptr, @intCast(size_.len),
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseLogSoftmax(
        self: *const Tensor, dim_: i64, half_to_float: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_log_softmax(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                if (half_to_float)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseLogSoftmaxBackwardData(
        self: *const Tensor, grad_output: *const Tensor, output: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_log_softmax_backward_data(@ptrCast(&c_tensors), grad_output.c_tensor,
                output.c_tensor,
                dim_,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseLogSoftmaxBackwardDataOut(
        self: *const Tensor, out: *const Tensor, grad_output: *const Tensor, output: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_log_softmax_backward_data_out(@ptrCast(&c_tensors), out.c_tensor,
                grad_output.c_tensor,
                output.c_tensor,
                dim_,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseLogSoftmaxInt(
        self: *const Tensor, dim_: i64, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_log_softmax_int(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseLogSoftmaxOut(
        self: *const Tensor, out: *const Tensor, dim_: i64, half_to_float: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_log_softmax_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_,
                if (half_to_float)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseMaskProjection(
        self: *const Tensor, mask: *const Tensor, accumulate_matches: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_mask_projection(@ptrCast(&c_tensors), self.c_tensor,
                mask.c_tensor,
                if (accumulate_matches)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseMaskProjectionOut(
        self: *const Tensor, out: *const Tensor, mask: *const Tensor, accumulate_matches: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_mask_projection_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                mask.c_tensor,
                if (accumulate_matches)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseMm(
        sparse: *const Tensor, dense: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_mm(@ptrCast(&c_tensors), sparse.c_tensor,
                dense.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseMmReduce(
        sparse: *const Tensor, dense: *const Tensor, reduce: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_mm_reduce(@ptrCast(&c_tensors), sparse.c_tensor,
                dense.c_tensor,
                reduce.ptr, reduce.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseMmReduceImpl(
        self: *const Tensor, other: *const Tensor, reduce: []const u8
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__sparse_mm_reduce_impl(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor,
                reduce.ptr, reduce.len);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalSparseSemiStructuredLinear(
        self: *const Tensor, weight: *const Tensor, meta: *const Tensor, bias: ?*const Tensor, activation: []const u8, out_dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_semi_structured_linear(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                meta.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                activation.ptr, activation.len,
                out_dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseSoftmax(
        self: *const Tensor, dim_: i64, half_to_float: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_softmax(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                if (half_to_float)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseSoftmaxBackwardData(
        self: *const Tensor, grad_output: *const Tensor, output: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_softmax_backward_data(@ptrCast(&c_tensors), grad_output.c_tensor,
                output.c_tensor,
                dim_,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseSoftmaxBackwardDataOut(
        self: *const Tensor, out: *const Tensor, grad_output: *const Tensor, output: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_softmax_backward_data_out(@ptrCast(&c_tensors), out.c_tensor,
                grad_output.c_tensor,
                output.c_tensor,
                dim_,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseSoftmaxInt(
        self: *const Tensor, dim_: i64, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_softmax_int(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseSoftmaxOut(
        self: *const Tensor, out: *const Tensor, dim_: i64, half_to_float: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_softmax_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_,
                if (half_to_float)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseSparseMatmul(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_sparse_matmul(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseSparseMatmulOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_sparse_matmul_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseSum(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_sum(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseSumBackward(
        self: *const Tensor, gradient: *const Tensor, dim_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_sum_backward(@ptrCast(&c_tensors), gradient.c_tensor,
                self.c_tensor,
                dim_.ptr, @intCast(dim_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseSumBackwardOut(
        self: *const Tensor, out: *const Tensor, gradient: *const Tensor, dim_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_sum_backward_out(@ptrCast(&c_tensors), out.c_tensor,
                gradient.c_tensor,
                self.c_tensor,
                dim_.ptr, @intCast(dim_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseSumDim(
        self: *const Tensor, dim_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_sum_dim(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseSumDimDtype(
        self: *const Tensor, dim_: []i64, dtype: Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_sum_dim_dtype(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                dtype.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseSumDimOut(
        self: *const Tensor, out: *const Tensor, dim_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_sum_dim_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_.ptr, @intCast(dim_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSparseSumDtype(
        self: *const Tensor, dtype: Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__sparse_sum_dtype(@ptrCast(&c_tensors), self.c_tensor,
                dtype.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSpdiags(
        diagonals: *const Tensor, offsets: *const Tensor, shape: []i64, layout: ?Layout
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__spdiags(@ptrCast(&c_tensors), diagonals.c_tensor,
                offsets.c_tensor,
                shape.ptr, @intCast(shape.len),
                layout orelse - 1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalSpdiagsOut(
        out: *const Tensor, diagonals: *const Tensor, offsets: *const Tensor, shape: []i64, layout: ?Layout
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__spdiags_out(@ptrCast(&c_tensors), out.c_tensor,
                diagonals.c_tensor,
                offsets.c_tensor,
                shape.ptr, @intCast(shape.len),
                layout orelse - 1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalStack(
        tensors: []*const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__stack(@ptrCast(&c_tensors), ptrList(tensors).ptr, @intCast(tensors.len),
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalStackOut(
        out: *const Tensor, tensors: []*const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__stack_out(@ptrCast(&c_tensors), out.c_tensor,
                ptrList(tensors).ptr, @intCast(tensors.len),
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalStandardGamma(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__standard_gamma(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalStandardGammaGrad(
        self: *const Tensor, output: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__standard_gamma_grad(@ptrCast(&c_tensors), self.c_tensor,
                output.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalStandardGammaGradOut(
        self: *const Tensor, out: *const Tensor, output: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__standard_gamma_grad_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                output.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalStandardGammaOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__standard_gamma_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalTestAmbiguousDefaults(
        dummy: *const Tensor, a: i64, b: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__test_ambiguous_defaults(@ptrCast(&c_tensors), dummy.c_tensor,
                a,
                b);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalTestAmbiguousDefaultsB(
        dummy: *const Tensor, a: i64, b: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__test_ambiguous_defaults_b(@ptrCast(&c_tensors), dummy.c_tensor,
                a,
                b.ptr, b.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalTestAutogradMultipleDispatch(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__test_autograd_multiple_dispatch(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalTestAutogradMultipleDispatchFullcoverageOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__test_autograd_multiple_dispatch_fullcoverage_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalTestAutogradMultipleDispatchNtonly(
        self: *const Tensor, b: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__test_autograd_multiple_dispatch_ntonly(@ptrCast(&c_tensors), self.c_tensor,
                if (b)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalTestAutogradMultipleDispatchView(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__test_autograd_multiple_dispatch_view(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalTestAutogradMultipleDispatchViewCopy(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__test_autograd_multiple_dispatch_view_copy(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalTestAutogradMultipleDispatchViewCopyOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__test_autograd_multiple_dispatch_view_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalTestCheckTensor(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__test_check_tensor(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalTestFunctorchFallback(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__test_functorch_fallback(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalTestFunctorchFallbackOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__test_functorch_fallback_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalTestOptionalFilledIntlist(
        values_: *const Tensor, addends: ?[]i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__test_optional_filled_intlist(@ptrCast(&c_tensors), values_.c_tensor,
                addends.ptr, @intCast(addends.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalTestOptionalFilledIntlistOut(
        out: *const Tensor, values_: *const Tensor, addends: ?[]i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__test_optional_filled_intlist_out(@ptrCast(&c_tensors), out.c_tensor,
                values_.c_tensor,
                addends.ptr, @intCast(addends.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalTestOptionalFloatlist(
        values_: *const Tensor, addends: []f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__test_optional_floatlist(@ptrCast(&c_tensors), values_.c_tensor,
                addends.ptr, @intCast(addends.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalTestOptionalFloatlistOut(
        out: *const Tensor, values_: *const Tensor, addends: []f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__test_optional_floatlist_out(@ptrCast(&c_tensors), out.c_tensor,
                values_.c_tensor,
                addends.ptr, @intCast(addends.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalTestOptionalIntlist(
        values_: *const Tensor, addends: ?[]i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__test_optional_intlist(@ptrCast(&c_tensors), values_.c_tensor,
                addends.ptr, @intCast(addends.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalTestOptionalIntlistOut(
        out: *const Tensor, values_: *const Tensor, addends: ?[]i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__test_optional_intlist_out(@ptrCast(&c_tensors), out.c_tensor,
                values_.c_tensor,
                addends.ptr, @intCast(addends.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalTestParallelMaterialize(
        self: *const Tensor, num_parallel: i64, skip_first: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__test_parallel_materialize(@ptrCast(&c_tensors), self.c_tensor,
                num_parallel,
                if (skip_first)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalTestSerializationSubcmul(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__test_serialization_subcmul(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalTestStringDefault(
        dummy: *const Tensor, a: []const u8, b: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__test_string_default(@ptrCast(&c_tensors), dummy.c_tensor,
                a.ptr, a.len,
                b.ptr, b.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalTestWarnInAutograd(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__test_warn_in_autograd(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalTestWarnInAutogradOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__test_warn_in_autograd_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalToCopy(
        self: *const Tensor, options_: TensorOptions, non_blocking: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__to_copy(@ptrCast(&c_tensors), self.c_tensor,
                options_.kind.cInt(), options_.device.cInt(),
                if (non_blocking)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalToCopyOut(
        self: *const Tensor, out: *const Tensor, non_blocking: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__to_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                if (non_blocking)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalToCpu(
        tensors: []*const Tensor
    ) []Tensor {
        const c_tensors = __c.atg__to_cpu(ptrList(tensors).ptr, @intCast(tensors.len));
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn internalToDense(
        self: *const Tensor, dtype: ?Kind, masked_grad: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__to_dense(@ptrCast(&c_tensors), self.c_tensor,
                dtype orelse -1,
                if (masked_grad)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalToDenseOut(
        self: *const Tensor, out: *const Tensor, dtype: ?Kind, masked_grad: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__to_dense_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dtype orelse -1,
                if (masked_grad)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalToSparse(
        self: *const Tensor, layout: ?Layout, blocksize: ?[]i64, dense_dim_: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__to_sparse(@ptrCast(&c_tensors), self.c_tensor,
                layout orelse - 1,
                blocksize.ptr, @intCast(blocksize.len),
                dense_dim_ orelse 0, (dense_dim_ == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalToSparseBsc(
        self: *const Tensor, blocksize: []i64, dense_dim_: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__to_sparse_bsc(@ptrCast(&c_tensors), self.c_tensor,
                blocksize.ptr, @intCast(blocksize.len),
                dense_dim_ orelse 0, (dense_dim_ == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalToSparseBscOut(
        self: *const Tensor, out: *const Tensor, blocksize: []i64, dense_dim_: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__to_sparse_bsc_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                blocksize.ptr, @intCast(blocksize.len),
                dense_dim_ orelse 0, (dense_dim_ == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalToSparseBsr(
        self: *const Tensor, blocksize: []i64, dense_dim_: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__to_sparse_bsr(@ptrCast(&c_tensors), self.c_tensor,
                blocksize.ptr, @intCast(blocksize.len),
                dense_dim_ orelse 0, (dense_dim_ == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalToSparseBsrOut(
        self: *const Tensor, out: *const Tensor, blocksize: []i64, dense_dim_: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__to_sparse_bsr_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                blocksize.ptr, @intCast(blocksize.len),
                dense_dim_ orelse 0, (dense_dim_ == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalToSparseCsc(
        self: *const Tensor, dense_dim_: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__to_sparse_csc(@ptrCast(&c_tensors), self.c_tensor,
                dense_dim_ orelse 0, (dense_dim_ == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalToSparseCscOut(
        self: *const Tensor, out: *const Tensor, dense_dim_: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__to_sparse_csc_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dense_dim_ orelse 0, (dense_dim_ == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalToSparseCsr(
        self: *const Tensor, dense_dim_: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__to_sparse_csr(@ptrCast(&c_tensors), self.c_tensor,
                dense_dim_ orelse 0, (dense_dim_ == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalToSparseCsrOut(
        self: *const Tensor, out: *const Tensor, dense_dim_: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__to_sparse_csr_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dense_dim_ orelse 0, (dense_dim_ == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalToSparseOut(
        self: *const Tensor, out: *const Tensor, layout: ?Layout, blocksize: ?[]i64, dense_dim_: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__to_sparse_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                layout orelse - 1,
                blocksize.ptr, @intCast(blocksize.len),
                dense_dim_ orelse 0, (dense_dim_ == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalToSparseSemiStructured(
        dense: *const Tensor
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__to_sparse_semi_structured(@ptrCast(&c_tensors), dense.c_tensor);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalToSparseSparseDim(
        self: *const Tensor, sparse_dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__to_sparse_sparse_dim(@ptrCast(&c_tensors), self.c_tensor,
                sparse_dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalToSparseSparseDimOut(
        self: *const Tensor, out: *const Tensor, sparse_dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__to_sparse_sparse_dim_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                sparse_dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalTransformBiasRescaleQkv(
        qkv: *const Tensor, qkv_bias: *const Tensor, num_heads: i64
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg__transform_bias_rescale_qkv(@ptrCast(&c_tensors), qkv.c_tensor,
                qkv_bias.c_tensor,
                num_heads);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn internalTransformBiasRescaleQkvOut(
        out0: *const Tensor, out1: *const Tensor, out2: *const Tensor, qkv: *const Tensor, qkv_bias: *const Tensor, num_heads: i64
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg__transform_bias_rescale_qkv_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                out2.c_tensor,
                qkv.c_tensor,
                qkv_bias.c_tensor,
                num_heads);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn internalTransformerEncoderLayerFwd(
        src: *const Tensor, embed_dim: i64, num_heads: i64, qkv_weight: *const Tensor, qkv_bias: *const Tensor, proj_weight: *const Tensor, proj_bias: *const Tensor, use_gelu: bool, norm_first: bool, eps: f64, norm_weight_1: *const Tensor, norm_bias_1: *const Tensor, norm_weight_2: *const Tensor, norm_bias_2: *const Tensor, ffn_weight_1: *const Tensor, ffn_bias_1: *const Tensor, ffn_weight_2: *const Tensor, ffn_bias_2: *const Tensor, mask: ?*const Tensor, mask_type: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__transformer_encoder_layer_fwd(@ptrCast(&c_tensors), src.c_tensor,
                embed_dim,
                num_heads,
                qkv_weight.c_tensor,
                qkv_bias.c_tensor,
                proj_weight.c_tensor,
                proj_bias.c_tensor,
                if (use_gelu)  1  else  0,
                if (norm_first)  1  else  0,
                eps,
                norm_weight_1.c_tensor,
                norm_bias_1.c_tensor,
                norm_weight_2.c_tensor,
                norm_bias_2.c_tensor,
                ffn_weight_1.c_tensor,
                ffn_bias_1.c_tensor,
                ffn_weight_2.c_tensor,
                ffn_bias_2.c_tensor,
                if (mask != null) mask.?.c_tensor else null,
                mask_type orelse 0, (mask_type == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalTransformerEncoderLayerFwdOut(
        out: *const Tensor, src: *const Tensor, embed_dim: i64, num_heads: i64, qkv_weight: *const Tensor, qkv_bias: *const Tensor, proj_weight: *const Tensor, proj_bias: *const Tensor, use_gelu: bool, norm_first: bool, eps: f64, norm_weight_1: *const Tensor, norm_bias_1: *const Tensor, norm_weight_2: *const Tensor, norm_bias_2: *const Tensor, ffn_weight_1: *const Tensor, ffn_bias_1: *const Tensor, ffn_weight_2: *const Tensor, ffn_bias_2: *const Tensor, mask: ?*const Tensor, mask_type: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__transformer_encoder_layer_fwd_out(@ptrCast(&c_tensors), out.c_tensor,
                src.c_tensor,
                embed_dim,
                num_heads,
                qkv_weight.c_tensor,
                qkv_bias.c_tensor,
                proj_weight.c_tensor,
                proj_bias.c_tensor,
                if (use_gelu)  1  else  0,
                if (norm_first)  1  else  0,
                eps,
                norm_weight_1.c_tensor,
                norm_bias_1.c_tensor,
                norm_weight_2.c_tensor,
                norm_bias_2.c_tensor,
                ffn_weight_1.c_tensor,
                ffn_bias_1.c_tensor,
                ffn_weight_2.c_tensor,
                ffn_bias_2.c_tensor,
                if (mask != null) mask.?.c_tensor else null,
                mask_type orelse 0, (mask_type == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalTrilinear(
        i1_: *const Tensor, i2_: *const Tensor, i3_: *const Tensor, expand1: []i64, expand2: []i64, expand3: []i64, sumdim: []i64, unroll_dim: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__trilinear(@ptrCast(&c_tensors), i1_.c_tensor,
                i2_.c_tensor,
                i3_.c_tensor,
                expand1.ptr, @intCast(expand1.len),
                expand2.ptr, @intCast(expand2.len),
                expand3.ptr, @intCast(expand3.len),
                sumdim.ptr, @intCast(sumdim.len),
                unroll_dim);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalTrilinearOut(
        out: *const Tensor, i1_: *const Tensor, i2_: *const Tensor, i3_: *const Tensor, expand1: []i64, expand2: []i64, expand3: []i64, sumdim: []i64, unroll_dim: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__trilinear_out(@ptrCast(&c_tensors), out.c_tensor,
                i1_.c_tensor,
                i2_.c_tensor,
                i3_.c_tensor,
                expand1.ptr, @intCast(expand1.len),
                expand2.ptr, @intCast(expand2.len),
                expand3.ptr, @intCast(expand3.len),
                sumdim.ptr, @intCast(sumdim.len),
                unroll_dim);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalTritonMultiHeadAttention(
        query: *const Tensor, key: *const Tensor, value: *const Tensor, embed_dim: i64, num_head: i64, qkv_weight: *const Tensor, qkv_bias: *const Tensor, proj_weight: *const Tensor, proj_bias: *const Tensor, mask: ?*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__triton_multi_head_attention(@ptrCast(&c_tensors), query.c_tensor,
                key.c_tensor,
                value.c_tensor,
                embed_dim,
                num_head,
                qkv_weight.c_tensor,
                qkv_bias.c_tensor,
                proj_weight.c_tensor,
                proj_bias.c_tensor,
                if (mask != null) mask.?.c_tensor else null);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalTritonMultiHeadAttentionOut(
        out: *const Tensor, query: *const Tensor, key: *const Tensor, value: *const Tensor, embed_dim: i64, num_head: i64, qkv_weight: *const Tensor, qkv_bias: *const Tensor, proj_weight: *const Tensor, proj_bias: *const Tensor, mask: ?*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__triton_multi_head_attention_out(@ptrCast(&c_tensors), out.c_tensor,
                query.c_tensor,
                key.c_tensor,
                value.c_tensor,
                embed_dim,
                num_head,
                qkv_weight.c_tensor,
                qkv_bias.c_tensor,
                proj_weight.c_tensor,
                proj_bias.c_tensor,
                if (mask != null) mask.?.c_tensor else null);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalTritonScaledDotAttention(
        q: *const Tensor, k: *const Tensor, v: *const Tensor, dropout_p: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__triton_scaled_dot_attention(@ptrCast(&c_tensors), q.c_tensor,
                k.c_tensor,
                v.c_tensor,
                dropout_p);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalTritonScaledDotAttentionOut(
        out: *const Tensor, q: *const Tensor, k: *const Tensor, v: *const Tensor, dropout_p: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__triton_scaled_dot_attention_out(@ptrCast(&c_tensors), out.c_tensor,
                q.c_tensor,
                k.c_tensor,
                v.c_tensor,
                dropout_p);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalUnique(
        self: *const Tensor, sorted: bool, return_inverse: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__unique(@ptrCast(&c_tensors), self.c_tensor,
                if (sorted)  1  else  0,
                if (return_inverse)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalUnique2(
        self: *const Tensor, sorted: bool, return_inverse: bool, return_counts: bool
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg__unique2(@ptrCast(&c_tensors), self.c_tensor,
                if (sorted)  1  else  0,
                if (return_inverse)  1  else  0,
                if (return_counts)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn internalUnique2Out(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, out2: *const Tensor, sorted: bool, return_inverse: bool, return_counts: bool
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg__unique2_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                out2.c_tensor,
                self.c_tensor,
                if (sorted)  1  else  0,
                if (return_inverse)  1  else  0,
                if (return_counts)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn internalUniqueOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, sorted: bool, return_inverse: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__unique_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                self.c_tensor,
                if (sorted)  1  else  0,
                if (return_inverse)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalUnpackDual(
        dual: *const Tensor, level: i64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__unpack_dual(@ptrCast(&c_tensors), dual.c_tensor,
                level);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalUnsafeIndex(
        self: *const Tensor, indices_: []?*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__unsafe_index(@ptrCast(&c_tensors), self.c_tensor,
                ptrListOpt(indices_).ptr, @intCast(indices_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalUnsafeIndexPut(
        self: *const Tensor, indices_: []?*const Tensor, values_: *const Tensor, accumulate: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__unsafe_index_put(@ptrCast(&c_tensors), self.c_tensor,
                ptrListOpt(indices_).ptr, @intCast(indices_.len),
                values_.c_tensor,
                if (accumulate)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalUnsafeView(
        self: *const Tensor, size_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__unsafe_view(@ptrCast(&c_tensors), self.c_tensor,
                size_.ptr, @intCast(size_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalUnsafeViewOut(
        self: *const Tensor, out: *const Tensor, size_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__unsafe_view_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                size_.ptr, @intCast(size_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalUpsampleBicubic2dAa(
        self: *const Tensor, output_size: []i64, align_corners: bool, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__upsample_bicubic2d_aa(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                if (align_corners)  1  else  0,
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalUpsampleBicubic2dAaBackward(
        grad_output: *const Tensor, output_size: []i64, input_size: []i64, align_corners: bool, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__upsample_bicubic2d_aa_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                input_size.ptr, @intCast(input_size.len),
                if (align_corners)  1  else  0,
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalUpsampleBicubic2dAaBackwardGradInput(
        grad_input: *const Tensor, grad_output: *const Tensor, output_size: []i64, input_size: []i64, align_corners: bool, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__upsample_bicubic2d_aa_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                input_size.ptr, @intCast(input_size.len),
                if (align_corners)  1  else  0,
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalUpsampleBicubic2dAaOut(
        self: *const Tensor, out: *const Tensor, output_size: []i64, align_corners: bool, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__upsample_bicubic2d_aa_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                if (align_corners)  1  else  0,
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalUpsampleBicubic2dAaVec(
        self: *const Tensor, output_size: ?[]i64, align_corners: bool, scale_factors: []f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__upsample_bicubic2d_aa_vec(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                if (align_corners)  1  else  0,
                scale_factors.ptr, @intCast(scale_factors.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalUpsampleBilinear2dAa(
        self: *const Tensor, output_size: []i64, align_corners: bool, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__upsample_bilinear2d_aa(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                if (align_corners)  1  else  0,
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalUpsampleBilinear2dAaBackward(
        grad_output: *const Tensor, output_size: []i64, input_size: []i64, align_corners: bool, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__upsample_bilinear2d_aa_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                input_size.ptr, @intCast(input_size.len),
                if (align_corners)  1  else  0,
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalUpsampleBilinear2dAaBackwardGradInput(
        grad_input: *const Tensor, grad_output: *const Tensor, output_size: []i64, input_size: []i64, align_corners: bool, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__upsample_bilinear2d_aa_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                input_size.ptr, @intCast(input_size.len),
                if (align_corners)  1  else  0,
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalUpsampleBilinear2dAaOut(
        self: *const Tensor, out: *const Tensor, output_size: []i64, align_corners: bool, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__upsample_bilinear2d_aa_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                if (align_corners)  1  else  0,
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalUpsampleBilinear2dAaVec(
        self: *const Tensor, output_size: ?[]i64, align_corners: bool, scale_factors: []f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__upsample_bilinear2d_aa_vec(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                if (align_corners)  1  else  0,
                scale_factors.ptr, @intCast(scale_factors.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalUpsampleNearestExact1d(
        self: *const Tensor, output_size: []i64, scales: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__upsample_nearest_exact1d(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                scales orelse std.math.nan, (scales == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalUpsampleNearestExact1dBackward(
        grad_output: *const Tensor, output_size: []i64, input_size: []i64, scales: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__upsample_nearest_exact1d_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                input_size.ptr, @intCast(input_size.len),
                scales orelse std.math.nan, (scales == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalUpsampleNearestExact1dBackwardGradInput(
        grad_input: *const Tensor, grad_output: *const Tensor, output_size: []i64, input_size: []i64, scales: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__upsample_nearest_exact1d_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                input_size.ptr, @intCast(input_size.len),
                scales orelse std.math.nan, (scales == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalUpsampleNearestExact1dOut(
        self: *const Tensor, out: *const Tensor, output_size: []i64, scales: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__upsample_nearest_exact1d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                scales orelse std.math.nan, (scales == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalUpsampleNearestExact1dVec(
        self: *const Tensor, output_size: ?[]i64, scale_factors: []f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__upsample_nearest_exact1d_vec(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                scale_factors.ptr, @intCast(scale_factors.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalUpsampleNearestExact2d(
        self: *const Tensor, output_size: []i64, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__upsample_nearest_exact2d(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalUpsampleNearestExact2dBackward(
        grad_output: *const Tensor, output_size: []i64, input_size: []i64, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__upsample_nearest_exact2d_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                input_size.ptr, @intCast(input_size.len),
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalUpsampleNearestExact2dBackwardGradInput(
        grad_input: *const Tensor, grad_output: *const Tensor, output_size: []i64, input_size: []i64, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__upsample_nearest_exact2d_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                input_size.ptr, @intCast(input_size.len),
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalUpsampleNearestExact2dOut(
        self: *const Tensor, out: *const Tensor, output_size: []i64, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__upsample_nearest_exact2d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalUpsampleNearestExact2dVec(
        self: *const Tensor, output_size: ?[]i64, scale_factors: []f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__upsample_nearest_exact2d_vec(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                scale_factors.ptr, @intCast(scale_factors.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalUpsampleNearestExact3d(
        self: *const Tensor, output_size: []i64, scales_d: ?f64, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__upsample_nearest_exact3d(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                scales_d orelse std.math.nan, (scales_d == null),
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalUpsampleNearestExact3dBackward(
        grad_output: *const Tensor, output_size: []i64, input_size: []i64, scales_d: ?f64, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__upsample_nearest_exact3d_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                input_size.ptr, @intCast(input_size.len),
                scales_d orelse std.math.nan, (scales_d == null),
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalUpsampleNearestExact3dBackwardGradInput(
        grad_input: *const Tensor, grad_output: *const Tensor, output_size: []i64, input_size: []i64, scales_d: ?f64, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__upsample_nearest_exact3d_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                input_size.ptr, @intCast(input_size.len),
                scales_d orelse std.math.nan, (scales_d == null),
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalUpsampleNearestExact3dOut(
        self: *const Tensor, out: *const Tensor, output_size: []i64, scales_d: ?f64, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__upsample_nearest_exact3d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                scales_d orelse std.math.nan, (scales_d == null),
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalUpsampleNearestExact3dVec(
        self: *const Tensor, output_size: ?[]i64, scale_factors: []f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__upsample_nearest_exact3d_vec(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                scale_factors.ptr, @intCast(scale_factors.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalUseCudnnCtcLoss(
        log_probs: *const Tensor, targets: *const Tensor, input_lengths: []i64, target_lengths: []i64, blank: i64
    ) bool {
        const return_ = __c.atg__use_cudnn_ctc_loss(log_probs.c_tensor,
                targets.c_tensor,
                input_lengths.ptr, @intCast(input_lengths.len),
                target_lengths.ptr, @intCast(target_lengths.len),
                blank);
       torch.readAndCleanError();
        return return_ != 0;
    }

    pub fn internalUseCudnnCtcLossTensor(
        log_probs: *const Tensor, targets: *const Tensor, input_lengths: *const Tensor, target_lengths: *const Tensor, blank: i64
    ) bool {
        const return_ = __c.atg__use_cudnn_ctc_loss_tensor(log_probs.c_tensor,
                targets.c_tensor,
                input_lengths.c_tensor,
                target_lengths.c_tensor,
                blank);
       torch.readAndCleanError();
        return return_ != 0;
    }

    pub fn internalUseCudnnRnnFlattenWeight(
        
    ) bool {
        const return_ = __c.atg__use_cudnn_rnn_flatten_weight();
       torch.readAndCleanError();
        return return_ != 0;
    }

    pub fn internalValidateCompressedSparseIndices(
        is_crow: bool, compressed_idx: *const Tensor, plain_idx: *const Tensor, cdim: i64, dim_: i64, nnz: i64
    ) void {
        __c.atg__validate_compressed_sparse_indices(if (is_crow)  1  else  0,
                compressed_idx.c_tensor,
                plain_idx.c_tensor,
                cdim,
                dim_,
                nnz);
        torch.readAndCleanError();
        return;
    }

    pub fn internalValidateSparseBscTensorArgs(
        ccol_indices_: *const Tensor, row_indices_: *const Tensor, values_: *const Tensor, size_: []i64
    ) void {
        __c.atg__validate_sparse_bsc_tensor_args(ccol_indices_.c_tensor,
                row_indices_.c_tensor,
                values_.c_tensor,
                size_.ptr, @intCast(size_.len));
        torch.readAndCleanError();
        return;
    }

    pub fn internalValidateSparseBsrTensorArgs(
        crow_indices_: *const Tensor, col_indices_: *const Tensor, values_: *const Tensor, size_: []i64
    ) void {
        __c.atg__validate_sparse_bsr_tensor_args(crow_indices_.c_tensor,
                col_indices_.c_tensor,
                values_.c_tensor,
                size_.ptr, @intCast(size_.len));
        torch.readAndCleanError();
        return;
    }

    pub fn internalValidateSparseCompressedTensorArgs(
        compressed_indices: *const Tensor, plain_indices: *const Tensor, values_: *const Tensor, size_: []i64, layout: Layout
    ) void {
        __c.atg__validate_sparse_compressed_tensor_args(compressed_indices.c_tensor,
                plain_indices.c_tensor,
                values_.c_tensor,
                size_.ptr, @intCast(size_.len),
                layout.toI8());
        torch.readAndCleanError();
        return;
    }

    pub fn internalValidateSparseCscTensorArgs(
        ccol_indices_: *const Tensor, row_indices_: *const Tensor, values_: *const Tensor, size_: []i64
    ) void {
        __c.atg__validate_sparse_csc_tensor_args(ccol_indices_.c_tensor,
                row_indices_.c_tensor,
                values_.c_tensor,
                size_.ptr, @intCast(size_.len));
        torch.readAndCleanError();
        return;
    }

    pub fn internalValidateSparseCsrTensorArgs(
        crow_indices_: *const Tensor, col_indices_: *const Tensor, values_: *const Tensor, size_: []i64
    ) void {
        __c.atg__validate_sparse_csr_tensor_args(crow_indices_.c_tensor,
                col_indices_.c_tensor,
                values_.c_tensor,
                size_.ptr, @intCast(size_.len));
        torch.readAndCleanError();
        return;
    }

    pub fn internalValues(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__values(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalValuesCopy(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__values_copy(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalValuesCopyOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__values_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalVersion(
        self: *const Tensor, 
    ) i64 {
        const return_ = __c.atg__version(self.c_tensor);
       torch.readAndCleanError();
        return return_;
    }

    pub fn internalWeightInt4packMm(
        self: *const Tensor, mat2: *const Tensor, qgroupsize: i64, qscaleandzeros: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__weight_int4pack_mm(@ptrCast(&c_tensors), self.c_tensor,
                mat2.c_tensor,
                qgroupsize,
                qscaleandzeros.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalWeightInt8packMm(
        self: *const Tensor, mat2: *const Tensor, scales: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__weight_int8pack_mm(@ptrCast(&c_tensors), self.c_tensor,
                mat2.c_tensor,
                scales.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalWeightNorm(
        v: *const Tensor, g: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg__weight_norm(@ptrCast(&c_tensors), v.c_tensor,
                g.c_tensor,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn internalWeightNormDifferentiableBackward(
        grad_w: *const Tensor, saved_v: *const Tensor, saved_g: *const Tensor, saved_norms: *const Tensor, dim_: i64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__weight_norm_differentiable_backward(@ptrCast(&c_tensors), grad_w.c_tensor,
                saved_v.c_tensor,
                saved_g.c_tensor,
                saved_norms.c_tensor,
                dim_);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalWeightNormInterface(
        v: *const Tensor, g: *const Tensor, dim_: i64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__weight_norm_interface(@ptrCast(&c_tensors), v.c_tensor,
                g.c_tensor,
                dim_);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalWeightNormInterfaceBackward(
        grad_w: *const Tensor, saved_v: *const Tensor, saved_g: *const Tensor, saved_norms: *const Tensor, dim_: i64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__weight_norm_interface_backward(@ptrCast(&c_tensors), grad_w.c_tensor,
                saved_v.c_tensor,
                saved_g.c_tensor,
                saved_norms.c_tensor,
                dim_);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalWeightNormInterfaceBackwardOut(
        out0: *const Tensor, out1: *const Tensor, grad_w: *const Tensor, saved_v: *const Tensor, saved_g: *const Tensor, saved_norms: *const Tensor, dim_: i64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__weight_norm_interface_backward_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                grad_w.c_tensor,
                saved_v.c_tensor,
                saved_g.c_tensor,
                saved_norms.c_tensor,
                dim_);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn internalWeightNormInterfaceOut(
        out0: *const Tensor, out1: *const Tensor, v: *const Tensor, g: *const Tensor, dim_: i64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg__weight_norm_interface_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                v.c_tensor,
                g.c_tensor,
                dim_);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn abs(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_abs(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn abs2(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_abs_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn absOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_abs_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn absolute(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_absolute(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn absolute_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_absolute_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn absoluteOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_absolute_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn acos(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_acos(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn acos_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_acos_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn acosOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_acos_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn acosh(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_acosh(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn acosh_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_acosh_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn acoshOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_acosh_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn adaptiveAvgPool1d(
        self: *const Tensor, output_size: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_adaptive_avg_pool1d(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn adaptiveAvgPool2d(
        self: *const Tensor, output_size: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_adaptive_avg_pool2d(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn adaptiveAvgPool2dOut(
        self: *const Tensor, out: *const Tensor, output_size: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_adaptive_avg_pool2d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                output_size.ptr, @intCast(output_size.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn adaptiveAvgPool3d(
        self: *const Tensor, output_size: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_adaptive_avg_pool3d(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn adaptiveAvgPool3dBackward(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_adaptive_avg_pool3d_backward(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn adaptiveAvgPool3dOut(
        self: *const Tensor, out: *const Tensor, output_size: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_adaptive_avg_pool3d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                output_size.ptr, @intCast(output_size.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn adaptiveMaxPool1d(
        self: *const Tensor, output_size: []i64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_adaptive_max_pool1d(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len));
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn adaptiveMaxPool2d(
        self: *const Tensor, output_size: []i64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_adaptive_max_pool2d(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len));
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn adaptiveMaxPool2dBackward(
        self: *const Tensor, grad_output: *const Tensor, indices_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_adaptive_max_pool2d_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                indices_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn adaptiveMaxPool2dBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor, indices_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_adaptive_max_pool2d_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                indices_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn adaptiveMaxPool2dOut(
        self: *const Tensor, out: *const Tensor, indices_: *const Tensor, output_size: []i64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_adaptive_max_pool2d_out(@ptrCast(&c_tensors), out.c_tensor,
                indices_.c_tensor,
                self.c_tensor,
                output_size.ptr, @intCast(output_size.len));
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn adaptiveMaxPool3d(
        self: *const Tensor, output_size: []i64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_adaptive_max_pool3d(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len));
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn adaptiveMaxPool3dBackward(
        self: *const Tensor, grad_output: *const Tensor, indices_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_adaptive_max_pool3d_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                indices_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn adaptiveMaxPool3dBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor, indices_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_adaptive_max_pool3d_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                indices_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn adaptiveMaxPool3dOut(
        self: *const Tensor, out: *const Tensor, indices_: *const Tensor, output_size: []i64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_adaptive_max_pool3d_out(@ptrCast(&c_tensors), out.c_tensor,
                indices_.c_tensor,
                self.c_tensor,
                output_size.ptr, @intCast(output_size.len));
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn add(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_add(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn add_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_add_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn addOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_add_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn addScalar(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_add_scalar(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn addScalar_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_add_scalar_(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn addScalarOut(
        self: *const Tensor, out: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_add_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn addbmm(
        self: *const Tensor, batch1: *const Tensor, batch2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_addbmm(@ptrCast(&c_tensors), self.c_tensor,
                batch1.c_tensor,
                batch2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn addbmm_(
        self: *Tensor, batch1: *const Tensor, batch2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_addbmm_(@ptrCast(&c_tensors), self.c_tensor,
                batch1.c_tensor,
                batch2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn addbmmOut(
        self: *const Tensor, out: *const Tensor, batch1: *const Tensor, batch2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_addbmm_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                batch1.c_tensor,
                batch2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn addcdiv(
        self: *const Tensor, tensor1: *const Tensor, tensor2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_addcdiv(@ptrCast(&c_tensors), self.c_tensor,
                tensor1.c_tensor,
                tensor2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn addcdiv_(
        self: *Tensor, tensor1: *const Tensor, tensor2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_addcdiv_(@ptrCast(&c_tensors), self.c_tensor,
                tensor1.c_tensor,
                tensor2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn addcdivOut(
        self: *const Tensor, out: *const Tensor, tensor1: *const Tensor, tensor2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_addcdiv_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                tensor1.c_tensor,
                tensor2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn addcmul(
        self: *const Tensor, tensor1: *const Tensor, tensor2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_addcmul(@ptrCast(&c_tensors), self.c_tensor,
                tensor1.c_tensor,
                tensor2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn addcmul_(
        self: *Tensor, tensor1: *const Tensor, tensor2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_addcmul_(@ptrCast(&c_tensors), self.c_tensor,
                tensor1.c_tensor,
                tensor2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn addcmulOut(
        self: *const Tensor, out: *const Tensor, tensor1: *const Tensor, tensor2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_addcmul_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                tensor1.c_tensor,
                tensor2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn addmm(
        self: *const Tensor, mat1: *const Tensor, mat2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_addmm(@ptrCast(&c_tensors), self.c_tensor,
                mat1.c_tensor,
                mat2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn addmm_(
        self: *Tensor, mat1: *const Tensor, mat2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_addmm_(@ptrCast(&c_tensors), self.c_tensor,
                mat1.c_tensor,
                mat2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn addmmOut(
        self: *const Tensor, out: *const Tensor, mat1: *const Tensor, mat2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_addmm_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                mat1.c_tensor,
                mat2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn addmv(
        self: *const Tensor, mat: *const Tensor, vec: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_addmv(@ptrCast(&c_tensors), self.c_tensor,
                mat.c_tensor,
                vec.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn addmv_(
        self: *Tensor, mat: *const Tensor, vec: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_addmv_(@ptrCast(&c_tensors), self.c_tensor,
                mat.c_tensor,
                vec.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn addmvOut(
        self: *const Tensor, out: *const Tensor, mat: *const Tensor, vec: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_addmv_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                mat.c_tensor,
                vec.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn addr(
        self: *const Tensor, vec1: *const Tensor, vec2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_addr(@ptrCast(&c_tensors), self.c_tensor,
                vec1.c_tensor,
                vec2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn addr_(
        self: *Tensor, vec1: *const Tensor, vec2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_addr_(@ptrCast(&c_tensors), self.c_tensor,
                vec1.c_tensor,
                vec2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn addrOut(
        self: *const Tensor, out: *const Tensor, vec1: *const Tensor, vec2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_addr_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                vec1.c_tensor,
                vec2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn adjoint(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_adjoint(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn affineGridGenerator(
        theta: *const Tensor, size_: []i64, align_corners: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_affine_grid_generator(@ptrCast(&c_tensors), theta.c_tensor,
                size_.ptr, @intCast(size_.len),
                if (align_corners)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn affineGridGeneratorBackward(
        gradient: *const Tensor, size_: []i64, align_corners: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_affine_grid_generator_backward(@ptrCast(&c_tensors), gradient.c_tensor,
                size_.ptr, @intCast(size_.len),
                if (align_corners)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn affineGridGeneratorOut(
        out: *const Tensor, theta: *const Tensor, size_: []i64, align_corners: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_affine_grid_generator_out(@ptrCast(&c_tensors), out.c_tensor,
                theta.c_tensor,
                size_.ptr, @intCast(size_.len),
                if (align_corners)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn alias(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_alias(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn aliasCopy(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_alias_copy(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn aliasCopyOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_alias_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn alignAs(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_align_as(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn alignTensors(
        tensors: []*const Tensor
    ) []Tensor {
        const c_tensors = __c.atg_align_tensors(ptrList(tensors).ptr, @intCast(tensors.len));
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn all(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_all(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn allAllOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_all_all_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn allDim(
        self: *const Tensor, dim_: i64, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_all_dim(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn allDims(
        self: *const Tensor, dim_: ?[]i64, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_all_dims(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn allDimsOut(
        self: *const Tensor, out: *const Tensor, dim_: ?[]i64, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_all_dims_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn allOut(
        self: *const Tensor, out: *const Tensor, dim_: i64, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_all_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn allclose(
        self: *const Tensor, other: *const Tensor, rtol: f64, atol: f64, equal_nan: bool
    ) bool {
        const return_ = __c.atg_allclose(self.c_tensor,
                other.c_tensor,
                rtol,
                atol,
                if (equal_nan)  1  else  0);
       torch.readAndCleanError();
        return return_ != 0;
    }

    pub fn alphaDropout(
        self: *const Tensor, p: f64, train: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_alpha_dropout(@ptrCast(&c_tensors), self.c_tensor,
                p,
                if (train)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn alphaDropout_(
        self: *Tensor, p: f64, train: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_alpha_dropout_(@ptrCast(&c_tensors), self.c_tensor,
                p,
                if (train)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn amax(
        self: *const Tensor, dim_: []i64, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_amax(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn amaxOut(
        self: *const Tensor, out: *const Tensor, dim_: []i64, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_amax_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn amin(
        self: *const Tensor, dim_: []i64, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_amin(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn aminOut(
        self: *const Tensor, out: *const Tensor, dim_: []i64, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_amin_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn aminmax(
        self: *const Tensor, dim_: ?i64, keepdim: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_aminmax(@ptrCast(&c_tensors), self.c_tensor,
                dim_ orelse 0, (dim_ == null),
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn aminmaxOut(
        self: *const Tensor, min_: *const Tensor, max_: *const Tensor, dim_: ?i64, keepdim: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_aminmax_out(@ptrCast(&c_tensors), min_.c_tensor,
                max_.c_tensor,
                self.c_tensor,
                dim_ orelse 0, (dim_ == null),
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn angle(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_angle(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn angleOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_angle_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn any(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_any(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn anyAllOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_any_all_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn anyDim(
        self: *const Tensor, dim_: i64, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_any_dim(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn anyDims(
        self: *const Tensor, dim_: ?[]i64, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_any_dims(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn anyDimsOut(
        self: *const Tensor, out: *const Tensor, dim_: ?[]i64, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_any_dims_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn anyOut(
        self: *const Tensor, out: *const Tensor, dim_: i64, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_any_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn arange(
        end: Scalar, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_arange(@ptrCast(&c_tensors), end.into().c_scalar,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn arangeStart(
        start: Scalar, end: Scalar, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_arange_start(@ptrCast(&c_tensors), start.into().c_scalar,
                end.into().c_scalar,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn arangeStartStep(
        start: Scalar, end: Scalar, step: Scalar, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_arange_start_step(@ptrCast(&c_tensors), start.into().c_scalar,
                end.into().c_scalar,
                step.into().c_scalar,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn arccos(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_arccos(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn arccos_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_arccos_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn arccosOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_arccos_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn arccosh(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_arccosh(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn arccosh_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_arccosh_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn arccoshOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_arccosh_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn arcsin(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_arcsin(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn arcsin_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_arcsin_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn arcsinOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_arcsin_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn arcsinh(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_arcsinh(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn arcsinh_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_arcsinh_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn arcsinhOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_arcsinh_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn arctan(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_arctan(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn arctan2(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_arctan2(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn arctan2_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_arctan2_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn arctan2Out(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_arctan2_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn arctan_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_arctan_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn arctanOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_arctan_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn arctanh(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_arctanh(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn arctanh_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_arctanh_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn arctanhOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_arctanh_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn argmax(
        self: *const Tensor, dim_: ?i64, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_argmax(@ptrCast(&c_tensors), self.c_tensor,
                dim_ orelse 0, (dim_ == null),
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn argmaxOut(
        self: *const Tensor, out: *const Tensor, dim_: ?i64, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_argmax_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_ orelse 0, (dim_ == null),
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn argmin(
        self: *const Tensor, dim_: ?i64, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_argmin(@ptrCast(&c_tensors), self.c_tensor,
                dim_ orelse 0, (dim_ == null),
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn argminOut(
        self: *const Tensor, out: *const Tensor, dim_: ?i64, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_argmin_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_ orelse 0, (dim_ == null),
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn argsort(
        self: *const Tensor, dim_: i64, descending: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_argsort(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                if (descending)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn argsortStable(
        self: *const Tensor, stable: bool, dim_: i64, descending: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_argsort_stable(@ptrCast(&c_tensors), self.c_tensor,
                if (stable)  1  else  0,
                dim_,
                if (descending)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn argsortStableOut(
        self: *const Tensor, out: *const Tensor, stable: bool, dim_: i64, descending: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_argsort_stable_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                if (stable)  1  else  0,
                dim_,
                if (descending)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn argwhere(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_argwhere(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn asStrided(
        self: *const Tensor, size_: []i64, stride_: []i64, storage_offset: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_as_strided(@ptrCast(&c_tensors), self.c_tensor,
                size_.ptr, @intCast(size_.len),
                stride_.ptr, @intCast(stride_.len),
                storage_offset orelse 0, (storage_offset == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn asStrided_(
        self: *Tensor, size_: []i64, stride_: []i64, storage_offset: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_as_strided_(@ptrCast(&c_tensors), self.c_tensor,
                size_.ptr, @intCast(size_.len),
                stride_.ptr, @intCast(stride_.len),
                storage_offset orelse 0, (storage_offset == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn asStridedCopy(
        self: *const Tensor, size_: []i64, stride_: []i64, storage_offset: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_as_strided_copy(@ptrCast(&c_tensors), self.c_tensor,
                size_.ptr, @intCast(size_.len),
                stride_.ptr, @intCast(stride_.len),
                storage_offset orelse 0, (storage_offset == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn asStridedCopyOut(
        self: *const Tensor, out: *const Tensor, size_: []i64, stride_: []i64, storage_offset: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_as_strided_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                size_.ptr, @intCast(size_.len),
                stride_.ptr, @intCast(stride_.len),
                storage_offset orelse 0, (storage_offset == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn asStridedScatter(
        self: *const Tensor, src: *const Tensor, size_: []i64, stride_: []i64, storage_offset: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_as_strided_scatter(@ptrCast(&c_tensors), self.c_tensor,
                src.c_tensor,
                size_.ptr, @intCast(size_.len),
                stride_.ptr, @intCast(stride_.len),
                storage_offset orelse 0, (storage_offset == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn asStridedScatterOut(
        self: *const Tensor, out: *const Tensor, src: *const Tensor, size_: []i64, stride_: []i64, storage_offset: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_as_strided_scatter_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                src.c_tensor,
                size_.ptr, @intCast(size_.len),
                stride_.ptr, @intCast(stride_.len),
                storage_offset orelse 0, (storage_offset == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn asin(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_asin(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn asin_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_asin_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn asinOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_asin_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn asinh(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_asinh(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn asinh_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_asinh_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn asinhOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_asinh_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn atan(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_atan(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn atan2(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_atan2(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn atan2_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_atan2_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn atan2Out(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_atan2_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn atan_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_atan_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn atanOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_atan_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn atanh(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_atanh(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn atanh_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_atanh_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn atanhOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_atanh_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn atleast1d(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_atleast_1d(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn atleast1dSequence(
        tensors: []*const Tensor
    ) []Tensor {
        const c_tensors = __c.atg_atleast_1d_sequence(ptrList(tensors).ptr, @intCast(tensors.len));
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn atleast2d(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_atleast_2d(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn atleast2dSequence(
        tensors: []*const Tensor
    ) []Tensor {
        const c_tensors = __c.atg_atleast_2d_sequence(ptrList(tensors).ptr, @intCast(tensors.len));
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn atleast3d(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_atleast_3d(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn atleast3dSequence(
        tensors: []*const Tensor
    ) []Tensor {
        const c_tensors = __c.atg_atleast_3d_sequence(ptrList(tensors).ptr, @intCast(tensors.len));
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn avgPool1d(
        self: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, ceil_mode: bool, count_include_pad: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_avg_pool1d(@ptrCast(&c_tensors), self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                if (ceil_mode)  1  else  0,
                if (count_include_pad)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn avgPool2d(
        self: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, ceil_mode: bool, count_include_pad: bool, divisor_override: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_avg_pool2d(@ptrCast(&c_tensors), self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                if (ceil_mode)  1  else  0,
                if (count_include_pad)  1  else  0,
                divisor_override orelse 0, (divisor_override == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn avgPool2dBackward(
        self: *const Tensor, grad_output: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, ceil_mode: bool, count_include_pad: bool, divisor_override: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_avg_pool2d_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                if (ceil_mode)  1  else  0,
                if (count_include_pad)  1  else  0,
                divisor_override orelse 0, (divisor_override == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn avgPool2dBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, ceil_mode: bool, count_include_pad: bool, divisor_override: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_avg_pool2d_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                if (ceil_mode)  1  else  0,
                if (count_include_pad)  1  else  0,
                divisor_override orelse 0, (divisor_override == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn avgPool2dOut(
        self: *const Tensor, out: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, ceil_mode: bool, count_include_pad: bool, divisor_override: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_avg_pool2d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                if (ceil_mode)  1  else  0,
                if (count_include_pad)  1  else  0,
                divisor_override orelse 0, (divisor_override == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn avgPool3d(
        self: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, ceil_mode: bool, count_include_pad: bool, divisor_override: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_avg_pool3d(@ptrCast(&c_tensors), self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                if (ceil_mode)  1  else  0,
                if (count_include_pad)  1  else  0,
                divisor_override orelse 0, (divisor_override == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn avgPool3dBackward(
        self: *const Tensor, grad_output: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, ceil_mode: bool, count_include_pad: bool, divisor_override: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_avg_pool3d_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                if (ceil_mode)  1  else  0,
                if (count_include_pad)  1  else  0,
                divisor_override orelse 0, (divisor_override == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn avgPool3dBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, ceil_mode: bool, count_include_pad: bool, divisor_override: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_avg_pool3d_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                if (ceil_mode)  1  else  0,
                if (count_include_pad)  1  else  0,
                divisor_override orelse 0, (divisor_override == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn avgPool3dOut(
        self: *const Tensor, out: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, ceil_mode: bool, count_include_pad: bool, divisor_override: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_avg_pool3d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                if (ceil_mode)  1  else  0,
                if (count_include_pad)  1  else  0,
                divisor_override orelse 0, (divisor_override == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn baddbmm(
        self: *const Tensor, batch1: *const Tensor, batch2: *const Tensor, beta: Scalar, alpha: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_baddbmm(@ptrCast(&c_tensors), self.c_tensor,
                batch1.c_tensor,
                batch2.c_tensor,
                beta.into().c_scalar,
                alpha.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn baddbmm_(
        self: *Tensor, batch1: *const Tensor, batch2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_baddbmm_(@ptrCast(&c_tensors), self.c_tensor,
                batch1.c_tensor,
                batch2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn baddbmmOut(
        self: *const Tensor, out: *const Tensor, batch1: *const Tensor, batch2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_baddbmm_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                batch1.c_tensor,
                batch2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bartlettWindow(
        window_length: i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bartlett_window(@ptrCast(&c_tensors), window_length,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bartlettWindowOut(
        out: *const Tensor, window_length: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bartlett_window_out(@ptrCast(&c_tensors), out.c_tensor,
                window_length);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bartlettWindowPeriodic(
        window_length: i64, periodic: bool, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bartlett_window_periodic(@ptrCast(&c_tensors), window_length,
                if (periodic)  1  else  0,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bartlettWindowPeriodicOut(
        out: *const Tensor, window_length: i64, periodic: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bartlett_window_periodic_out(@ptrCast(&c_tensors), out.c_tensor,
                window_length,
                if (periodic)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn batchNorm(
        self: *const Tensor, weight: ?*const Tensor, bias: ?*const Tensor, running_mean: ?*const Tensor, running_var: ?*const Tensor, training: bool, momentum: f64, eps: f64, cudnn_enabled: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_batch_norm(@ptrCast(&c_tensors), self.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                if (bias != null) bias.?.c_tensor else null,
                if (running_mean != null) running_mean.?.c_tensor else null,
                if (running_var != null) running_var.?.c_tensor else null,
                if (training)  1  else  0,
                momentum,
                eps,
                if (cudnn_enabled)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn batchNormBackwardElemt(
        self: *const Tensor, grad_out: *const Tensor, mean_: *const Tensor, invstd: *const Tensor, weight: ?*const Tensor, sum_dy: *const Tensor, sum_dy_xmu: *const Tensor, count: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_batch_norm_backward_elemt(@ptrCast(&c_tensors), grad_out.c_tensor,
                self.c_tensor,
                mean_.c_tensor,
                invstd.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                sum_dy.c_tensor,
                sum_dy_xmu.c_tensor,
                count.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn batchNormBackwardElemtOut(
        self: *const Tensor, out: *const Tensor, grad_out: *const Tensor, mean_: *const Tensor, invstd: *const Tensor, weight: ?*const Tensor, sum_dy: *const Tensor, sum_dy_xmu: *const Tensor, count: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_batch_norm_backward_elemt_out(@ptrCast(&c_tensors), out.c_tensor,
                grad_out.c_tensor,
                self.c_tensor,
                mean_.c_tensor,
                invstd.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                sum_dy.c_tensor,
                sum_dy_xmu.c_tensor,
                count.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn batchNormBackwardReduce(
        self: *const Tensor, grad_out: *const Tensor, mean_: *const Tensor, invstd: *const Tensor, weight: ?*const Tensor, input_g: bool, weight_g: bool, bias_g: bool
    ) [4]Tensor {
        var c_tensors = [_]C_tensor{null} ** 4;
        __c.atg_batch_norm_backward_reduce(@ptrCast(&c_tensors), grad_out.c_tensor,
                self.c_tensor,
                mean_.c_tensor,
                invstd.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                if (input_g)  1  else  0,
                if (weight_g)  1  else  0,
                if (bias_g)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }, Tensor { .c_tensor = c_tensors[3] }};
    }

    pub fn batchNormBackwardReduceOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, out2: *const Tensor, out3: *const Tensor, grad_out: *const Tensor, mean_: *const Tensor, invstd: *const Tensor, weight: ?*const Tensor, input_g: bool, weight_g: bool, bias_g: bool
    ) [4]Tensor {
        var c_tensors = [_]C_tensor{null} ** 4;
        __c.atg_batch_norm_backward_reduce_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                out2.c_tensor,
                out3.c_tensor,
                grad_out.c_tensor,
                self.c_tensor,
                mean_.c_tensor,
                invstd.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                if (input_g)  1  else  0,
                if (weight_g)  1  else  0,
                if (bias_g)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }, Tensor { .c_tensor = c_tensors[3] }};
    }

    pub fn batchNormElemt(
        self: *const Tensor, weight: ?*const Tensor, bias: ?*const Tensor, mean_: *const Tensor, invstd: *const Tensor, eps: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_batch_norm_elemt(@ptrCast(&c_tensors), self.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                if (bias != null) bias.?.c_tensor else null,
                mean_.c_tensor,
                invstd.c_tensor,
                eps);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn batchNormElemtOut(
        self: *const Tensor, out: *const Tensor, weight: ?*const Tensor, bias: ?*const Tensor, mean_: *const Tensor, invstd: *const Tensor, eps: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_batch_norm_elemt_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                if (bias != null) bias.?.c_tensor else null,
                mean_.c_tensor,
                invstd.c_tensor,
                eps);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn batchNormGatherStats(
        self: *const Tensor, mean_: *const Tensor, invstd: *const Tensor, running_mean: ?*const Tensor, running_var: ?*const Tensor, momentum: f64, eps: f64, count: i64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_batch_norm_gather_stats(@ptrCast(&c_tensors), self.c_tensor,
                mean_.c_tensor,
                invstd.c_tensor,
                if (running_mean != null) running_mean.?.c_tensor else null,
                if (running_var != null) running_var.?.c_tensor else null,
                momentum,
                eps,
                count);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn batchNormGatherStatsOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, mean_: *const Tensor, invstd: *const Tensor, running_mean: ?*const Tensor, running_var: ?*const Tensor, momentum: f64, eps: f64, count: i64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_batch_norm_gather_stats_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                self.c_tensor,
                mean_.c_tensor,
                invstd.c_tensor,
                if (running_mean != null) running_mean.?.c_tensor else null,
                if (running_var != null) running_var.?.c_tensor else null,
                momentum,
                eps,
                count);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn batchNormGatherStatsWithCounts(
        self: *const Tensor, mean_: *const Tensor, invstd: *const Tensor, running_mean: ?*const Tensor, running_var: ?*const Tensor, momentum: f64, eps: f64, counts: *const Tensor
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_batch_norm_gather_stats_with_counts(@ptrCast(&c_tensors), self.c_tensor,
                mean_.c_tensor,
                invstd.c_tensor,
                if (running_mean != null) running_mean.?.c_tensor else null,
                if (running_var != null) running_var.?.c_tensor else null,
                momentum,
                eps,
                counts.c_tensor);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn batchNormGatherStatsWithCountsOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, mean_: *const Tensor, invstd: *const Tensor, running_mean: ?*const Tensor, running_var: ?*const Tensor, momentum: f64, eps: f64, counts: *const Tensor
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_batch_norm_gather_stats_with_counts_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                self.c_tensor,
                mean_.c_tensor,
                invstd.c_tensor,
                if (running_mean != null) running_mean.?.c_tensor else null,
                if (running_var != null) running_var.?.c_tensor else null,
                momentum,
                eps,
                counts.c_tensor);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn batchNormStats(
        self: *const Tensor, eps: f64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_batch_norm_stats(@ptrCast(&c_tensors), self.c_tensor,
                eps);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn batchNormStatsOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, eps: f64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_batch_norm_stats_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                self.c_tensor,
                eps);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn batchNormUpdateStats(
        self: *const Tensor, running_mean: ?*const Tensor, running_var: ?*const Tensor, momentum: f64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_batch_norm_update_stats(@ptrCast(&c_tensors), self.c_tensor,
                if (running_mean != null) running_mean.?.c_tensor else null,
                if (running_var != null) running_var.?.c_tensor else null,
                momentum);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn batchNormUpdateStatsOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, running_mean: ?*const Tensor, running_var: ?*const Tensor, momentum: f64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_batch_norm_update_stats_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                self.c_tensor,
                if (running_mean != null) running_mean.?.c_tensor else null,
                if (running_var != null) running_var.?.c_tensor else null,
                momentum);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn bernoulli(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bernoulli(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bernoulli_(
        self: *Tensor, p: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bernoulli_(@ptrCast(&c_tensors), self.c_tensor,
                p.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bernoulliFloat_(
        self: *Tensor, p: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bernoulli_float_(@ptrCast(&c_tensors), self.c_tensor,
                p);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bernoulliP(
        self: *const Tensor, p: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bernoulli_p(@ptrCast(&c_tensors), self.c_tensor,
                p);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bernoulliTensor(
        self: *const Tensor, p: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bernoulli_tensor(@ptrCast(&c_tensors), self.c_tensor,
                p.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bilinear(
        input1: *const Tensor, input2: *const Tensor, weight: *const Tensor, bias: ?*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bilinear(@ptrCast(&c_tensors), input1.c_tensor,
                input2.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn binaryCrossEntropy(
        self: *const Tensor, target: *const Tensor, weight: ?*const Tensor, reduction: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_binary_cross_entropy(@ptrCast(&c_tensors), self.c_tensor,
                target.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                reduction.to_int());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn binaryCrossEntropyBackward(
        self: *const Tensor, grad_output: *const Tensor, target: *const Tensor, weight: ?*const Tensor, reduction: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_binary_cross_entropy_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                target.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                reduction.to_int());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn binaryCrossEntropyBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor, target: *const Tensor, weight: ?*const Tensor, reduction: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_binary_cross_entropy_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                target.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                reduction.to_int());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn binaryCrossEntropyOut(
        self: *const Tensor, out: *const Tensor, target: *const Tensor, weight: ?*const Tensor, reduction: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_binary_cross_entropy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                target.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                reduction.to_int());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn binaryCrossEntropyWithLogits(
        self: *const Tensor, target: *const Tensor, weight: ?*const Tensor, pos_weight: ?*const Tensor, reduction: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_binary_cross_entropy_with_logits(@ptrCast(&c_tensors), self.c_tensor,
                target.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                if (pos_weight != null) pos_weight.?.c_tensor else null,
                reduction.to_int());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn binaryCrossEntropyWithLogitsOut(
        self: *const Tensor, out: *const Tensor, target: *const Tensor, weight: ?*const Tensor, pos_weight: ?*const Tensor, reduction: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_binary_cross_entropy_with_logits_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                target.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                if (pos_weight != null) pos_weight.?.c_tensor else null,
                reduction.to_int());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bincount(
        self: *const Tensor, weights: ?*const Tensor, minlength: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bincount(@ptrCast(&c_tensors), self.c_tensor,
                if (weights != null) weights.?.c_tensor else null,
                minlength);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bincountOut(
        self: *const Tensor, out: *const Tensor, weights: ?*const Tensor, minlength: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bincount_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                if (weights != null) weights.?.c_tensor else null,
                minlength);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn binomial(
        count: *const Tensor, prob: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_binomial(@ptrCast(&c_tensors), count.c_tensor,
                prob.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn binomialOut(
        out: *const Tensor, count: *const Tensor, prob: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_binomial_out(@ptrCast(&c_tensors), out.c_tensor,
                count.c_tensor,
                prob.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseAnd(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_and(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseAnd_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_and_(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseAndScalarOut(
        self: *const Tensor, out: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_and_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseAndScalarTensor(
        self_scalar: Scalar, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_and_scalar_tensor(@ptrCast(&c_tensors), self_scalar.into().c_scalar,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseAndScalarTensorOut(
        out: *const Tensor, self_scalar: Scalar, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_and_scalar_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self_scalar.into().c_scalar,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseAndTensor(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_and_tensor(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseAndTensor_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_and_tensor_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseAndTensorOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_and_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseLeftShift(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_left_shift(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseLeftShift_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_left_shift_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseLeftShiftScalarTensor(
        self_scalar: Scalar, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_left_shift_scalar_tensor(@ptrCast(&c_tensors), self_scalar.into().c_scalar,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseLeftShiftScalarTensorOut(
        out: *const Tensor, self_scalar: Scalar, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_left_shift_scalar_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self_scalar.into().c_scalar,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseLeftShiftTensorOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_left_shift_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseLeftShiftTensorScalar(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_left_shift_tensor_scalar(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseLeftShiftTensorScalar_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_left_shift_tensor_scalar_(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseLeftShiftTensorScalarOut(
        self: *const Tensor, out: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_left_shift_tensor_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseNot(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_not(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseNot_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_not_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseNotOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_not_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseOr(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_or(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseOr_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_or_(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseOrScalarOut(
        self: *const Tensor, out: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_or_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseOrScalarTensor(
        self_scalar: Scalar, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_or_scalar_tensor(@ptrCast(&c_tensors), self_scalar.into().c_scalar,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseOrScalarTensorOut(
        out: *const Tensor, self_scalar: Scalar, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_or_scalar_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self_scalar.into().c_scalar,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseOrTensor(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_or_tensor(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseOrTensor_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_or_tensor_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseOrTensorOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_or_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseRightShift(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_right_shift(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseRightShift_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_right_shift_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseRightShiftScalarTensor(
        self_scalar: Scalar, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_right_shift_scalar_tensor(@ptrCast(&c_tensors), self_scalar.into().c_scalar,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseRightShiftScalarTensorOut(
        out: *const Tensor, self_scalar: Scalar, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_right_shift_scalar_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self_scalar.into().c_scalar,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseRightShiftTensorOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_right_shift_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseRightShiftTensorScalar(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_right_shift_tensor_scalar(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseRightShiftTensorScalar_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_right_shift_tensor_scalar_(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseRightShiftTensorScalarOut(
        self: *const Tensor, out: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_right_shift_tensor_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseXor(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_xor(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseXor_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_xor_(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseXorScalarOut(
        self: *const Tensor, out: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_xor_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseXorScalarTensor(
        self_scalar: Scalar, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_xor_scalar_tensor(@ptrCast(&c_tensors), self_scalar.into().c_scalar,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseXorScalarTensorOut(
        out: *const Tensor, self_scalar: Scalar, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_xor_scalar_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self_scalar.into().c_scalar,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseXorTensor(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_xor_tensor(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseXorTensor_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_xor_tensor_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bitwiseXorTensorOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bitwise_xor_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn blackmanWindow(
        window_length: i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_blackman_window(@ptrCast(&c_tensors), window_length,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn blackmanWindowOut(
        out: *const Tensor, window_length: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_blackman_window_out(@ptrCast(&c_tensors), out.c_tensor,
                window_length);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn blackmanWindowPeriodic(
        window_length: i64, periodic: bool, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_blackman_window_periodic(@ptrCast(&c_tensors), window_length,
                if (periodic)  1  else  0,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn blackmanWindowPeriodicOut(
        out: *const Tensor, window_length: i64, periodic: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_blackman_window_periodic_out(@ptrCast(&c_tensors), out.c_tensor,
                window_length,
                if (periodic)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn blockDiag(
        tensors: []*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_block_diag(@ptrCast(&c_tensors), ptrList(tensors).ptr, @intCast(tensors.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn blockDiagOut(
        out: *const Tensor, tensors: []*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_block_diag_out(@ptrCast(&c_tensors), out.c_tensor,
                ptrList(tensors).ptr, @intCast(tensors.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bmm(
        self: *const Tensor, mat2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bmm(@ptrCast(&c_tensors), self.c_tensor,
                mat2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bmmOut(
        self: *const Tensor, out: *const Tensor, mat2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bmm_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                mat2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn broadcastTensors(
        tensors: []*const Tensor
    ) []Tensor {
        const c_tensors = __c.atg_broadcast_tensors(ptrList(tensors).ptr, @intCast(tensors.len));
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn broadcastTo(
        self: *const Tensor, size_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_broadcast_to(@ptrCast(&c_tensors), self.c_tensor,
                size_.ptr, @intCast(size_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bucketize(
        self: *const Tensor, boundaries: *const Tensor, out_int32: bool, right: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bucketize(@ptrCast(&c_tensors), self.c_tensor,
                boundaries.c_tensor,
                if (out_int32)  1  else  0,
                if (right)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bucketizeScalar(
        self_scalar: Scalar, boundaries: *const Tensor, out_int32: bool, right: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bucketize_scalar(@ptrCast(&c_tensors), self_scalar.into().c_scalar,
                boundaries.c_tensor,
                if (out_int32)  1  else  0,
                if (right)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bucketizeScalarOut(
        out: *const Tensor, self_scalar: Scalar, boundaries: *const Tensor, out_int32: bool, right: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bucketize_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self_scalar.into().c_scalar,
                boundaries.c_tensor,
                if (out_int32)  1  else  0,
                if (right)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn bucketizeTensorOut(
        self: *const Tensor, out: *const Tensor, boundaries: *const Tensor, out_int32: bool, right: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_bucketize_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                boundaries.c_tensor,
                if (out_int32)  1  else  0,
                if (right)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn canCast(
        from: Kind, to_: Kind
    ) bool {
        const return_ = __c.atg_can_cast(from.cInt(),
                to_.cInt());
       torch.readAndCleanError();
        return return_ != 0;
    }

    pub fn cartesianProd(
        tensors: []*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cartesian_prod(@ptrCast(&c_tensors), ptrList(tensors).ptr, @intCast(tensors.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cat(
        tensors: []*const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cat(@ptrCast(&c_tensors), ptrList(tensors).ptr, @intCast(tensors.len),
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn catOut(
        out: *const Tensor, tensors: []*const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cat_out(@ptrCast(&c_tensors), out.c_tensor,
                ptrList(tensors).ptr, @intCast(tensors.len),
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cauchy(
        self: *const Tensor, median_: f64, sigma: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cauchy(@ptrCast(&c_tensors), self.c_tensor,
                median_,
                sigma);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cauchy_(
        self: *Tensor, median_: f64, sigma: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cauchy_(@ptrCast(&c_tensors), self.c_tensor,
                median_,
                sigma);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cauchyOut(
        self: *const Tensor, out: *const Tensor, median_: f64, sigma: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cauchy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                median_,
                sigma);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn ccolIndices(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_ccol_indices(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn ccolIndicesCopy(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_ccol_indices_copy(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn ccolIndicesCopyOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_ccol_indices_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cdist(
        x1: *const Tensor, x2: *const Tensor, p: f64, compute_mode: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cdist(@ptrCast(&c_tensors), x1.c_tensor,
                x2.c_tensor,
                p,
                compute_mode orelse 0, (compute_mode == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn ceil(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_ceil(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn ceil_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_ceil_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn ceilOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_ceil_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn celu(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_celu(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn celu_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_celu_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn celuOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_celu_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn chainMatmul(
        matrices: []*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_chain_matmul(@ptrCast(&c_tensors), ptrList(matrices).ptr, @intCast(matrices.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn chainMatmulOut(
        out: *const Tensor, matrices: []*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_chain_matmul_out(@ptrCast(&c_tensors), out.c_tensor,
                ptrList(matrices).ptr, @intCast(matrices.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn chalf(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_chalf(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn channelShuffle(
        self: *const Tensor, groups: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_channel_shuffle(@ptrCast(&c_tensors), self.c_tensor,
                groups);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn channelShuffleOut(
        self: *const Tensor, out: *const Tensor, groups: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_channel_shuffle_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                groups);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cholesky(
        self: *const Tensor, upper: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cholesky(@ptrCast(&c_tensors), self.c_tensor,
                if (upper)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn choleskyInverse(
        self: *const Tensor, upper: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cholesky_inverse(@ptrCast(&c_tensors), self.c_tensor,
                if (upper)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn choleskyInverseOut(
        self: *const Tensor, out: *const Tensor, upper: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cholesky_inverse_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                if (upper)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn choleskyOut(
        self: *const Tensor, out: *const Tensor, upper: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cholesky_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                if (upper)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn choleskySolve(
        self: *const Tensor, input2: *const Tensor, upper: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cholesky_solve(@ptrCast(&c_tensors), self.c_tensor,
                input2.c_tensor,
                if (upper)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn choleskySolveOut(
        self: *const Tensor, out: *const Tensor, input2: *const Tensor, upper: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cholesky_solve_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                input2.c_tensor,
                if (upper)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn chooseQparamsOptimized(
        self: *const Tensor, numel_: i64, n_bins: i64, ratio: f64, bit_width: i64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_choose_qparams_optimized(@ptrCast(&c_tensors), self.c_tensor,
                numel_,
                n_bins,
                ratio,
                bit_width);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn chunk(
        self: *const Tensor, chunks: i64, dim_: i64
    ) []Tensor {
        const c_tensors = __c.atg_chunk(self.c_tensor,
                chunks,
                dim_);
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn clamp(
        self: *const Tensor, min_: Scalar, max_: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_clamp(@ptrCast(&c_tensors), self.c_tensor,
                min_.into().c_scalar,
                max_.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn clamp_(
        self: *Tensor, min_: Scalar, max_: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_clamp_(@ptrCast(&c_tensors), self.c_tensor,
                min_.into().c_scalar,
                max_.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn clampMax(
        self: *const Tensor, max_: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_clamp_max(@ptrCast(&c_tensors), self.c_tensor,
                max_.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn clampMax_(
        self: *Tensor, max_: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_clamp_max_(@ptrCast(&c_tensors), self.c_tensor,
                max_.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn clampMaxOut(
        self: *const Tensor, out: *const Tensor, max_: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_clamp_max_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                max_.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn clampMaxTensor(
        self: *const Tensor, max_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_clamp_max_tensor(@ptrCast(&c_tensors), self.c_tensor,
                max_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn clampMaxTensor_(
        self: *Tensor, max_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_clamp_max_tensor_(@ptrCast(&c_tensors), self.c_tensor,
                max_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn clampMaxTensorOut(
        self: *const Tensor, out: *const Tensor, max_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_clamp_max_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                max_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn clampMin(
        self: *const Tensor, min_: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_clamp_min(@ptrCast(&c_tensors), self.c_tensor,
                min_.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn clampMin_(
        self: *Tensor, min_: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_clamp_min_(@ptrCast(&c_tensors), self.c_tensor,
                min_.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn clampMinOut(
        self: *const Tensor, out: *const Tensor, min_: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_clamp_min_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                min_.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn clampMinTensor(
        self: *const Tensor, min_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_clamp_min_tensor(@ptrCast(&c_tensors), self.c_tensor,
                min_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn clampMinTensor_(
        self: *Tensor, min_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_clamp_min_tensor_(@ptrCast(&c_tensors), self.c_tensor,
                min_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn clampMinTensorOut(
        self: *const Tensor, out: *const Tensor, min_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_clamp_min_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                min_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn clampOut(
        self: *const Tensor, out: *const Tensor, min_: Scalar, max_: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_clamp_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                min_.into().c_scalar,
                max_.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn clampTensor(
        self: *const Tensor, min_: ?*const Tensor, max_: ?*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_clamp_tensor(@ptrCast(&c_tensors), self.c_tensor,
                if (min_ != null) min_.?.c_tensor else null,
                if (max_ != null) max_.?.c_tensor else null);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn clampTensor_(
        self: *Tensor, min_: ?*const Tensor, max_: ?*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_clamp_tensor_(@ptrCast(&c_tensors), self.c_tensor,
                if (min_ != null) min_.?.c_tensor else null,
                if (max_ != null) max_.?.c_tensor else null);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn clampTensorOut(
        self: *const Tensor, out: *const Tensor, min_: ?*const Tensor, max_: ?*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_clamp_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                if (min_ != null) min_.?.c_tensor else null,
                if (max_ != null) max_.?.c_tensor else null);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn clip(
        self: *const Tensor, min_: Scalar, max_: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_clip(@ptrCast(&c_tensors), self.c_tensor,
                min_.into().c_scalar,
                max_.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn clip_(
        self: *Tensor, min_: Scalar, max_: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_clip_(@ptrCast(&c_tensors), self.c_tensor,
                min_.into().c_scalar,
                max_.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn clipOut(
        self: *const Tensor, out: *const Tensor, min_: Scalar, max_: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_clip_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                min_.into().c_scalar,
                max_.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn clipTensor(
        self: *const Tensor, min_: ?*const Tensor, max_: ?*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_clip_tensor(@ptrCast(&c_tensors), self.c_tensor,
                if (min_ != null) min_.?.c_tensor else null,
                if (max_ != null) max_.?.c_tensor else null);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn clipTensor_(
        self: *Tensor, min_: ?*const Tensor, max_: ?*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_clip_tensor_(@ptrCast(&c_tensors), self.c_tensor,
                if (min_ != null) min_.?.c_tensor else null,
                if (max_ != null) max_.?.c_tensor else null);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn clipTensorOut(
        self: *const Tensor, out: *const Tensor, min_: ?*const Tensor, max_: ?*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_clip_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                if (min_ != null) min_.?.c_tensor else null,
                if (max_ != null) max_.?.c_tensor else null);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn clone(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_clone(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn coalesce(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_coalesce(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn col2im(
        self: *const Tensor, output_size: []i64, kernel_size: []i64, dilation: []i64, padding: []i64, stride_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_col2im(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                kernel_size.ptr, @intCast(kernel_size.len),
                dilation.ptr, @intCast(dilation.len),
                padding.ptr, @intCast(padding.len),
                stride_.ptr, @intCast(stride_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn col2imOut(
        self: *const Tensor, out: *const Tensor, output_size: []i64, kernel_size: []i64, dilation: []i64, padding: []i64, stride_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_col2im_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                kernel_size.ptr, @intCast(kernel_size.len),
                dilation.ptr, @intCast(dilation.len),
                padding.ptr, @intCast(padding.len),
                stride_.ptr, @intCast(stride_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn colIndices(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_col_indices(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn colIndicesCopy(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_col_indices_copy(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn colIndicesCopyOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_col_indices_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn columnStack(
        tensors: []*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_column_stack(@ptrCast(&c_tensors), ptrList(tensors).ptr, @intCast(tensors.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn columnStackOut(
        out: *const Tensor, tensors: []*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_column_stack_out(@ptrCast(&c_tensors), out.c_tensor,
                ptrList(tensors).ptr, @intCast(tensors.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn combinations(
        self: *const Tensor, r: i64, with_replacement: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_combinations(@ptrCast(&c_tensors), self.c_tensor,
                r,
                if (with_replacement)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn complex(
        real_: *const Tensor, imag_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_complex(@ptrCast(&c_tensors), real_.c_tensor,
                imag_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn complexOut(
        out: *const Tensor, real_: *const Tensor, imag_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_complex_out(@ptrCast(&c_tensors), out.c_tensor,
                real_.c_tensor,
                imag_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn concat(
        tensors: []*const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_concat(@ptrCast(&c_tensors), ptrList(tensors).ptr, @intCast(tensors.len),
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn concatOut(
        out: *const Tensor, tensors: []*const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_concat_out(@ptrCast(&c_tensors), out.c_tensor,
                ptrList(tensors).ptr, @intCast(tensors.len),
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn concatenate(
        tensors: []*const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_concatenate(@ptrCast(&c_tensors), ptrList(tensors).ptr, @intCast(tensors.len),
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn concatenateOut(
        out: *const Tensor, tensors: []*const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_concatenate_out(@ptrCast(&c_tensors), out.c_tensor,
                ptrList(tensors).ptr, @intCast(tensors.len),
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn conj(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_conj(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn conjPhysical(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_conj_physical(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn conjPhysical_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_conj_physical_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn conjPhysicalOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_conj_physical_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn constantPadNd(
        self: *const Tensor, padding: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_constant_pad_nd(@ptrCast(&c_tensors), self.c_tensor,
                padding.ptr, @intCast(padding.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn constantPadNdOut(
        self: *const Tensor, out: *const Tensor, padding: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_constant_pad_nd_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                padding.ptr, @intCast(padding.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn contiguous(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_contiguous(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn conv1d(
        self: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, stride_: []i64, padding: []i64, dilation: []i64, groups: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_conv1d(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                groups);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn conv1dPadding(
        self: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, stride_: []i64, padding: []const u8, dilation: []i64, groups: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_conv1d_padding(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, padding.len,
                dilation.ptr, @intCast(dilation.len),
                groups);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn conv2d(
        self: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, stride_: []i64, padding: []i64, dilation: []i64, groups: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_conv2d(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                groups);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn conv2dPadding(
        self: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, stride_: []i64, padding: []const u8, dilation: []i64, groups: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_conv2d_padding(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, padding.len,
                dilation.ptr, @intCast(dilation.len),
                groups);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn conv3d(
        self: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, stride_: []i64, padding: []i64, dilation: []i64, groups: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_conv3d(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                groups);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn conv3dPadding(
        self: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, stride_: []i64, padding: []const u8, dilation: []i64, groups: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_conv3d_padding(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, padding.len,
                dilation.ptr, @intCast(dilation.len),
                groups);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn convDepthwise3d(
        self: *const Tensor, weight: *const Tensor, kernel_size: []i64, bias: ?*const Tensor, stride_: []i64, padding: []i64, dilation: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_conv_depthwise3d(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn convDepthwise3dOut(
        self: *const Tensor, out: *const Tensor, weight: *const Tensor, kernel_size: []i64, bias: ?*const Tensor, stride_: []i64, padding: []i64, dilation: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_conv_depthwise3d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                weight.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn convTbc(
        self: *const Tensor, weight: *const Tensor, bias: *const Tensor, padding: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_conv_tbc(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                bias.c_tensor,
                padding);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn convTbcBackward(
        self: *const Tensor, input: *const Tensor, weight: *const Tensor, bias: *const Tensor, padding: i64
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_conv_tbc_backward(@ptrCast(&c_tensors), self.c_tensor,
                input.c_tensor,
                weight.c_tensor,
                bias.c_tensor,
                padding);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn convTbcOut(
        self: *const Tensor, out: *const Tensor, weight: *const Tensor, bias: *const Tensor, padding: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_conv_tbc_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                weight.c_tensor,
                bias.c_tensor,
                padding);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn convTranspose1d(
        self: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, stride_: []i64, padding: []i64, output_padding: []i64, groups: i64, dilation: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_conv_transpose1d(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                output_padding.ptr, @intCast(output_padding.len),
                groups,
                dilation.ptr, @intCast(dilation.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn convTranspose2d(
        self: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, stride_: []i64, padding: []i64, output_padding: []i64, groups: i64, dilation: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_conv_transpose2d(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                output_padding.ptr, @intCast(output_padding.len),
                groups,
                dilation.ptr, @intCast(dilation.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn convTranspose3d(
        self: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, stride_: []i64, padding: []i64, output_padding: []i64, groups: i64, dilation: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_conv_transpose3d(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                output_padding.ptr, @intCast(output_padding.len),
                groups,
                dilation.ptr, @intCast(dilation.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn convolution(
        self: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, stride_: []i64, padding: []i64, dilation: []i64, transposed: bool, output_padding: []i64, groups: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_convolution(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (transposed)  1  else  0,
                output_padding.ptr, @intCast(output_padding.len),
                groups);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn convolutionOut(
        self: *const Tensor, out: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, stride_: []i64, padding: []i64, dilation: []i64, transposed: bool, output_padding: []i64, groups: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_convolution_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (transposed)  1  else  0,
                output_padding.ptr, @intCast(output_padding.len),
                groups);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn convolutionOverrideable(
        self: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, stride_: []i64, padding: []i64, dilation: []i64, transposed: bool, output_padding: []i64, groups: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_convolution_overrideable(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (transposed)  1  else  0,
                output_padding.ptr, @intCast(output_padding.len),
                groups);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn convolutionOverrideableOut(
        self: *const Tensor, out: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, stride_: []i64, padding: []i64, dilation: []i64, transposed: bool, output_padding: []i64, groups: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_convolution_overrideable_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (transposed)  1  else  0,
                output_padding.ptr, @intCast(output_padding.len),
                groups);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn copySparseToSparse(
        self: *const Tensor, src: *const Tensor, non_blocking: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_copy_sparse_to_sparse(@ptrCast(&c_tensors), self.c_tensor,
                src.c_tensor,
                if (non_blocking)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn copySparseToSparse_(
        self: *Tensor, src: *const Tensor, non_blocking: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_copy_sparse_to_sparse_(@ptrCast(&c_tensors), self.c_tensor,
                src.c_tensor,
                if (non_blocking)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn copySparseToSparseOut(
        self: *const Tensor, out: *const Tensor, src: *const Tensor, non_blocking: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_copy_sparse_to_sparse_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                src.c_tensor,
                if (non_blocking)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn copysign(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_copysign(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn copysign_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_copysign_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn copysignOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_copysign_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn copysignScalar(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_copysign_scalar(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn copysignScalar_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_copysign_scalar_(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn copysignScalarOut(
        self: *const Tensor, out: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_copysign_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn corrcoef(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_corrcoef(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cos(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cos(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cos_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cos_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cosOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cos_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cosh(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cosh(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cosh_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cosh_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn coshOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cosh_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cosineEmbeddingLoss(
        input1: *const Tensor, input2: *const Tensor, target: *const Tensor, margin: f64, reduction: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cosine_embedding_loss(@ptrCast(&c_tensors), input1.c_tensor,
                input2.c_tensor,
                target.c_tensor,
                margin,
                reduction.to_int());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cosineSimilarity(
        x1: *const Tensor, x2: *const Tensor, dim_: i64, eps: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cosine_similarity(@ptrCast(&c_tensors), x1.c_tensor,
                x2.c_tensor,
                dim_,
                eps);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn countNonzero(
        self: *const Tensor, dim_: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_count_nonzero(@ptrCast(&c_tensors), self.c_tensor,
                dim_ orelse 0, (dim_ == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn countNonzeroDimIntlist(
        self: *const Tensor, dim_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_count_nonzero_dim_intlist(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn countNonzeroDimIntlistOut(
        self: *const Tensor, out: *const Tensor, dim_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_count_nonzero_dim_intlist_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_.ptr, @intCast(dim_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn countNonzeroOut(
        self: *const Tensor, out: *const Tensor, dim_: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_count_nonzero_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_ orelse 0, (dim_ == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cov(
        self: *const Tensor, correction: i64, fweights: ?*const Tensor, aweights: ?*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cov(@ptrCast(&c_tensors), self.c_tensor,
                correction,
                if (fweights != null) fweights.?.c_tensor else null,
                if (aweights != null) aweights.?.c_tensor else null);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cross(
        self: *const Tensor, other: *const Tensor, dim_: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cross(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor,
                dim_ orelse 0, (dim_ == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn crossEntropyLoss(
        self: *const Tensor, target: *const Tensor, weight: ?*const Tensor, reduction: i64, ignore_index: i64, label_smoothing: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cross_entropy_loss(@ptrCast(&c_tensors), self.c_tensor,
                target.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                reduction.to_int(),
                ignore_index,
                label_smoothing);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn crossOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor, dim_: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cross_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor,
                dim_ orelse 0, (dim_ == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn crowIndices(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_crow_indices(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn crowIndicesCopy(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_crow_indices_copy(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn crowIndicesCopyOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_crow_indices_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn ctcLoss(
        log_probs: *const Tensor, targets: *const Tensor, input_lengths: []i64, target_lengths: []i64, blank: i64, reduction: i64, zero_infinity: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_ctc_loss(@ptrCast(&c_tensors), log_probs.c_tensor,
                targets.c_tensor,
                input_lengths.ptr, @intCast(input_lengths.len),
                target_lengths.ptr, @intCast(target_lengths.len),
                blank,
                reduction.to_int(),
                if (zero_infinity)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn ctcLossTensor(
        log_probs: *const Tensor, targets: *const Tensor, input_lengths: *const Tensor, target_lengths: *const Tensor, blank: i64, reduction: i64, zero_infinity: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_ctc_loss_tensor(@ptrCast(&c_tensors), log_probs.c_tensor,
                targets.c_tensor,
                input_lengths.c_tensor,
                target_lengths.c_tensor,
                blank,
                reduction.to_int(),
                if (zero_infinity)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cudnnAffineGridGenerator(
        theta: *const Tensor, n: i64, c: i64, h: i64, w: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cudnn_affine_grid_generator(@ptrCast(&c_tensors), theta.c_tensor,
                n,
                c,
                h,
                w);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cudnnAffineGridGeneratorBackward(
        gradient: *const Tensor, n: i64, c: i64, h: i64, w: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cudnn_affine_grid_generator_backward(@ptrCast(&c_tensors), gradient.c_tensor,
                n,
                c,
                h,
                w);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cudnnAffineGridGeneratorBackwardOut(
        out: *const Tensor, gradient: *const Tensor, n: i64, c: i64, h: i64, w: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cudnn_affine_grid_generator_backward_out(@ptrCast(&c_tensors), out.c_tensor,
                gradient.c_tensor,
                n,
                c,
                h,
                w);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cudnnAffineGridGeneratorOut(
        out: *const Tensor, theta: *const Tensor, n: i64, c: i64, h: i64, w: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cudnn_affine_grid_generator_out(@ptrCast(&c_tensors), out.c_tensor,
                theta.c_tensor,
                n,
                c,
                h,
                w);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cudnnBatchNorm(
        self: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, running_mean: ?*const Tensor, running_var: ?*const Tensor, training: bool, exponential_average_factor: f64, epsilon: f64
    ) [4]Tensor {
        var c_tensors = [_]C_tensor{null} ** 4;
        __c.atg_cudnn_batch_norm(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                if (running_mean != null) running_mean.?.c_tensor else null,
                if (running_var != null) running_var.?.c_tensor else null,
                if (training)  1  else  0,
                exponential_average_factor,
                epsilon);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }, Tensor { .c_tensor = c_tensors[3] }};
    }

    pub fn cudnnBatchNormBackward(
        self: *const Tensor, grad_output: *const Tensor, weight: *const Tensor, running_mean: ?*const Tensor, running_var: ?*const Tensor, save_mean: ?*const Tensor, save_var: ?*const Tensor, epsilon: f64, reservespace: *const Tensor
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_cudnn_batch_norm_backward(@ptrCast(&c_tensors), self.c_tensor,
                grad_output.c_tensor,
                weight.c_tensor,
                if (running_mean != null) running_mean.?.c_tensor else null,
                if (running_var != null) running_var.?.c_tensor else null,
                if (save_mean != null) save_mean.?.c_tensor else null,
                if (save_var != null) save_var.?.c_tensor else null,
                epsilon,
                reservespace.c_tensor);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn cudnnBatchNormBackwardOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, out2: *const Tensor, grad_output: *const Tensor, weight: *const Tensor, running_mean: ?*const Tensor, running_var: ?*const Tensor, save_mean: ?*const Tensor, save_var: ?*const Tensor, epsilon: f64, reservespace: *const Tensor
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_cudnn_batch_norm_backward_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                out2.c_tensor,
                self.c_tensor,
                grad_output.c_tensor,
                weight.c_tensor,
                if (running_mean != null) running_mean.?.c_tensor else null,
                if (running_var != null) running_var.?.c_tensor else null,
                if (save_mean != null) save_mean.?.c_tensor else null,
                if (save_var != null) save_var.?.c_tensor else null,
                epsilon,
                reservespace.c_tensor);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn cudnnBatchNormOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, out2: *const Tensor, out3: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, running_mean: ?*const Tensor, running_var: ?*const Tensor, training: bool, exponential_average_factor: f64, epsilon: f64
    ) [4]Tensor {
        var c_tensors = [_]C_tensor{null} ** 4;
        __c.atg_cudnn_batch_norm_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                out2.c_tensor,
                out3.c_tensor,
                self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                if (running_mean != null) running_mean.?.c_tensor else null,
                if (running_var != null) running_var.?.c_tensor else null,
                if (training)  1  else  0,
                exponential_average_factor,
                epsilon);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }, Tensor { .c_tensor = c_tensors[3] }};
    }

    pub fn cudnnConvolution(
        self: *const Tensor, weight: *const Tensor, padding: []i64, stride_: []i64, dilation: []i64, groups: i64, benchmark: bool, deterministic: bool, allow_tf32: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cudnn_convolution(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                padding.ptr, @intCast(padding.len),
                stride_.ptr, @intCast(stride_.len),
                dilation.ptr, @intCast(dilation.len),
                groups,
                if (benchmark)  1  else  0,
                if (deterministic)  1  else  0,
                if (allow_tf32)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cudnnConvolutionAddRelu(
        self: *const Tensor, weight: *const Tensor, z: *const Tensor, alpha: Scalar, bias: ?*const Tensor, stride_: []i64, padding: []i64, dilation: []i64, groups: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cudnn_convolution_add_relu(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                z.c_tensor,
                alpha.into().c_scalar,
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                groups);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cudnnConvolutionAddReluOut(
        self: *const Tensor, out: *const Tensor, weight: *const Tensor, z: *const Tensor, alpha: Scalar, bias: ?*const Tensor, stride_: []i64, padding: []i64, dilation: []i64, groups: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cudnn_convolution_add_relu_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                weight.c_tensor,
                z.c_tensor,
                alpha.into().c_scalar,
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                groups);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cudnnConvolutionOut(
        self: *const Tensor, out: *const Tensor, weight: *const Tensor, padding: []i64, stride_: []i64, dilation: []i64, groups: i64, benchmark: bool, deterministic: bool, allow_tf32: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cudnn_convolution_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                weight.c_tensor,
                padding.ptr, @intCast(padding.len),
                stride_.ptr, @intCast(stride_.len),
                dilation.ptr, @intCast(dilation.len),
                groups,
                if (benchmark)  1  else  0,
                if (deterministic)  1  else  0,
                if (allow_tf32)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cudnnConvolutionRelu(
        self: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, stride_: []i64, padding: []i64, dilation: []i64, groups: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cudnn_convolution_relu(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                groups);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cudnnConvolutionReluOut(
        self: *const Tensor, out: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, stride_: []i64, padding: []i64, dilation: []i64, groups: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cudnn_convolution_relu_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                groups);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cudnnConvolutionTranspose(
        self: *const Tensor, weight: *const Tensor, padding: []i64, output_padding: []i64, stride_: []i64, dilation: []i64, groups: i64, benchmark: bool, deterministic: bool, allow_tf32: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cudnn_convolution_transpose(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                padding.ptr, @intCast(padding.len),
                output_padding.ptr, @intCast(output_padding.len),
                stride_.ptr, @intCast(stride_.len),
                dilation.ptr, @intCast(dilation.len),
                groups,
                if (benchmark)  1  else  0,
                if (deterministic)  1  else  0,
                if (allow_tf32)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cudnnConvolutionTransposeOut(
        self: *const Tensor, out: *const Tensor, weight: *const Tensor, padding: []i64, output_padding: []i64, stride_: []i64, dilation: []i64, groups: i64, benchmark: bool, deterministic: bool, allow_tf32: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cudnn_convolution_transpose_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                weight.c_tensor,
                padding.ptr, @intCast(padding.len),
                output_padding.ptr, @intCast(output_padding.len),
                stride_.ptr, @intCast(stride_.len),
                dilation.ptr, @intCast(dilation.len),
                groups,
                if (benchmark)  1  else  0,
                if (deterministic)  1  else  0,
                if (allow_tf32)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cudnnGridSampler(
        self: *const Tensor, grid: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cudnn_grid_sampler(@ptrCast(&c_tensors), self.c_tensor,
                grid.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cudnnGridSamplerBackward(
        self: *const Tensor, grid: *const Tensor, grad_output: *const Tensor
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_cudnn_grid_sampler_backward(@ptrCast(&c_tensors), self.c_tensor,
                grid.c_tensor,
                grad_output.c_tensor);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn cudnnGridSamplerBackwardOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, grid: *const Tensor, grad_output: *const Tensor
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_cudnn_grid_sampler_backward_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                self.c_tensor,
                grid.c_tensor,
                grad_output.c_tensor);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn cudnnGridSamplerOut(
        self: *const Tensor, out: *const Tensor, grid: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cudnn_grid_sampler_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                grid.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cudnnIsAcceptable(
        self: *const Tensor, 
    ) bool {
        const return_ = __c.atg_cudnn_is_acceptable(self.c_tensor);
       torch.readAndCleanError();
        return return_ != 0;
    }

    pub fn cummax(
        self: *const Tensor, dim_: i64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_cummax(@ptrCast(&c_tensors), self.c_tensor,
                dim_);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn cummaxOut(
        self: *const Tensor, values_: *const Tensor, indices_: *const Tensor, dim_: i64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_cummax_out(@ptrCast(&c_tensors), values_.c_tensor,
                indices_.c_tensor,
                self.c_tensor,
                dim_);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn cummaxminBackward(
        self: *const Tensor, gradient: *const Tensor, indices_: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cummaxmin_backward(@ptrCast(&c_tensors), gradient.c_tensor,
                self.c_tensor,
                indices_.c_tensor,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cummin(
        self: *const Tensor, dim_: i64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_cummin(@ptrCast(&c_tensors), self.c_tensor,
                dim_);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn cumminOut(
        self: *const Tensor, values_: *const Tensor, indices_: *const Tensor, dim_: i64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_cummin_out(@ptrCast(&c_tensors), values_.c_tensor,
                indices_.c_tensor,
                self.c_tensor,
                dim_);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn cumprod(
        self: *const Tensor, dim_: i64, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cumprod(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cumprod_(
        self: *Tensor, dim_: i64, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cumprod_(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cumprodBackward(
        self: *const Tensor, gradient: *const Tensor, dim_: i64, output: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cumprod_backward(@ptrCast(&c_tensors), gradient.c_tensor,
                self.c_tensor,
                dim_,
                output.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cumprodOut(
        self: *const Tensor, out: *const Tensor, dim_: i64, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cumprod_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cumsum(
        self: *const Tensor, dim_: i64, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cumsum(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cumsum_(
        self: *Tensor, dim_: i64, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cumsum_(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cumsumOut(
        self: *const Tensor, out: *const Tensor, dim_: i64, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cumsum_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cumulativeTrapezoid(
        y: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cumulative_trapezoid(@ptrCast(&c_tensors), y.c_tensor,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn cumulativeTrapezoidX(
        y: *const Tensor, x: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_cumulative_trapezoid_x(@ptrCast(&c_tensors), y.c_tensor,
                x.c_tensor,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn data(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_data(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn deg2rad(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_deg2rad(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn deg2rad_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_deg2rad_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn deg2radOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_deg2rad_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn denseDim(
        self: *const Tensor, 
    ) i64 {
        const return_ = __c.atg_dense_dim(self.c_tensor);
       torch.readAndCleanError();
        return return_;
    }

    pub fn dequantize(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_dequantize(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn dequantizeSelfOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_dequantize_self_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn dequantizeTensors(
        tensors: []*const Tensor
    ) []Tensor {
        const c_tensors = __c.atg_dequantize_tensors(ptrList(tensors).ptr, @intCast(tensors.len));
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn dequantizeTensorsOut(
        out: []*const Tensor, tensors: []*const Tensor
    ) void {
        __c.atg_dequantize_tensors_out(ptrList(out).ptr, @intCast(out.len),
                ptrList(tensors).ptr, @intCast(tensors.len));
        torch.readAndCleanError();
        return;
    }

    pub fn det(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_det(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn detach(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_detach(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn detach_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_detach_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn detachCopy(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_detach_copy(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn detachCopyOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_detach_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn diag(
        self: *const Tensor, diagonal_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_diag(@ptrCast(&c_tensors), self.c_tensor,
                diagonal_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn diagEmbed(
        self: *const Tensor, offset: i64, dim1: i64, dim2: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_diag_embed(@ptrCast(&c_tensors), self.c_tensor,
                offset,
                dim1,
                dim2);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn diagEmbedOut(
        self: *const Tensor, out: *const Tensor, offset: i64, dim1: i64, dim2: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_diag_embed_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                offset,
                dim1,
                dim2);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn diagOut(
        self: *const Tensor, out: *const Tensor, diagonal_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_diag_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                diagonal_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn diagflat(
        self: *const Tensor, offset: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_diagflat(@ptrCast(&c_tensors), self.c_tensor,
                offset);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn diagonal(
        self: *const Tensor, offset: i64, dim1: i64, dim2: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_diagonal(@ptrCast(&c_tensors), self.c_tensor,
                offset,
                dim1,
                dim2);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn diagonalBackward(
        grad_output: *const Tensor, input_sizes: []i64, offset: i64, dim1: i64, dim2: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_diagonal_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                input_sizes.ptr, @intCast(input_sizes.len),
                offset,
                dim1,
                dim2);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn diagonalBackwardOut(
        out: *const Tensor, grad_output: *const Tensor, input_sizes: []i64, offset: i64, dim1: i64, dim2: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_diagonal_backward_out(@ptrCast(&c_tensors), out.c_tensor,
                grad_output.c_tensor,
                input_sizes.ptr, @intCast(input_sizes.len),
                offset,
                dim1,
                dim2);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn diagonalCopy(
        self: *const Tensor, offset: i64, dim1: i64, dim2: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_diagonal_copy(@ptrCast(&c_tensors), self.c_tensor,
                offset,
                dim1,
                dim2);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn diagonalCopyOut(
        self: *const Tensor, out: *const Tensor, offset: i64, dim1: i64, dim2: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_diagonal_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                offset,
                dim1,
                dim2);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn diagonalScatter(
        self: *const Tensor, src: *const Tensor, offset: i64, dim1: i64, dim2: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_diagonal_scatter(@ptrCast(&c_tensors), self.c_tensor,
                src.c_tensor,
                offset,
                dim1,
                dim2);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn diagonalScatterOut(
        self: *const Tensor, out: *const Tensor, src: *const Tensor, offset: i64, dim1: i64, dim2: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_diagonal_scatter_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                src.c_tensor,
                offset,
                dim1,
                dim2);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn diff(
        self: *const Tensor, n: i64, dim_: i64, prepend: ?*const Tensor, append: ?*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_diff(@ptrCast(&c_tensors), self.c_tensor,
                n,
                dim_,
                if (prepend != null) prepend.?.c_tensor else null,
                if (append != null) append.?.c_tensor else null);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn diffOut(
        self: *const Tensor, out: *const Tensor, n: i64, dim_: i64, prepend: ?*const Tensor, append: ?*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_diff_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                n,
                dim_,
                if (prepend != null) prepend.?.c_tensor else null,
                if (append != null) append.?.c_tensor else null);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn digamma(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_digamma(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn digamma_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_digamma_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn digammaOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_digamma_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn dist(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_dist(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn distOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_dist_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn div(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_div(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn div_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_div_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn divOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_div_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn divOutMode(
        self: *const Tensor, out: *const Tensor, other: *const Tensor, rounding_mode: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_div_out_mode(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor,
                rounding_mode.ptr, rounding_mode.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn divScalar(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_div_scalar(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn divScalar_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_div_scalar_(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn divScalarMode(
        self: *const Tensor, other: Scalar, rounding_mode: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_div_scalar_mode(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar,
                rounding_mode.ptr, rounding_mode.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn divScalarMode_(
        self: *Tensor, other: Scalar, rounding_mode: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_div_scalar_mode_(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar,
                rounding_mode.ptr, rounding_mode.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn divScalarModeOut(
        self: *const Tensor, out: *const Tensor, other: Scalar, rounding_mode: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_div_scalar_mode_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar,
                rounding_mode.ptr, rounding_mode.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn divScalarOut(
        self: *const Tensor, out: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_div_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn divTensorMode(
        self: *const Tensor, other: *const Tensor, rounding_mode: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_div_tensor_mode(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor,
                rounding_mode.ptr, rounding_mode.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn divTensorMode_(
        self: *Tensor, other: *const Tensor, rounding_mode: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_div_tensor_mode_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor,
                rounding_mode.ptr, rounding_mode.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn divide(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_divide(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn divide_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_divide_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn divideOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_divide_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn divideOutMode(
        self: *const Tensor, out: *const Tensor, other: *const Tensor, rounding_mode: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_divide_out_mode(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor,
                rounding_mode.ptr, rounding_mode.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn divideScalar(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_divide_scalar(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn divideScalar_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_divide_scalar_(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn divideScalarMode(
        self: *const Tensor, other: Scalar, rounding_mode: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_divide_scalar_mode(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar,
                rounding_mode.ptr, rounding_mode.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn divideScalarMode_(
        self: *Tensor, other: Scalar, rounding_mode: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_divide_scalar_mode_(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar,
                rounding_mode.ptr, rounding_mode.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn divideTensorMode(
        self: *const Tensor, other: *const Tensor, rounding_mode: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_divide_tensor_mode(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor,
                rounding_mode.ptr, rounding_mode.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn divideTensorMode_(
        self: *Tensor, other: *const Tensor, rounding_mode: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_divide_tensor_mode_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor,
                rounding_mode.ptr, rounding_mode.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn dot(
        self: *const Tensor, tensor: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_dot(@ptrCast(&c_tensors), self.c_tensor,
                tensor.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn dotOut(
        self: *const Tensor, out: *const Tensor, tensor: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_dot_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                tensor.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn dropout(
        self: *const Tensor, p: f64, train: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_dropout(@ptrCast(&c_tensors), self.c_tensor,
                p,
                if (train)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn dropout2(
        self: *Tensor, p: f64, train: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_dropout_(@ptrCast(&c_tensors), self.c_tensor,
                p,
                if (train)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn dsplit(
        self: *const Tensor, sections: i64
    ) []Tensor {
        const c_tensors = __c.atg_dsplit(self.c_tensor,
                sections);
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn dsplitArray(
        self: *const Tensor, indices_: []i64
    ) []Tensor {
        const c_tensors = __c.atg_dsplit_array(self.c_tensor,
                indices_.ptr, @intCast(indices_.len));
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn dstack(
        tensors: []*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_dstack(@ptrCast(&c_tensors), ptrList(tensors).ptr, @intCast(tensors.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn dstackOut(
        out: *const Tensor, tensors: []*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_dstack_out(@ptrCast(&c_tensors), out.c_tensor,
                ptrList(tensors).ptr, @intCast(tensors.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn einsum(
        equation: []const u8, tensors: []*const Tensor, path: ?[]i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_einsum(@ptrCast(&c_tensors), equation.ptr, equation.len,
                ptrList(tensors).ptr, @intCast(tensors.len),
                path.ptr, @intCast(path.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn elu(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_elu(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn elu_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_elu_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn eluBackward(
        grad_output: *const Tensor, alpha: Scalar, scale: Scalar, input_scale: Scalar, is_result: bool, self_or_result: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_elu_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                alpha.into().c_scalar,
                scale.into().c_scalar,
                input_scale.into().c_scalar,
                if (is_result)  1  else  0,
                self_or_result.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn eluBackwardGradInput(
        grad_input: *const Tensor, grad_output: *const Tensor, alpha: Scalar, scale: Scalar, input_scale: Scalar, is_result: bool, self_or_result: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_elu_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                alpha.into().c_scalar,
                scale.into().c_scalar,
                input_scale.into().c_scalar,
                if (is_result)  1  else  0,
                self_or_result.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn eluOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_elu_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn embedding(
        weight: *const Tensor, indices_: *const Tensor, padding_idx: i64, scale_grad_by_freq: bool, sparse: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_embedding(@ptrCast(&c_tensors), weight.c_tensor,
                indices_.c_tensor,
                padding_idx,
                if (scale_grad_by_freq)  1  else  0,
                if (sparse)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn embeddingBackward(
        gradient: *const Tensor, indices_: *const Tensor, num_weights: i64, padding_idx: i64, scale_grad_by_freq: bool, sparse: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_embedding_backward(@ptrCast(&c_tensors), gradient.c_tensor,
                indices_.c_tensor,
                num_weights,
                padding_idx,
                if (scale_grad_by_freq)  1  else  0,
                if (sparse)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn embeddingBag(
        weight: *const Tensor, indices_: *const Tensor, offsets: *const Tensor, scale_grad_by_freq: bool, mode_: i64, sparse: bool, per_sample_weights: ?*const Tensor, include_last_offset: bool
    ) [4]Tensor {
        var c_tensors = [_]C_tensor{null} ** 4;
        __c.atg_embedding_bag(@ptrCast(&c_tensors), weight.c_tensor,
                indices_.c_tensor,
                offsets.c_tensor,
                if (scale_grad_by_freq)  1  else  0,
                mode_,
                if (sparse)  1  else  0,
                if (per_sample_weights != null) per_sample_weights.?.c_tensor else null,
                if (include_last_offset)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }, Tensor { .c_tensor = c_tensors[3] }};
    }

    pub fn embeddingBagPaddingIdx(
        weight: *const Tensor, indices_: *const Tensor, offsets: *const Tensor, scale_grad_by_freq: bool, mode_: i64, sparse: bool, per_sample_weights: ?*const Tensor, include_last_offset: bool, padding_idx: ?i64
    ) [4]Tensor {
        var c_tensors = [_]C_tensor{null} ** 4;
        __c.atg_embedding_bag_padding_idx(@ptrCast(&c_tensors), weight.c_tensor,
                indices_.c_tensor,
                offsets.c_tensor,
                if (scale_grad_by_freq)  1  else  0,
                mode_,
                if (sparse)  1  else  0,
                if (per_sample_weights != null) per_sample_weights.?.c_tensor else null,
                if (include_last_offset)  1  else  0,
                padding_idx orelse 0, (padding_idx == null));
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }, Tensor { .c_tensor = c_tensors[3] }};
    }

    pub fn embeddingDenseBackward(
        grad_output: *const Tensor, indices_: *const Tensor, num_weights: i64, padding_idx: i64, scale_grad_by_freq: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_embedding_dense_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                indices_.c_tensor,
                num_weights,
                padding_idx,
                if (scale_grad_by_freq)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn embeddingDenseBackwardOut(
        out: *const Tensor, grad_output: *const Tensor, indices_: *const Tensor, num_weights: i64, padding_idx: i64, scale_grad_by_freq: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_embedding_dense_backward_out(@ptrCast(&c_tensors), out.c_tensor,
                grad_output.c_tensor,
                indices_.c_tensor,
                num_weights,
                padding_idx,
                if (scale_grad_by_freq)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn embeddingOut(
        out: *const Tensor, weight: *const Tensor, indices_: *const Tensor, padding_idx: i64, scale_grad_by_freq: bool, sparse: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_embedding_out(@ptrCast(&c_tensors), out.c_tensor,
                weight.c_tensor,
                indices_.c_tensor,
                padding_idx,
                if (scale_grad_by_freq)  1  else  0,
                if (sparse)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn embeddingRenorm(
        self: *const Tensor, indices_: *const Tensor, max_norm: f64, norm_type: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_embedding_renorm(@ptrCast(&c_tensors), self.c_tensor,
                indices_.c_tensor,
                max_norm,
                norm_type);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn embeddingRenorm_(
        self: *Tensor, indices_: *const Tensor, max_norm: f64, norm_type: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_embedding_renorm_(@ptrCast(&c_tensors), self.c_tensor,
                indices_.c_tensor,
                max_norm,
                norm_type);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn embeddingRenormOut(
        self: *const Tensor, out: *const Tensor, indices_: *const Tensor, max_norm: f64, norm_type: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_embedding_renorm_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                indices_.c_tensor,
                max_norm,
                norm_type);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn embeddingSparseBackward(
        gradient: *const Tensor, indices_: *const Tensor, num_weights: i64, padding_idx: i64, scale_grad_by_freq: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_embedding_sparse_backward(@ptrCast(&c_tensors), gradient.c_tensor,
                indices_.c_tensor,
                num_weights,
                padding_idx,
                if (scale_grad_by_freq)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn empty(
        size_: []i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_empty(@ptrCast(&c_tensors), size_.ptr, @intCast(size_.len),
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn emptyLike(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_empty_like(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn emptyLikeOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_empty_like_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn emptyOut(
        out: *const Tensor, size_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_empty_out(@ptrCast(&c_tensors), out.c_tensor,
                size_.ptr, @intCast(size_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn emptyPermuted(
        size_: []i64, physical_layout: []i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_empty_permuted(@ptrCast(&c_tensors), size_.ptr, @intCast(size_.len),
                physical_layout.ptr, @intCast(physical_layout.len),
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn emptyPermutedOut(
        out: *const Tensor, size_: []i64, physical_layout: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_empty_permuted_out(@ptrCast(&c_tensors), out.c_tensor,
                size_.ptr, @intCast(size_.len),
                physical_layout.ptr, @intCast(physical_layout.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn emptyQuantized(
        size_: []i64, qtensor: *const Tensor, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_empty_quantized(@ptrCast(&c_tensors), size_.ptr, @intCast(size_.len),
                qtensor.c_tensor,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn emptyQuantizedOut(
        out: *const Tensor, size_: []i64, qtensor: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_empty_quantized_out(@ptrCast(&c_tensors), out.c_tensor,
                size_.ptr, @intCast(size_.len),
                qtensor.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn emptyStrided(
        size_: []i64, stride_: []i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_empty_strided(@ptrCast(&c_tensors), size_.ptr, @intCast(size_.len),
                stride_.ptr, @intCast(stride_.len),
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn emptyStridedOut(
        out: *const Tensor, size_: []i64, stride_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_empty_strided_out(@ptrCast(&c_tensors), out.c_tensor,
                size_.ptr, @intCast(size_.len),
                stride_.ptr, @intCast(stride_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn eq(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_eq(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn eq_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_eq_(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn eqScalarOut(
        self: *const Tensor, out: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_eq_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn eqTensor(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_eq_tensor(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn eqTensor_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_eq_tensor_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn eqTensorOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_eq_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn equal(
        self: *const Tensor, other: *const Tensor
    ) bool {
        const return_ = __c.atg_equal(self.c_tensor,
                other.c_tensor);
       torch.readAndCleanError();
        return return_ != 0;
    }

    pub fn erf(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_erf(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn erf_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_erf_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn erfOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_erf_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn erfc(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_erfc(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn erfc_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_erfc_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn erfcOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_erfc_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn erfinv(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_erfinv(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn erfinv_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_erfinv_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn erfinvOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_erfinv_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn exp(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_exp(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn exp2(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_exp2(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn exp2_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_exp2_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn exp2Out(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_exp2_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn exp_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_exp_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn expOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_exp_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn expand(
        self: *const Tensor, size_: []i64, implicit: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_expand(@ptrCast(&c_tensors), self.c_tensor,
                size_.ptr, @intCast(size_.len),
                if (implicit)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn expandAs(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_expand_as(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn expandCopy(
        self: *const Tensor, size_: []i64, implicit: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_expand_copy(@ptrCast(&c_tensors), self.c_tensor,
                size_.ptr, @intCast(size_.len),
                if (implicit)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn expandCopyOut(
        self: *const Tensor, out: *const Tensor, size_: []i64, implicit: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_expand_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                size_.ptr, @intCast(size_.len),
                if (implicit)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn expm1(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_expm1(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn expm1_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_expm1_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn expm1Out(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_expm1_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn exponential(
        self: *const Tensor, lambd: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_exponential(@ptrCast(&c_tensors), self.c_tensor,
                lambd);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn exponential_(
        self: *Tensor, lambd: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_exponential_(@ptrCast(&c_tensors), self.c_tensor,
                lambd);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn exponentialOut(
        self: *const Tensor, out: *const Tensor, lambd: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_exponential_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                lambd);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn eye(
        n: i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_eye(@ptrCast(&c_tensors), n,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn eyeM(
        n: i64, m: i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_eye_m(@ptrCast(&c_tensors), n,
                m,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn eyeMOut(
        out: *const Tensor, n: i64, m: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_eye_m_out(@ptrCast(&c_tensors), out.c_tensor,
                n,
                m);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn eyeOut(
        out: *const Tensor, n: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_eye_out(@ptrCast(&c_tensors), out.c_tensor,
                n);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fakeQuantizePerChannelAffine(
        self: *const Tensor, scale: *const Tensor, zero_point: *const Tensor, axis: i64, quant_min: i64, quant_max: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fake_quantize_per_channel_affine(@ptrCast(&c_tensors), self.c_tensor,
                scale.c_tensor,
                zero_point.c_tensor,
                axis,
                quant_min,
                quant_max);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fakeQuantizePerChannelAffineCachemask(
        self: *const Tensor, scale: *const Tensor, zero_point: *const Tensor, axis: i64, quant_min: i64, quant_max: i64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_fake_quantize_per_channel_affine_cachemask(@ptrCast(&c_tensors), self.c_tensor,
                scale.c_tensor,
                zero_point.c_tensor,
                axis,
                quant_min,
                quant_max);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn fakeQuantizePerChannelAffineCachemaskBackward(
        gradient: *const Tensor, mask: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fake_quantize_per_channel_affine_cachemask_backward(@ptrCast(&c_tensors), gradient.c_tensor,
                mask.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fakeQuantizePerChannelAffineCachemaskOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, scale: *const Tensor, zero_point: *const Tensor, axis: i64, quant_min: i64, quant_max: i64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_fake_quantize_per_channel_affine_cachemask_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                self.c_tensor,
                scale.c_tensor,
                zero_point.c_tensor,
                axis,
                quant_min,
                quant_max);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn fakeQuantizePerTensorAffine(
        self: *const Tensor, scale: f64, zero_point: i64, quant_min: i64, quant_max: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fake_quantize_per_tensor_affine(@ptrCast(&c_tensors), self.c_tensor,
                scale,
                zero_point,
                quant_min,
                quant_max);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fakeQuantizePerTensorAffineCachemask(
        self: *const Tensor, scale: f64, zero_point: i64, quant_min: i64, quant_max: i64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_fake_quantize_per_tensor_affine_cachemask(@ptrCast(&c_tensors), self.c_tensor,
                scale,
                zero_point,
                quant_min,
                quant_max);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn fakeQuantizePerTensorAffineCachemaskBackward(
        gradient: *const Tensor, mask: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fake_quantize_per_tensor_affine_cachemask_backward(@ptrCast(&c_tensors), gradient.c_tensor,
                mask.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fakeQuantizePerTensorAffineCachemaskOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, scale: f64, zero_point: i64, quant_min: i64, quant_max: i64
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_fake_quantize_per_tensor_affine_cachemask_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                self.c_tensor,
                scale,
                zero_point,
                quant_min,
                quant_max);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn fakeQuantizePerTensorAffineTensorQparams(
        self: *const Tensor, scale: *const Tensor, zero_point: *const Tensor, quant_min: i64, quant_max: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fake_quantize_per_tensor_affine_tensor_qparams(@ptrCast(&c_tensors), self.c_tensor,
                scale.c_tensor,
                zero_point.c_tensor,
                quant_min,
                quant_max);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fbgemmLinearFp16Weight(
        self: *const Tensor, packed_weight: *const Tensor, bias: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fbgemm_linear_fp16_weight(@ptrCast(&c_tensors), self.c_tensor,
                packed_weight.c_tensor,
                bias.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fbgemmLinearFp16WeightFp32Activation(
        self: *const Tensor, packed_weight: *const Tensor, bias: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fbgemm_linear_fp16_weight_fp32_activation(@ptrCast(&c_tensors), self.c_tensor,
                packed_weight.c_tensor,
                bias.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fbgemmLinearInt8Weight(
        self: *const Tensor, weight: *const Tensor, packed_: *const Tensor, col_offsets: *const Tensor, weight_scale: Scalar, weight_zero_point: Scalar, bias: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fbgemm_linear_int8_weight(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                packed_.c_tensor,
                col_offsets.c_tensor,
                weight_scale.into().c_scalar,
                weight_zero_point.into().c_scalar,
                bias.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fbgemmLinearInt8WeightFp32Activation(
        self: *const Tensor, weight: *const Tensor, packed_: *const Tensor, col_offsets: *const Tensor, weight_scale: Scalar, weight_zero_point: Scalar, bias: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fbgemm_linear_int8_weight_fp32_activation(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                packed_.c_tensor,
                col_offsets.c_tensor,
                weight_scale.into().c_scalar,
                weight_zero_point.into().c_scalar,
                bias.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fbgemmPackGemmMatrixFp16(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fbgemm_pack_gemm_matrix_fp16(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fbgemmPackQuantizedMatrix(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fbgemm_pack_quantized_matrix(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fbgemmPackQuantizedMatrixKn(
        self: *const Tensor, k: i64, n: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fbgemm_pack_quantized_matrix_kn(@ptrCast(&c_tensors), self.c_tensor,
                k,
                n);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn featureAlphaDropout(
        self: *const Tensor, p: f64, train: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_feature_alpha_dropout(@ptrCast(&c_tensors), self.c_tensor,
                p,
                if (train)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn featureAlphaDropout_(
        self: *Tensor, p: f64, train: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_feature_alpha_dropout_(@ptrCast(&c_tensors), self.c_tensor,
                p,
                if (train)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn featureDropout(
        self: *const Tensor, p: f64, train: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_feature_dropout(@ptrCast(&c_tensors), self.c_tensor,
                p,
                if (train)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn featureDropout_(
        self: *Tensor, p: f64, train: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_feature_dropout_(@ptrCast(&c_tensors), self.c_tensor,
                p,
                if (train)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftFft(
        self: *const Tensor, n: ?i64, dim_: i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_fft(@ptrCast(&c_tensors), self.c_tensor,
                n orelse 0, (n == null),
                dim_,
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftFft2(
        self: *const Tensor, s: ?[]i64, dim_: []i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_fft2(@ptrCast(&c_tensors), self.c_tensor,
                s.ptr, @intCast(s.len),
                dim_.ptr, @intCast(dim_.len),
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftFft2Out(
        self: *const Tensor, out: *const Tensor, s: ?[]i64, dim_: []i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_fft2_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                s.ptr, @intCast(s.len),
                dim_.ptr, @intCast(dim_.len),
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftFftOut(
        self: *const Tensor, out: *const Tensor, n: ?i64, dim_: i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_fft_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                n orelse 0, (n == null),
                dim_,
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftFftfreq(
        n: i64, d: f64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_fftfreq(@ptrCast(&c_tensors), n,
                d,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftFftfreqOut(
        out: *const Tensor, n: i64, d: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_fftfreq_out(@ptrCast(&c_tensors), out.c_tensor,
                n,
                d);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftFftn(
        self: *const Tensor, s: ?[]i64, dim_: ?[]i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_fftn(@ptrCast(&c_tensors), self.c_tensor,
                s.ptr, @intCast(s.len),
                dim_.ptr, @intCast(dim_.len),
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftFftnOut(
        self: *const Tensor, out: *const Tensor, s: ?[]i64, dim_: ?[]i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_fftn_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                s.ptr, @intCast(s.len),
                dim_.ptr, @intCast(dim_.len),
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftFftshift(
        self: *const Tensor, dim_: ?[]i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_fftshift(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftHfft(
        self: *const Tensor, n: ?i64, dim_: i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_hfft(@ptrCast(&c_tensors), self.c_tensor,
                n orelse 0, (n == null),
                dim_,
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftHfft2(
        self: *const Tensor, s: ?[]i64, dim_: []i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_hfft2(@ptrCast(&c_tensors), self.c_tensor,
                s.ptr, @intCast(s.len),
                dim_.ptr, @intCast(dim_.len),
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftHfft2Out(
        self: *const Tensor, out: *const Tensor, s: ?[]i64, dim_: []i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_hfft2_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                s.ptr, @intCast(s.len),
                dim_.ptr, @intCast(dim_.len),
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftHfftOut(
        self: *const Tensor, out: *const Tensor, n: ?i64, dim_: i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_hfft_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                n orelse 0, (n == null),
                dim_,
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftHfftn(
        self: *const Tensor, s: ?[]i64, dim_: ?[]i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_hfftn(@ptrCast(&c_tensors), self.c_tensor,
                s.ptr, @intCast(s.len),
                dim_.ptr, @intCast(dim_.len),
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftHfftnOut(
        self: *const Tensor, out: *const Tensor, s: ?[]i64, dim_: ?[]i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_hfftn_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                s.ptr, @intCast(s.len),
                dim_.ptr, @intCast(dim_.len),
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftIfft(
        self: *const Tensor, n: ?i64, dim_: i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_ifft(@ptrCast(&c_tensors), self.c_tensor,
                n orelse 0, (n == null),
                dim_,
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftIfft2(
        self: *const Tensor, s: ?[]i64, dim_: []i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_ifft2(@ptrCast(&c_tensors), self.c_tensor,
                s.ptr, @intCast(s.len),
                dim_.ptr, @intCast(dim_.len),
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftIfft2Out(
        self: *const Tensor, out: *const Tensor, s: ?[]i64, dim_: []i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_ifft2_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                s.ptr, @intCast(s.len),
                dim_.ptr, @intCast(dim_.len),
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftIfftOut(
        self: *const Tensor, out: *const Tensor, n: ?i64, dim_: i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_ifft_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                n orelse 0, (n == null),
                dim_,
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftIfftn(
        self: *const Tensor, s: ?[]i64, dim_: ?[]i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_ifftn(@ptrCast(&c_tensors), self.c_tensor,
                s.ptr, @intCast(s.len),
                dim_.ptr, @intCast(dim_.len),
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftIfftnOut(
        self: *const Tensor, out: *const Tensor, s: ?[]i64, dim_: ?[]i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_ifftn_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                s.ptr, @intCast(s.len),
                dim_.ptr, @intCast(dim_.len),
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftIfftshift(
        self: *const Tensor, dim_: ?[]i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_ifftshift(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftIhfft(
        self: *const Tensor, n: ?i64, dim_: i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_ihfft(@ptrCast(&c_tensors), self.c_tensor,
                n orelse 0, (n == null),
                dim_,
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftIhfft2(
        self: *const Tensor, s: ?[]i64, dim_: []i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_ihfft2(@ptrCast(&c_tensors), self.c_tensor,
                s.ptr, @intCast(s.len),
                dim_.ptr, @intCast(dim_.len),
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftIhfft2Out(
        self: *const Tensor, out: *const Tensor, s: ?[]i64, dim_: []i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_ihfft2_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                s.ptr, @intCast(s.len),
                dim_.ptr, @intCast(dim_.len),
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftIhfftOut(
        self: *const Tensor, out: *const Tensor, n: ?i64, dim_: i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_ihfft_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                n orelse 0, (n == null),
                dim_,
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftIhfftn(
        self: *const Tensor, s: ?[]i64, dim_: ?[]i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_ihfftn(@ptrCast(&c_tensors), self.c_tensor,
                s.ptr, @intCast(s.len),
                dim_.ptr, @intCast(dim_.len),
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftIhfftnOut(
        self: *const Tensor, out: *const Tensor, s: ?[]i64, dim_: ?[]i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_ihfftn_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                s.ptr, @intCast(s.len),
                dim_.ptr, @intCast(dim_.len),
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftIrfft(
        self: *const Tensor, n: ?i64, dim_: i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_irfft(@ptrCast(&c_tensors), self.c_tensor,
                n orelse 0, (n == null),
                dim_,
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftIrfft2(
        self: *const Tensor, s: ?[]i64, dim_: []i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_irfft2(@ptrCast(&c_tensors), self.c_tensor,
                s.ptr, @intCast(s.len),
                dim_.ptr, @intCast(dim_.len),
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftIrfft2Out(
        self: *const Tensor, out: *const Tensor, s: ?[]i64, dim_: []i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_irfft2_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                s.ptr, @intCast(s.len),
                dim_.ptr, @intCast(dim_.len),
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftIrfftOut(
        self: *const Tensor, out: *const Tensor, n: ?i64, dim_: i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_irfft_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                n orelse 0, (n == null),
                dim_,
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftIrfftn(
        self: *const Tensor, s: ?[]i64, dim_: ?[]i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_irfftn(@ptrCast(&c_tensors), self.c_tensor,
                s.ptr, @intCast(s.len),
                dim_.ptr, @intCast(dim_.len),
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftIrfftnOut(
        self: *const Tensor, out: *const Tensor, s: ?[]i64, dim_: ?[]i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_irfftn_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                s.ptr, @intCast(s.len),
                dim_.ptr, @intCast(dim_.len),
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftRfft(
        self: *const Tensor, n: ?i64, dim_: i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_rfft(@ptrCast(&c_tensors), self.c_tensor,
                n orelse 0, (n == null),
                dim_,
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftRfft2(
        self: *const Tensor, s: ?[]i64, dim_: []i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_rfft2(@ptrCast(&c_tensors), self.c_tensor,
                s.ptr, @intCast(s.len),
                dim_.ptr, @intCast(dim_.len),
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftRfft2Out(
        self: *const Tensor, out: *const Tensor, s: ?[]i64, dim_: []i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_rfft2_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                s.ptr, @intCast(s.len),
                dim_.ptr, @intCast(dim_.len),
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftRfftOut(
        self: *const Tensor, out: *const Tensor, n: ?i64, dim_: i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_rfft_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                n orelse 0, (n == null),
                dim_,
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftRfftfreq(
        n: i64, d: f64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_rfftfreq(@ptrCast(&c_tensors), n,
                d,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftRfftfreqOut(
        out: *const Tensor, n: i64, d: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_rfftfreq_out(@ptrCast(&c_tensors), out.c_tensor,
                n,
                d);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftRfftn(
        self: *const Tensor, s: ?[]i64, dim_: ?[]i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_rfftn(@ptrCast(&c_tensors), self.c_tensor,
                s.ptr, @intCast(s.len),
                dim_.ptr, @intCast(dim_.len),
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fftRfftnOut(
        self: *const Tensor, out: *const Tensor, s: ?[]i64, dim_: ?[]i64, norm_: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fft_rfftn_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                s.ptr, @intCast(s.len),
                dim_.ptr, @intCast(dim_.len),
                norm_.ptr, norm_.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fill(
        self: *const Tensor, value: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fill(@ptrCast(&c_tensors), self.c_tensor,
                value.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fill_(
        self: *Tensor, value: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fill_(@ptrCast(&c_tensors), self.c_tensor,
                value.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fillDiagonal_(
        self: *Tensor, fill_value: Scalar, wrap: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fill_diagonal_(@ptrCast(&c_tensors), self.c_tensor,
                fill_value.into().c_scalar,
                if (wrap)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fillScalarOut(
        self: *const Tensor, out: *const Tensor, value: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fill_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                value.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fillTensor(
        self: *const Tensor, value: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fill_tensor(@ptrCast(&c_tensors), self.c_tensor,
                value.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fillTensor_(
        self: *Tensor, value: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fill_tensor_(@ptrCast(&c_tensors), self.c_tensor,
                value.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fillTensorOut(
        self: *const Tensor, out: *const Tensor, value: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fill_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                value.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fix(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fix(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fix_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fix_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fixOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fix_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn flatten(
        self: *const Tensor, start_dim: i64, end_dim: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_flatten(@ptrCast(&c_tensors), self.c_tensor,
                start_dim,
                end_dim);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn flattenDenseTensors(
        tensors: []*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_flatten_dense_tensors(@ptrCast(&c_tensors), ptrList(tensors).ptr, @intCast(tensors.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn flip(
        self: *const Tensor, dims: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_flip(@ptrCast(&c_tensors), self.c_tensor,
                dims.ptr, @intCast(dims.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn flipOut(
        self: *const Tensor, out: *const Tensor, dims: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_flip_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dims.ptr, @intCast(dims.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fliplr(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fliplr(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn flipud(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_flipud(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn floatPower(
        self: *const Tensor, exponent: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_float_power(@ptrCast(&c_tensors), self.c_tensor,
                exponent.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn floatPower_(
        self: *Tensor, exponent: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_float_power_(@ptrCast(&c_tensors), self.c_tensor,
                exponent.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn floatPowerScalar(
        self_scalar: Scalar, exponent: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_float_power_scalar(@ptrCast(&c_tensors), self_scalar.into().c_scalar,
                exponent.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn floatPowerScalarOut(
        out: *const Tensor, self_scalar: Scalar, exponent: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_float_power_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self_scalar.into().c_scalar,
                exponent.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn floatPowerTensor_(
        self: *Tensor, exponent: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_float_power_tensor_(@ptrCast(&c_tensors), self.c_tensor,
                exponent.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn floatPowerTensorScalar(
        self: *const Tensor, exponent: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_float_power_tensor_scalar(@ptrCast(&c_tensors), self.c_tensor,
                exponent.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn floatPowerTensorScalarOut(
        self: *const Tensor, out: *const Tensor, exponent: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_float_power_tensor_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                exponent.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn floatPowerTensorTensorOut(
        self: *const Tensor, out: *const Tensor, exponent: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_float_power_tensor_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                exponent.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn floor(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_floor(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn floor_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_floor_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn floorDivide(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_floor_divide(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn floorDivide_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_floor_divide_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn floorDivideOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_floor_divide_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn floorDivideScalar(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_floor_divide_scalar(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn floorDivideScalar_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_floor_divide_scalar_(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn floorDivideScalarOut(
        self: *const Tensor, out: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_floor_divide_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn floorOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_floor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fmax(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fmax(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fmaxOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fmax_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fmin(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fmin(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fminOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fmin_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fmod(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fmod(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fmod_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fmod_(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fmodScalarOut(
        self: *const Tensor, out: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fmod_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fmodTensor(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fmod_tensor(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fmodTensor_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fmod_tensor_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fmodTensorOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fmod_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn frac(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_frac(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn frac_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_frac_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fracOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_frac_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fractionalMaxPool2d(
        self: *const Tensor, kernel_size: []i64, output_size: []i64, random_samples: *const Tensor
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_fractional_max_pool2d(@ptrCast(&c_tensors), self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                output_size.ptr, @intCast(output_size.len),
                random_samples.c_tensor);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn fractionalMaxPool2dBackward(
        self: *const Tensor, grad_output: *const Tensor, kernel_size: []i64, output_size: []i64, indices_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fractional_max_pool2d_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                output_size.ptr, @intCast(output_size.len),
                indices_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fractionalMaxPool2dBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor, kernel_size: []i64, output_size: []i64, indices_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fractional_max_pool2d_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                output_size.ptr, @intCast(output_size.len),
                indices_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fractionalMaxPool2dOutput(
        self: *const Tensor, output: *const Tensor, indices_: *const Tensor, kernel_size: []i64, output_size: []i64, random_samples: *const Tensor
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_fractional_max_pool2d_output(@ptrCast(&c_tensors), output.c_tensor,
                indices_.c_tensor,
                self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                output_size.ptr, @intCast(output_size.len),
                random_samples.c_tensor);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn fractionalMaxPool3d(
        self: *const Tensor, kernel_size: []i64, output_size: []i64, random_samples: *const Tensor
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_fractional_max_pool3d(@ptrCast(&c_tensors), self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                output_size.ptr, @intCast(output_size.len),
                random_samples.c_tensor);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn fractionalMaxPool3dBackward(
        self: *const Tensor, grad_output: *const Tensor, kernel_size: []i64, output_size: []i64, indices_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fractional_max_pool3d_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                output_size.ptr, @intCast(output_size.len),
                indices_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fractionalMaxPool3dBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor, kernel_size: []i64, output_size: []i64, indices_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fractional_max_pool3d_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                output_size.ptr, @intCast(output_size.len),
                indices_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fractionalMaxPool3dOutput(
        self: *const Tensor, output: *const Tensor, indices_: *const Tensor, kernel_size: []i64, output_size: []i64, random_samples: *const Tensor
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_fractional_max_pool3d_output(@ptrCast(&c_tensors), output.c_tensor,
                indices_.c_tensor,
                self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                output_size.ptr, @intCast(output_size.len),
                random_samples.c_tensor);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn frexp(
        self: *const Tensor, 
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_frexp(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn frexpTensorOut(
        self: *const Tensor, mantissa: *const Tensor, exponent: *const Tensor
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_frexp_tensor_out(@ptrCast(&c_tensors), mantissa.c_tensor,
                exponent.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn frobeniusNorm(
        self: *const Tensor, dim_: []i64, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_frobenius_norm(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn frobeniusNormOut(
        self: *const Tensor, out: *const Tensor, dim_: []i64, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_frobenius_norm_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fromFile(
        filename: []const u8, shared: bool, size_: ?i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_from_file(@ptrCast(&c_tensors), filename.ptr, filename.len,
                if (shared)  1  else  0,
                size_ orelse 0, (size_ == null),
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fromFileOut(
        out: *const Tensor, filename: []const u8, shared: bool, size_: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_from_file_out(@ptrCast(&c_tensors), out.c_tensor,
                filename.ptr, filename.len,
                if (shared)  1  else  0,
                size_ orelse 0, (size_ == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn full(
        size_: []i64, fill_value: Scalar, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_full(@ptrCast(&c_tensors), size_.ptr, @intCast(size_.len),
                fill_value.into().c_scalar,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fullLike(
        self: *const Tensor, fill_value: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_full_like(@ptrCast(&c_tensors), self.c_tensor,
                fill_value.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fullLikeOut(
        self: *const Tensor, out: *const Tensor, fill_value: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_full_like_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                fill_value.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fullOut(
        out: *const Tensor, size_: []i64, fill_value: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_full_out(@ptrCast(&c_tensors), out.c_tensor,
                size_.ptr, @intCast(size_.len),
                fill_value.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn fusedMovingAvgObsFakeQuant(
        self: *const Tensor, observer_on: *const Tensor, fake_quant_on: *const Tensor, running_min: *const Tensor, running_max: *const Tensor, scale: *const Tensor, zero_point: *const Tensor, averaging_const: f64, quant_min: i64, quant_max: i64, ch_axis: i64, per_row_fake_quant: bool, symmetric_quant: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_fused_moving_avg_obs_fake_quant(@ptrCast(&c_tensors), self.c_tensor,
                observer_on.c_tensor,
                fake_quant_on.c_tensor,
                running_min.c_tensor,
                running_max.c_tensor,
                scale.c_tensor,
                zero_point.c_tensor,
                averaging_const,
                quant_min,
                quant_max,
                ch_axis,
                if (per_row_fake_quant)  1  else  0,
                if (symmetric_quant)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn gather(
        self: *const Tensor, dim_: i64, index_: *const Tensor, sparse_grad: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_gather(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                index_.c_tensor,
                if (sparse_grad)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn gatherBackward(
        self: *const Tensor, gradient: *const Tensor, dim_: i64, index_: *const Tensor, sparse_grad: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_gather_backward(@ptrCast(&c_tensors), gradient.c_tensor,
                self.c_tensor,
                dim_,
                index_.c_tensor,
                if (sparse_grad)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn gatherOut(
        self: *const Tensor, out: *const Tensor, dim_: i64, index_: *const Tensor, sparse_grad: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_gather_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_,
                index_.c_tensor,
                if (sparse_grad)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn gcd(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_gcd(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn gcd_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_gcd_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn gcdOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_gcd_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn ge(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_ge(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn ge_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_ge_(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn geScalarOut(
        self: *const Tensor, out: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_ge_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn geTensor(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_ge_tensor(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn geTensor_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_ge_tensor_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn geTensorOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_ge_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn gelu(
        self: *const Tensor, approximate: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_gelu(@ptrCast(&c_tensors), self.c_tensor,
                approximate.ptr, approximate.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn gelu_(
        self: *Tensor, approximate: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_gelu_(@ptrCast(&c_tensors), self.c_tensor,
                approximate.ptr, approximate.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn geluBackward(
        self: *const Tensor, grad_output: *const Tensor, approximate: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_gelu_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                approximate.ptr, approximate.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn geluBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor, approximate: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_gelu_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                approximate.ptr, approximate.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn geluOut(
        self: *const Tensor, out: *const Tensor, approximate: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_gelu_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                approximate.ptr, approximate.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn geometric(
        self: *const Tensor, p: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_geometric(@ptrCast(&c_tensors), self.c_tensor,
                p);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn geometric_(
        self: *Tensor, p: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_geometric_(@ptrCast(&c_tensors), self.c_tensor,
                p);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn geometricOut(
        self: *const Tensor, out: *const Tensor, p: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_geometric_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                p);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn geqrf(
        self: *const Tensor, 
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_geqrf(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn geqrfA(
        self: *const Tensor, a: *const Tensor, tau: *const Tensor
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_geqrf_a(@ptrCast(&c_tensors), a.c_tensor,
                tau.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn ger(
        self: *const Tensor, vec2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_ger(@ptrCast(&c_tensors), self.c_tensor,
                vec2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn gerOut(
        self: *const Tensor, out: *const Tensor, vec2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_ger_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                vec2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn glu(
        self: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_glu(@ptrCast(&c_tensors), self.c_tensor,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn gluBackward(
        self: *const Tensor, grad_output: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_glu_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn gluBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_glu_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn gluBackwardJvp(
        grad_x: *const Tensor, grad_glu: *const Tensor, x: *const Tensor, dgrad_glu: *const Tensor, dx: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_glu_backward_jvp(@ptrCast(&c_tensors), grad_x.c_tensor,
                grad_glu.c_tensor,
                x.c_tensor,
                dgrad_glu.c_tensor,
                dx.c_tensor,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn gluBackwardJvpOut(
        out: *const Tensor, grad_x: *const Tensor, grad_glu: *const Tensor, x: *const Tensor, dgrad_glu: *const Tensor, dx: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_glu_backward_jvp_out(@ptrCast(&c_tensors), out.c_tensor,
                grad_x.c_tensor,
                grad_glu.c_tensor,
                x.c_tensor,
                dgrad_glu.c_tensor,
                dx.c_tensor,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn gluJvp(
        glu_: *const Tensor, x: *const Tensor, dx: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_glu_jvp(@ptrCast(&c_tensors), glu_.c_tensor,
                x.c_tensor,
                dx.c_tensor,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn gluJvpOut(
        out: *const Tensor, glu_: *const Tensor, x: *const Tensor, dx: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_glu_jvp_out(@ptrCast(&c_tensors), out.c_tensor,
                glu_.c_tensor,
                x.c_tensor,
                dx.c_tensor,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn gluOut(
        self: *const Tensor, out: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_glu_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn grad(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_grad(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn greater(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_greater(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn greater_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_greater_(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn greaterEqual(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_greater_equal(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn greaterEqual_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_greater_equal_(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn greaterEqualScalarOut(
        self: *const Tensor, out: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_greater_equal_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn greaterEqualTensor(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_greater_equal_tensor(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn greaterEqualTensor_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_greater_equal_tensor_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn greaterEqualTensorOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_greater_equal_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn greaterScalarOut(
        self: *const Tensor, out: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_greater_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn greaterTensor(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_greater_tensor(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn greaterTensor_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_greater_tensor_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn greaterTensorOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_greater_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn gridSampler(
        self: *const Tensor, grid: *const Tensor, interpolation_mode: i64, padding_mode: i64, align_corners: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_grid_sampler(@ptrCast(&c_tensors), self.c_tensor,
                grid.c_tensor,
                interpolation_mode,
                padding_mode,
                if (align_corners)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn gridSampler2d(
        self: *const Tensor, grid: *const Tensor, interpolation_mode: i64, padding_mode: i64, align_corners: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_grid_sampler_2d(@ptrCast(&c_tensors), self.c_tensor,
                grid.c_tensor,
                interpolation_mode,
                padding_mode,
                if (align_corners)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn gridSampler2dOut(
        self: *const Tensor, out: *const Tensor, grid: *const Tensor, interpolation_mode: i64, padding_mode: i64, align_corners: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_grid_sampler_2d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                grid.c_tensor,
                interpolation_mode,
                padding_mode,
                if (align_corners)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn gridSampler3d(
        self: *const Tensor, grid: *const Tensor, interpolation_mode: i64, padding_mode: i64, align_corners: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_grid_sampler_3d(@ptrCast(&c_tensors), self.c_tensor,
                grid.c_tensor,
                interpolation_mode,
                padding_mode,
                if (align_corners)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn gridSampler3dOut(
        self: *const Tensor, out: *const Tensor, grid: *const Tensor, interpolation_mode: i64, padding_mode: i64, align_corners: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_grid_sampler_3d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                grid.c_tensor,
                interpolation_mode,
                padding_mode,
                if (align_corners)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn groupNorm(
        self: *const Tensor, num_groups: i64, weight: ?*const Tensor, bias: ?*const Tensor, eps: f64, cudnn_enabled: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_group_norm(@ptrCast(&c_tensors), self.c_tensor,
                num_groups,
                if (weight != null) weight.?.c_tensor else null,
                if (bias != null) bias.?.c_tensor else null,
                eps,
                if (cudnn_enabled)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn gru(
        self: *const Tensor, hx: *const Tensor, params: []*const Tensor, has_biases: bool, num_layers: i64, dropout_: f64, train: bool, bidirectional: bool, batch_first: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_gru(@ptrCast(&c_tensors), self.c_tensor,
                hx.c_tensor,
                ptrList(params).ptr, @intCast(params.len),
                if (has_biases)  1  else  0,
                num_layers,
                dropout_,
                if (train)  1  else  0,
                if (bidirectional)  1  else  0,
                if (batch_first)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn gruCell(
        self: *const Tensor, hx: *const Tensor, w_ih: *const Tensor, w_hh: *const Tensor, b_ih: ?*const Tensor, b_hh: ?*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_gru_cell(@ptrCast(&c_tensors), self.c_tensor,
                hx.c_tensor,
                w_ih.c_tensor,
                w_hh.c_tensor,
                if (b_ih != null) b_ih.?.c_tensor else null,
                if (b_hh != null) b_hh.?.c_tensor else null);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn gruData(
        data_: *const Tensor, batch_sizes: *const Tensor, hx: *const Tensor, params: []*const Tensor, has_biases: bool, num_layers: i64, dropout_: f64, train: bool, bidirectional: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_gru_data(@ptrCast(&c_tensors), data_.c_tensor,
                batch_sizes.c_tensor,
                hx.c_tensor,
                ptrList(params).ptr, @intCast(params.len),
                if (has_biases)  1  else  0,
                num_layers,
                dropout_,
                if (train)  1  else  0,
                if (bidirectional)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn gt(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_gt(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn gt_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_gt_(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn gtScalarOut(
        self: *const Tensor, out: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_gt_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn gtTensor(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_gt_tensor(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn gtTensor_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_gt_tensor_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn gtTensorOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_gt_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hammingWindow(
        window_length: i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hamming_window(@ptrCast(&c_tensors), window_length,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hammingWindowOut(
        out: *const Tensor, window_length: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hamming_window_out(@ptrCast(&c_tensors), out.c_tensor,
                window_length);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hammingWindowPeriodic(
        window_length: i64, periodic: bool, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hamming_window_periodic(@ptrCast(&c_tensors), window_length,
                if (periodic)  1  else  0,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hammingWindowPeriodicAlpha(
        window_length: i64, periodic: bool, alpha: f64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hamming_window_periodic_alpha(@ptrCast(&c_tensors), window_length,
                if (periodic)  1  else  0,
                alpha,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hammingWindowPeriodicAlphaBeta(
        window_length: i64, periodic: bool, alpha: f64, beta: f64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hamming_window_periodic_alpha_beta(@ptrCast(&c_tensors), window_length,
                if (periodic)  1  else  0,
                alpha,
                beta,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hammingWindowPeriodicAlphaBetaOut(
        out: *const Tensor, window_length: i64, periodic: bool, alpha: f64, beta: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hamming_window_periodic_alpha_beta_out(@ptrCast(&c_tensors), out.c_tensor,
                window_length,
                if (periodic)  1  else  0,
                alpha,
                beta);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hammingWindowPeriodicAlphaOut(
        out: *const Tensor, window_length: i64, periodic: bool, alpha: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hamming_window_periodic_alpha_out(@ptrCast(&c_tensors), out.c_tensor,
                window_length,
                if (periodic)  1  else  0,
                alpha);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hammingWindowPeriodicOut(
        out: *const Tensor, window_length: i64, periodic: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hamming_window_periodic_out(@ptrCast(&c_tensors), out.c_tensor,
                window_length,
                if (periodic)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hannWindow(
        window_length: i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hann_window(@ptrCast(&c_tensors), window_length,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hannWindowOut(
        out: *const Tensor, window_length: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hann_window_out(@ptrCast(&c_tensors), out.c_tensor,
                window_length);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hannWindowPeriodic(
        window_length: i64, periodic: bool, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hann_window_periodic(@ptrCast(&c_tensors), window_length,
                if (periodic)  1  else  0,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hannWindowPeriodicOut(
        out: *const Tensor, window_length: i64, periodic: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hann_window_periodic_out(@ptrCast(&c_tensors), out.c_tensor,
                window_length,
                if (periodic)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hardshrink(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hardshrink(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hardshrinkBackward(
        self: *const Tensor, grad_out: *const Tensor, lambd: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hardshrink_backward(@ptrCast(&c_tensors), grad_out.c_tensor,
                self.c_tensor,
                lambd.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hardshrinkBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_out: *const Tensor, lambd: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hardshrink_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_out.c_tensor,
                self.c_tensor,
                lambd.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hardshrinkOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hardshrink_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hardsigmoid(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hardsigmoid(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hardsigmoid_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hardsigmoid_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hardsigmoidBackward(
        self: *const Tensor, grad_output: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hardsigmoid_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hardsigmoidBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hardsigmoid_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hardsigmoidOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hardsigmoid_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hardswish(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hardswish(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hardswish_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hardswish_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hardswishBackward(
        self: *const Tensor, grad_output: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hardswish_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hardswishBackwardOut(
        self: *const Tensor, out: *const Tensor, grad_output: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hardswish_backward_out(@ptrCast(&c_tensors), out.c_tensor,
                grad_output.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hardswishOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hardswish_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hardtanh(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hardtanh(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hardtanh_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hardtanh_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hardtanhBackward(
        self: *const Tensor, grad_output: *const Tensor, min_val: Scalar, max_val: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hardtanh_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                min_val.into().c_scalar,
                max_val.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hardtanhBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor, min_val: Scalar, max_val: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hardtanh_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                min_val.into().c_scalar,
                max_val.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hardtanhOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hardtanh_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn heaviside(
        self: *const Tensor, values_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_heaviside(@ptrCast(&c_tensors), self.c_tensor,
                values_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn heaviside_(
        self: *Tensor, values_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_heaviside_(@ptrCast(&c_tensors), self.c_tensor,
                values_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn heavisideOut(
        self: *const Tensor, out: *const Tensor, values_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_heaviside_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                values_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hingeEmbeddingLoss(
        self: *const Tensor, target: *const Tensor, margin: f64, reduction: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hinge_embedding_loss(@ptrCast(&c_tensors), self.c_tensor,
                target.c_tensor,
                margin,
                reduction.to_int());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn histc(
        self: *const Tensor, bins: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_histc(@ptrCast(&c_tensors), self.c_tensor,
                bins);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn histcOut(
        self: *const Tensor, out: *const Tensor, bins: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_histc_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                bins);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn histogram(
        self: *const Tensor, bins: *const Tensor, weight: ?*const Tensor, density: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_histogram(@ptrCast(&c_tensors), self.c_tensor,
                bins.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                if (density)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn histogramBinCt(
        self: *const Tensor, bins: i64, range_: []f64, weight: ?*const Tensor, density: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_histogram_bin_ct(@ptrCast(&c_tensors), self.c_tensor,
                bins,
                range_.ptr, @intCast(range_.len),
                if (weight != null) weight.?.c_tensor else null,
                if (density)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn histogramBinCtOut(
        self: *const Tensor, hist: *const Tensor, bin_edges: *const Tensor, bins: i64, range_: []f64, weight: ?*const Tensor, density: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_histogram_bin_ct_out(@ptrCast(&c_tensors), hist.c_tensor,
                bin_edges.c_tensor,
                self.c_tensor,
                bins,
                range_.ptr, @intCast(range_.len),
                if (weight != null) weight.?.c_tensor else null,
                if (density)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn histogramBinsTensorOut(
        self: *const Tensor, hist: *const Tensor, bin_edges: *const Tensor, bins: *const Tensor, weight: ?*const Tensor, density: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_histogram_bins_tensor_out(@ptrCast(&c_tensors), hist.c_tensor,
                bin_edges.c_tensor,
                self.c_tensor,
                bins.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                if (density)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn hsplit(
        self: *const Tensor, sections: i64
    ) []Tensor {
        const c_tensors = __c.atg_hsplit(self.c_tensor,
                sections);
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn hsplitArray(
        self: *const Tensor, indices_: []i64
    ) []Tensor {
        const c_tensors = __c.atg_hsplit_array(self.c_tensor,
                indices_.ptr, @intCast(indices_.len));
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn hspmm(
        mat1: *const Tensor, mat2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hspmm(@ptrCast(&c_tensors), mat1.c_tensor,
                mat2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hspmmOut(
        out: *const Tensor, mat1: *const Tensor, mat2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hspmm_out(@ptrCast(&c_tensors), out.c_tensor,
                mat1.c_tensor,
                mat2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hstack(
        tensors: []*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hstack(@ptrCast(&c_tensors), ptrList(tensors).ptr, @intCast(tensors.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hstackOut(
        out: *const Tensor, tensors: []*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hstack_out(@ptrCast(&c_tensors), out.c_tensor,
                ptrList(tensors).ptr, @intCast(tensors.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn huberLoss(
        self: *const Tensor, target: *const Tensor, reduction: i64, delta: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_huber_loss(@ptrCast(&c_tensors), self.c_tensor,
                target.c_tensor,
                reduction.to_int(),
                delta);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn huberLossBackward(
        self: *const Tensor, grad_output: *const Tensor, target: *const Tensor, reduction: i64, delta: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_huber_loss_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                target.c_tensor,
                reduction.to_int(),
                delta);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn huberLossBackwardOut(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor, target: *const Tensor, reduction: i64, delta: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_huber_loss_backward_out(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                target.c_tensor,
                reduction.to_int(),
                delta);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn huberLossOut(
        self: *const Tensor, out: *const Tensor, target: *const Tensor, reduction: i64, delta: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_huber_loss_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                target.c_tensor,
                reduction.to_int(),
                delta);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hypot(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hypot(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hypot_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hypot_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn hypotOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_hypot_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn i0Bessel(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_i0(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn i0Bessel_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_i0_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn i0Out(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_i0_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn igamma(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_igamma(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn igamma_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_igamma_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn igammaOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_igamma_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn igammac(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_igammac(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn igammac_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_igammac_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn igammacOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_igammac_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn im2col(
        self: *const Tensor, kernel_size: []i64, dilation: []i64, padding: []i64, stride_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_im2col(@ptrCast(&c_tensors), self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                dilation.ptr, @intCast(dilation.len),
                padding.ptr, @intCast(padding.len),
                stride_.ptr, @intCast(stride_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn im2colOut(
        self: *const Tensor, out: *const Tensor, kernel_size: []i64, dilation: []i64, padding: []i64, stride_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_im2col_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                dilation.ptr, @intCast(dilation.len),
                padding.ptr, @intCast(padding.len),
                stride_.ptr, @intCast(stride_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn imag(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_imag(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn index(
        self: *const Tensor, indices_: []?*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_index(@ptrCast(&c_tensors), self.c_tensor,
                ptrListOpt(indices_).ptr, @intCast(indices_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn indexAdd(
        self: *const Tensor, dim_: i64, index_: *const Tensor, source: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_index_add(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                index_.c_tensor,
                source.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn indexAdd_(
        self: *Tensor, dim_: i64, index_: *const Tensor, source: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_index_add_(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                index_.c_tensor,
                source.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn indexAddOut(
        self: *const Tensor, out: *const Tensor, dim_: i64, index_: *const Tensor, source: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_index_add_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_,
                index_.c_tensor,
                source.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn indexCopy(
        self: *const Tensor, dim_: i64, index_: *const Tensor, source: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_index_copy(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                index_.c_tensor,
                source.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn indexCopy_(
        self: *Tensor, dim_: i64, index_: *const Tensor, source: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_index_copy_(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                index_.c_tensor,
                source.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn indexCopyOut(
        self: *const Tensor, out: *const Tensor, dim_: i64, index_: *const Tensor, source: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_index_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_,
                index_.c_tensor,
                source.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn indexFill(
        self: *const Tensor, dim_: i64, index_: *const Tensor, value: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_index_fill(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                index_.c_tensor,
                value.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn indexFill_(
        self: *Tensor, dim_: i64, index_: *const Tensor, value: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_index_fill_(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                index_.c_tensor,
                value.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn indexFillIntScalarOut(
        self: *const Tensor, out: *const Tensor, dim_: i64, index_: *const Tensor, value: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_index_fill_int_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_,
                index_.c_tensor,
                value.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn indexFillIntTensor(
        self: *const Tensor, dim_: i64, index_: *const Tensor, value: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_index_fill_int_tensor(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                index_.c_tensor,
                value.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn indexFillIntTensor_(
        self: *Tensor, dim_: i64, index_: *const Tensor, value: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_index_fill_int_tensor_(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                index_.c_tensor,
                value.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn indexFillIntTensorOut(
        self: *const Tensor, out: *const Tensor, dim_: i64, index_: *const Tensor, value: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_index_fill_int_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_,
                index_.c_tensor,
                value.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn indexPut(
        self: *const Tensor, indices_: []?*const Tensor, values_: *const Tensor, accumulate: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_index_put(@ptrCast(&c_tensors), self.c_tensor,
                ptrListOpt(indices_).ptr, @intCast(indices_.len),
                values_.c_tensor,
                if (accumulate)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn indexPut_(
        self: *Tensor, indices_: []?*const Tensor, values_: *const Tensor, accumulate: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_index_put_(@ptrCast(&c_tensors), self.c_tensor,
                ptrListOpt(indices_).ptr, @intCast(indices_.len),
                values_.c_tensor,
                if (accumulate)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn indexPutOut(
        self: *const Tensor, out: *const Tensor, indices_: []?*const Tensor, values_: *const Tensor, accumulate: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_index_put_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                ptrListOpt(indices_).ptr, @intCast(indices_.len),
                values_.c_tensor,
                if (accumulate)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn indexReduce(
        self: *const Tensor, dim_: i64, index_: *const Tensor, source: *const Tensor, reduce: []const u8, include_self: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_index_reduce(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                index_.c_tensor,
                source.c_tensor,
                reduce.ptr, reduce.len,
                if (include_self)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn indexReduce_(
        self: *Tensor, dim_: i64, index_: *const Tensor, source: *const Tensor, reduce: []const u8, include_self: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_index_reduce_(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                index_.c_tensor,
                source.c_tensor,
                reduce.ptr, reduce.len,
                if (include_self)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn indexReduceOut(
        self: *const Tensor, out: *const Tensor, dim_: i64, index_: *const Tensor, source: *const Tensor, reduce: []const u8, include_self: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_index_reduce_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_,
                index_.c_tensor,
                source.c_tensor,
                reduce.ptr, reduce.len,
                if (include_self)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn indexSelect(
        self: *const Tensor, dim_: i64, index_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_index_select(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                index_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn indexSelectBackward(
        gradient: *const Tensor, self_sizes: []i64, dim_: i64, index_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_index_select_backward(@ptrCast(&c_tensors), gradient.c_tensor,
                self_sizes.ptr, @intCast(self_sizes.len),
                dim_,
                index_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn indexSelectOut(
        self: *const Tensor, out: *const Tensor, dim_: i64, index_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_index_select_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_,
                index_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn indexTensorOut(
        self: *const Tensor, out: *const Tensor, indices_: []?*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_index_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                ptrListOpt(indices_).ptr, @intCast(indices_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn indices(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_indices(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn indicesCopy(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_indices_copy(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn indicesCopyOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_indices_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn infinitelyDifferentiableGeluBackward(
        self: *const Tensor, gradient: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_infinitely_differentiable_gelu_backward(@ptrCast(&c_tensors), gradient.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn inner(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_inner(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn innerOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_inner_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn instanceNorm(
        self: *const Tensor, weight: ?*const Tensor, bias: ?*const Tensor, running_mean: ?*const Tensor, running_var: ?*const Tensor, use_input_stats: bool, momentum: f64, eps: f64, cudnn_enabled: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_instance_norm(@ptrCast(&c_tensors), self.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                if (bias != null) bias.?.c_tensor else null,
                if (running_mean != null) running_mean.?.c_tensor else null,
                if (running_var != null) running_var.?.c_tensor else null,
                if (use_input_stats)  1  else  0,
                momentum,
                eps,
                if (cudnn_enabled)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn intRepr(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_int_repr(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn intReprOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_int_repr_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn inverse(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_inverse(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn inverseOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_inverse_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn isCoalesced(
        self: *const Tensor, 
    ) bool {
        const return_ = __c.atg_is_coalesced(self.c_tensor);
       torch.readAndCleanError();
        return return_ != 0;
    }

    pub fn isComplex(
        self: *const Tensor, 
    ) bool {
        const return_ = __c.atg_is_complex(self.c_tensor);
       torch.readAndCleanError();
        return return_ != 0;
    }

    pub fn isConj(
        self: *const Tensor, 
    ) bool {
        const return_ = __c.atg_is_conj(self.c_tensor);
       torch.readAndCleanError();
        return return_ != 0;
    }

    pub fn isDistributed(
        self: *const Tensor, 
    ) bool {
        const return_ = __c.atg_is_distributed(self.c_tensor);
       torch.readAndCleanError();
        return return_ != 0;
    }

    pub fn isFloatingPoint(
        self: *const Tensor, 
    ) bool {
        const return_ = __c.atg_is_floating_point(self.c_tensor);
       torch.readAndCleanError();
        return return_ != 0;
    }

    pub fn isInference(
        self: *const Tensor, 
    ) bool {
        const return_ = __c.atg_is_inference(self.c_tensor);
       torch.readAndCleanError();
        return return_ != 0;
    }

    pub fn isLeaf(
        self: *const Tensor, 
    ) bool {
        const return_ = __c.atg_is_leaf(self.c_tensor);
       torch.readAndCleanError();
        return return_ != 0;
    }

    pub fn isNeg(
        self: *const Tensor, 
    ) bool {
        const return_ = __c.atg_is_neg(self.c_tensor);
       torch.readAndCleanError();
        return return_ != 0;
    }

    pub fn isNonzero(
        self: *const Tensor, 
    ) bool {
        const return_ = __c.atg_is_nonzero(self.c_tensor);
       torch.readAndCleanError();
        return return_ != 0;
    }

    pub fn isPinned(
        self: *const Tensor, device_: Device
    ) bool {
        const return_ = __c.atg_is_pinned(self.c_tensor,
                device_.cInt());
       torch.readAndCleanError();
        return return_ != 0;
    }

    pub fn isSameSize(
        self: *const Tensor, other: *const Tensor
    ) bool {
        const return_ = __c.atg_is_same_size(self.c_tensor,
                other.c_tensor);
       torch.readAndCleanError();
        return return_ != 0;
    }

    pub fn isSetTo(
        self: *const Tensor, tensor: *const Tensor
    ) bool {
        const return_ = __c.atg_is_set_to(self.c_tensor,
                tensor.c_tensor);
       torch.readAndCleanError();
        return return_ != 0;
    }

    pub fn isSigned(
        self: *const Tensor, 
    ) bool {
        const return_ = __c.atg_is_signed(self.c_tensor);
       torch.readAndCleanError();
        return return_ != 0;
    }

    pub fn isVulkanAvailable(
        
    ) bool {
        const return_ = __c.atg_is_vulkan_available();
       torch.readAndCleanError();
        return return_ != 0;
    }

    pub fn isclose(
        self: *const Tensor, other: *const Tensor, rtol: f64, atol: f64, equal_nan: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_isclose(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor,
                rtol,
                atol,
                if (equal_nan)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn isfinite(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_isfinite(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn isin(
        elements: *const Tensor, test_elements: *const Tensor, assume_unique: bool, invert: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_isin(@ptrCast(&c_tensors), elements.c_tensor,
                test_elements.c_tensor,
                if (assume_unique)  1  else  0,
                if (invert)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn isinScalarTensor(
        element: Scalar, test_elements: *const Tensor, assume_unique: bool, invert: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_isin_scalar_tensor(@ptrCast(&c_tensors), element.into().c_scalar,
                test_elements.c_tensor,
                if (assume_unique)  1  else  0,
                if (invert)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn isinScalarTensorOut(
        out: *const Tensor, element: Scalar, test_elements: *const Tensor, assume_unique: bool, invert: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_isin_scalar_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                element.into().c_scalar,
                test_elements.c_tensor,
                if (assume_unique)  1  else  0,
                if (invert)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn isinTensorScalar(
        elements: *const Tensor, test_element: Scalar, assume_unique: bool, invert: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_isin_tensor_scalar(@ptrCast(&c_tensors), elements.c_tensor,
                test_element.into().c_scalar,
                if (assume_unique)  1  else  0,
                if (invert)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn isinTensorScalarOut(
        out: *const Tensor, elements: *const Tensor, test_element: Scalar, assume_unique: bool, invert: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_isin_tensor_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                elements.c_tensor,
                test_element.into().c_scalar,
                if (assume_unique)  1  else  0,
                if (invert)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn isinTensorTensorOut(
        out: *const Tensor, elements: *const Tensor, test_elements: *const Tensor, assume_unique: bool, invert: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_isin_tensor_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                elements.c_tensor,
                test_elements.c_tensor,
                if (assume_unique)  1  else  0,
                if (invert)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn isinf(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_isinf(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn isinfOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_isinf_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn isnan(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_isnan(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn isnanOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_isnan_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn isneginf(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_isneginf(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn isneginfOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_isneginf_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn isposinf(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_isposinf(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn isposinfOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_isposinf_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn isreal(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_isreal(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn istft(
        self: *const Tensor, n_fft: i64, hop_length: ?i64, win_length: ?i64, window: ?*const Tensor, center: bool, normalized: bool, onesided: bool, length: ?i64, return_complex: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_istft(@ptrCast(&c_tensors), self.c_tensor,
                n_fft,
                hop_length orelse 0, (hop_length == null),
                win_length orelse 0, (win_length == null),
                if (window != null) window.?.c_tensor else null,
                if (center)  1  else  0,
                if (normalized)  1  else  0,
                if (onesided)  1  else  0,
                length orelse 0, (length == null),
                if (return_complex)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn kaiserWindow(
        window_length: i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_kaiser_window(@ptrCast(&c_tensors), window_length,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn kaiserWindowBeta(
        window_length: i64, periodic: bool, beta: f64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_kaiser_window_beta(@ptrCast(&c_tensors), window_length,
                if (periodic)  1  else  0,
                beta,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn kaiserWindowBetaOut(
        out: *const Tensor, window_length: i64, periodic: bool, beta: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_kaiser_window_beta_out(@ptrCast(&c_tensors), out.c_tensor,
                window_length,
                if (periodic)  1  else  0,
                beta);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn kaiserWindowOut(
        out: *const Tensor, window_length: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_kaiser_window_out(@ptrCast(&c_tensors), out.c_tensor,
                window_length);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn kaiserWindowPeriodic(
        window_length: i64, periodic: bool, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_kaiser_window_periodic(@ptrCast(&c_tensors), window_length,
                if (periodic)  1  else  0,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn kaiserWindowPeriodicOut(
        out: *const Tensor, window_length: i64, periodic: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_kaiser_window_periodic_out(@ptrCast(&c_tensors), out.c_tensor,
                window_length,
                if (periodic)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn klDiv(
        self: *const Tensor, target: *const Tensor, reduction: i64, log_target: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_kl_div(@ptrCast(&c_tensors), self.c_tensor,
                target.c_tensor,
                reduction.to_int(),
                if (log_target)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn kron(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_kron(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn kronOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_kron_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn kthvalue(
        self: *const Tensor, k: i64, dim_: i64, keepdim: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_kthvalue(@ptrCast(&c_tensors), self.c_tensor,
                k,
                dim_,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn kthvalueValues(
        self: *const Tensor, values_: *const Tensor, indices_: *const Tensor, k: i64, dim_: i64, keepdim: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_kthvalue_values(@ptrCast(&c_tensors), values_.c_tensor,
                indices_.c_tensor,
                self.c_tensor,
                k,
                dim_,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn l1Loss(
        self: *const Tensor, target: *const Tensor, reduction: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_l1_loss(@ptrCast(&c_tensors), self.c_tensor,
                target.c_tensor,
                reduction.to_int());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn layerNorm(
        self: *const Tensor, normalized_shape: []i64, weight: ?*const Tensor, bias: ?*const Tensor, eps: f64, cudnn_enable: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_layer_norm(@ptrCast(&c_tensors), self.c_tensor,
                normalized_shape.ptr, @intCast(normalized_shape.len),
                if (weight != null) weight.?.c_tensor else null,
                if (bias != null) bias.?.c_tensor else null,
                eps,
                if (cudnn_enable)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn lcm(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_lcm(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn lcm_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_lcm_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn lcmOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_lcm_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn ldexp(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_ldexp(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn ldexp_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_ldexp_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn ldexpOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_ldexp_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn le(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_le(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn le_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_le_(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn leScalarOut(
        self: *const Tensor, out: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_le_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn leTensor(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_le_tensor(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn leTensor_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_le_tensor_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn leTensorOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_le_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn leakyRelu(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_leaky_relu(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn leakyRelu_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_leaky_relu_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn leakyReluBackward(
        self: *const Tensor, grad_output: *const Tensor, negative_slope: Scalar, self_is_result: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_leaky_relu_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                negative_slope.into().c_scalar,
                if (self_is_result)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn leakyReluBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor, negative_slope: Scalar, self_is_result: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_leaky_relu_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                negative_slope.into().c_scalar,
                if (self_is_result)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn leakyReluOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_leaky_relu_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn lerp(
        self: *const Tensor, end: *const Tensor, weight: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_lerp(@ptrCast(&c_tensors), self.c_tensor,
                end.c_tensor,
                weight.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn lerp_(
        self: *Tensor, end: *const Tensor, weight: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_lerp_(@ptrCast(&c_tensors), self.c_tensor,
                end.c_tensor,
                weight.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn lerpScalarOut(
        self: *const Tensor, out: *const Tensor, end: *const Tensor, weight: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_lerp_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                end.c_tensor,
                weight.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn lerpTensor(
        self: *const Tensor, end: *const Tensor, weight: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_lerp_tensor(@ptrCast(&c_tensors), self.c_tensor,
                end.c_tensor,
                weight.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn lerpTensor_(
        self: *Tensor, end: *const Tensor, weight: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_lerp_tensor_(@ptrCast(&c_tensors), self.c_tensor,
                end.c_tensor,
                weight.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn lerpTensorOut(
        self: *const Tensor, out: *const Tensor, end: *const Tensor, weight: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_lerp_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                end.c_tensor,
                weight.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn less(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_less(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn less_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_less_(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn lessEqual(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_less_equal(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn lessEqual_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_less_equal_(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn lessEqualScalarOut(
        self: *const Tensor, out: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_less_equal_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn lessEqualTensor(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_less_equal_tensor(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn lessEqualTensor_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_less_equal_tensor_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn lessEqualTensorOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_less_equal_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn lessScalarOut(
        self: *const Tensor, out: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_less_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn lessTensor(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_less_tensor(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn lessTensor_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_less_tensor_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn lessTensorOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_less_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn lgamma(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_lgamma(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn lgamma_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_lgamma_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn lgammaOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_lgamma_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn lift(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_lift(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn liftFresh(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_lift_fresh(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn liftFreshCopy(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_lift_fresh_copy(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn liftFreshCopyOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_lift_fresh_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn liftOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_lift_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgCholesky(
        self: *const Tensor, upper: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_cholesky(@ptrCast(&c_tensors), self.c_tensor,
                if (upper)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgCholeskyEx(
        self: *const Tensor, upper: bool, check_errors: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_linalg_cholesky_ex(@ptrCast(&c_tensors), self.c_tensor,
                if (upper)  1  else  0,
                if (check_errors)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn linalgCholeskyExL(
        self: *const Tensor, l: *const Tensor, info: *const Tensor, upper: bool, check_errors: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_linalg_cholesky_ex_l(@ptrCast(&c_tensors), l.c_tensor,
                info.c_tensor,
                self.c_tensor,
                if (upper)  1  else  0,
                if (check_errors)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn linalgCholeskyOut(
        self: *const Tensor, out: *const Tensor, upper: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_cholesky_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                if (upper)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgCond(
        self: *const Tensor, p: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_cond(@ptrCast(&c_tensors), self.c_tensor,
                p.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgCondOut(
        self: *const Tensor, out: *const Tensor, p: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_cond_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                p.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgCondPStr(
        self: *const Tensor, p: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_cond_p_str(@ptrCast(&c_tensors), self.c_tensor,
                p.ptr, p.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgCondPStrOut(
        self: *const Tensor, out: *const Tensor, p: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_cond_p_str_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                p.ptr, p.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgCross(
        self: *const Tensor, other: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_cross(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgCrossOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_cross_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgDet(
        a: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_det(@ptrCast(&c_tensors), a.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgDetOut(
        out: *const Tensor, a: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_det_out(@ptrCast(&c_tensors), out.c_tensor,
                a.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgDiagonal(
        a: *const Tensor, offset: i64, dim1: i64, dim2: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_diagonal(@ptrCast(&c_tensors), a.c_tensor,
                offset,
                dim1,
                dim2);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgEig(
        self: *const Tensor, 
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_linalg_eig(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn linalgEigOut(
        self: *const Tensor, eigenvalues: *const Tensor, eigenvectors: *const Tensor
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_linalg_eig_out(@ptrCast(&c_tensors), eigenvalues.c_tensor,
                eigenvectors.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn linalgEigh(
        self: *const Tensor, uplo: []const u8
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_linalg_eigh(@ptrCast(&c_tensors), self.c_tensor,
                uplo.ptr, uplo.len);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn linalgEighEigvals(
        self: *const Tensor, eigvals: *const Tensor, eigvecs: *const Tensor, uplo: []const u8
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_linalg_eigh_eigvals(@ptrCast(&c_tensors), eigvals.c_tensor,
                eigvecs.c_tensor,
                self.c_tensor,
                uplo.ptr, uplo.len);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn linalgEigvals(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_eigvals(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgEigvalsOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_eigvals_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgEigvalsh(
        self: *const Tensor, uplo: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_eigvalsh(@ptrCast(&c_tensors), self.c_tensor,
                uplo.ptr, uplo.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgEigvalshOut(
        self: *const Tensor, out: *const Tensor, uplo: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_eigvalsh_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                uplo.ptr, uplo.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgHouseholderProduct(
        self: *const Tensor, tau: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_householder_product(@ptrCast(&c_tensors), self.c_tensor,
                tau.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgHouseholderProductOut(
        self: *const Tensor, out: *const Tensor, tau: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_householder_product_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                tau.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgInv(
        a: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_inv(@ptrCast(&c_tensors), a.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgInvEx(
        a: *const Tensor, check_errors: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_linalg_inv_ex(@ptrCast(&c_tensors), a.c_tensor,
                if (check_errors)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn linalgInvExInverse(
        inverse_: *const Tensor, info: *const Tensor, a: *const Tensor, check_errors: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_linalg_inv_ex_inverse(@ptrCast(&c_tensors), inverse_.c_tensor,
                info.c_tensor,
                a.c_tensor,
                if (check_errors)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn linalgInvOut(
        out: *const Tensor, a: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_inv_out(@ptrCast(&c_tensors), out.c_tensor,
                a.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgLdlFactor(
        self: *const Tensor, hermitian: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_linalg_ldl_factor(@ptrCast(&c_tensors), self.c_tensor,
                if (hermitian)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn linalgLdlFactorEx(
        self: *const Tensor, hermitian: bool, check_errors: bool
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_linalg_ldl_factor_ex(@ptrCast(&c_tensors), self.c_tensor,
                if (hermitian)  1  else  0,
                if (check_errors)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn linalgLdlFactorExOut(
        self: *const Tensor, ld: *const Tensor, pivots: *const Tensor, info: *const Tensor, hermitian: bool, check_errors: bool
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_linalg_ldl_factor_ex_out(@ptrCast(&c_tensors), ld.c_tensor,
                pivots.c_tensor,
                info.c_tensor,
                self.c_tensor,
                if (hermitian)  1  else  0,
                if (check_errors)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn linalgLdlFactorOut(
        self: *const Tensor, ld: *const Tensor, pivots: *const Tensor, hermitian: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_linalg_ldl_factor_out(@ptrCast(&c_tensors), ld.c_tensor,
                pivots.c_tensor,
                self.c_tensor,
                if (hermitian)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn linalgLdlSolve(
        ld: *const Tensor, pivots: *const Tensor, b: *const Tensor, hermitian: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_ldl_solve(@ptrCast(&c_tensors), ld.c_tensor,
                pivots.c_tensor,
                b.c_tensor,
                if (hermitian)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgLdlSolveOut(
        out: *const Tensor, ld: *const Tensor, pivots: *const Tensor, b: *const Tensor, hermitian: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_ldl_solve_out(@ptrCast(&c_tensors), out.c_tensor,
                ld.c_tensor,
                pivots.c_tensor,
                b.c_tensor,
                if (hermitian)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgLstsq(
        self: *const Tensor, b: *const Tensor, rcond: ?f64, driver: []const u8
    ) [4]Tensor {
        var c_tensors = [_]C_tensor{null} ** 4;
        __c.atg_linalg_lstsq(@ptrCast(&c_tensors), self.c_tensor,
                b.c_tensor,
                rcond orelse std.math.nan, (rcond == null),
                driver.ptr, driver.len);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }, Tensor { .c_tensor = c_tensors[3] }};
    }

    pub fn linalgLstsqOut(
        self: *const Tensor, solution: *const Tensor, residuals: *const Tensor, rank: *const Tensor, singular_values: *const Tensor, b: *const Tensor, rcond: ?f64, driver: []const u8
    ) [4]Tensor {
        var c_tensors = [_]C_tensor{null} ** 4;
        __c.atg_linalg_lstsq_out(@ptrCast(&c_tensors), solution.c_tensor,
                residuals.c_tensor,
                rank.c_tensor,
                singular_values.c_tensor,
                self.c_tensor,
                b.c_tensor,
                rcond orelse std.math.nan, (rcond == null),
                driver.ptr, driver.len);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }, Tensor { .c_tensor = c_tensors[3] }};
    }

    pub fn linalgLu(
        a: *const Tensor, pivot: bool
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_linalg_lu(@ptrCast(&c_tensors), a.c_tensor,
                if (pivot)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn linalgLuFactor(
        a: *const Tensor, pivot: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_linalg_lu_factor(@ptrCast(&c_tensors), a.c_tensor,
                if (pivot)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn linalgLuFactorEx(
        a: *const Tensor, pivot: bool, check_errors: bool
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_linalg_lu_factor_ex(@ptrCast(&c_tensors), a.c_tensor,
                if (pivot)  1  else  0,
                if (check_errors)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn linalgLuFactorExOut(
        lu: *const Tensor, pivots: *const Tensor, info: *const Tensor, a: *const Tensor, pivot: bool, check_errors: bool
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_linalg_lu_factor_ex_out(@ptrCast(&c_tensors), lu.c_tensor,
                pivots.c_tensor,
                info.c_tensor,
                a.c_tensor,
                if (pivot)  1  else  0,
                if (check_errors)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn linalgLuFactorOut(
        lu: *const Tensor, pivots: *const Tensor, a: *const Tensor, pivot: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_linalg_lu_factor_out(@ptrCast(&c_tensors), lu.c_tensor,
                pivots.c_tensor,
                a.c_tensor,
                if (pivot)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn linalgLuOut(
        p: *const Tensor, l: *const Tensor, u: *const Tensor, a: *const Tensor, pivot: bool
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_linalg_lu_out(@ptrCast(&c_tensors), p.c_tensor,
                l.c_tensor,
                u.c_tensor,
                a.c_tensor,
                if (pivot)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn linalgLuSolve(
        lu: *const Tensor, pivots: *const Tensor, b: *const Tensor, left: bool, adjoint_: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_lu_solve(@ptrCast(&c_tensors), lu.c_tensor,
                pivots.c_tensor,
                b.c_tensor,
                if (left)  1  else  0,
                if (adjoint_)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgLuSolveOut(
        out: *const Tensor, lu: *const Tensor, pivots: *const Tensor, b: *const Tensor, left: bool, adjoint_: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_lu_solve_out(@ptrCast(&c_tensors), out.c_tensor,
                lu.c_tensor,
                pivots.c_tensor,
                b.c_tensor,
                if (left)  1  else  0,
                if (adjoint_)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgMatmul(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_matmul(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgMatmulOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_matmul_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgMatrixExp(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_matrix_exp(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgMatrixExpOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_matrix_exp_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgMatrixPower(
        self: *const Tensor, n: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_matrix_power(@ptrCast(&c_tensors), self.c_tensor,
                n);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgMatrixPowerOut(
        self: *const Tensor, out: *const Tensor, n: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_matrix_power_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                n);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgMatrixRank(
        self: *const Tensor, tol: f64, hermitian: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_matrix_rank(@ptrCast(&c_tensors), self.c_tensor,
                tol,
                if (hermitian)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgMatrixRankAtolRtolFloat(
        self: *const Tensor, atol: ?f64, rtol: ?f64, hermitian: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_matrix_rank_atol_rtol_float(@ptrCast(&c_tensors), self.c_tensor,
                atol orelse std.math.nan, (atol == null),
                rtol orelse std.math.nan, (rtol == null),
                if (hermitian)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgMatrixRankAtolRtolFloatOut(
        self: *const Tensor, out: *const Tensor, atol: ?f64, rtol: ?f64, hermitian: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_matrix_rank_atol_rtol_float_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                atol orelse std.math.nan, (atol == null),
                rtol orelse std.math.nan, (rtol == null),
                if (hermitian)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgMatrixRankAtolRtolTensor(
        self: *const Tensor, atol: ?*const Tensor, rtol: ?*const Tensor, hermitian: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_matrix_rank_atol_rtol_tensor(@ptrCast(&c_tensors), self.c_tensor,
                if (atol != null) atol.?.c_tensor else null,
                if (rtol != null) rtol.?.c_tensor else null,
                if (hermitian)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgMatrixRankAtolRtolTensorOut(
        self: *const Tensor, out: *const Tensor, atol: ?*const Tensor, rtol: ?*const Tensor, hermitian: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_matrix_rank_atol_rtol_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                if (atol != null) atol.?.c_tensor else null,
                if (rtol != null) rtol.?.c_tensor else null,
                if (hermitian)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgMatrixRankOut(
        self: *const Tensor, out: *const Tensor, tol: f64, hermitian: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_matrix_rank_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                tol,
                if (hermitian)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgMatrixRankOutTolTensor(
        self: *const Tensor, out: *const Tensor, tol: *const Tensor, hermitian: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_matrix_rank_out_tol_tensor(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                tol.c_tensor,
                if (hermitian)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgMatrixRankTolTensor(
        self: *const Tensor, tol: *const Tensor, hermitian: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_matrix_rank_tol_tensor(@ptrCast(&c_tensors), self.c_tensor,
                tol.c_tensor,
                if (hermitian)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgMultiDot(
        tensors: []*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_multi_dot(@ptrCast(&c_tensors), ptrList(tensors).ptr, @intCast(tensors.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgMultiDotOut(
        out: *const Tensor, tensors: []*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_multi_dot_out(@ptrCast(&c_tensors), out.c_tensor,
                ptrList(tensors).ptr, @intCast(tensors.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgNorm(
        self: *const Tensor, ord: Scalar, dim_: ?[]i64, keepdim: bool, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_norm(@ptrCast(&c_tensors), self.c_tensor,
                ord.into().c_scalar,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgNormOrdStr(
        self: *const Tensor, ord: []const u8, dim_: ?[]i64, keepdim: bool, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_norm_ord_str(@ptrCast(&c_tensors), self.c_tensor,
                ord.ptr, ord.len,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgNormOrdStrOut(
        self: *const Tensor, out: *const Tensor, ord: []const u8, dim_: ?[]i64, keepdim: bool, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_norm_ord_str_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                ord.ptr, ord.len,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgNormOut(
        self: *const Tensor, out: *const Tensor, ord: Scalar, dim_: ?[]i64, keepdim: bool, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_norm_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                ord.into().c_scalar,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgPinv(
        self: *const Tensor, rcond: f64, hermitian: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_pinv(@ptrCast(&c_tensors), self.c_tensor,
                rcond,
                if (hermitian)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgPinvAtolRtolFloat(
        self: *const Tensor, atol: ?f64, rtol: ?f64, hermitian: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_pinv_atol_rtol_float(@ptrCast(&c_tensors), self.c_tensor,
                atol orelse std.math.nan, (atol == null),
                rtol orelse std.math.nan, (rtol == null),
                if (hermitian)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgPinvAtolRtolFloatOut(
        self: *const Tensor, out: *const Tensor, atol: ?f64, rtol: ?f64, hermitian: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_pinv_atol_rtol_float_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                atol orelse std.math.nan, (atol == null),
                rtol orelse std.math.nan, (rtol == null),
                if (hermitian)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgPinvAtolRtolTensor(
        self: *const Tensor, atol: ?*const Tensor, rtol: ?*const Tensor, hermitian: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_pinv_atol_rtol_tensor(@ptrCast(&c_tensors), self.c_tensor,
                if (atol != null) atol.?.c_tensor else null,
                if (rtol != null) rtol.?.c_tensor else null,
                if (hermitian)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgPinvAtolRtolTensorOut(
        self: *const Tensor, out: *const Tensor, atol: ?*const Tensor, rtol: ?*const Tensor, hermitian: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_pinv_atol_rtol_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                if (atol != null) atol.?.c_tensor else null,
                if (rtol != null) rtol.?.c_tensor else null,
                if (hermitian)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgPinvOut(
        self: *const Tensor, out: *const Tensor, rcond: f64, hermitian: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_pinv_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                rcond,
                if (hermitian)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgPinvOutRcondTensor(
        self: *const Tensor, out: *const Tensor, rcond: *const Tensor, hermitian: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_pinv_out_rcond_tensor(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                rcond.c_tensor,
                if (hermitian)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgPinvRcondTensor(
        self: *const Tensor, rcond: *const Tensor, hermitian: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_pinv_rcond_tensor(@ptrCast(&c_tensors), self.c_tensor,
                rcond.c_tensor,
                if (hermitian)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgQr(
        a: *const Tensor, mode_: []const u8
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_linalg_qr(@ptrCast(&c_tensors), a.c_tensor,
                mode_.ptr, mode_.len);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn linalgQrOut(
        q: *const Tensor, r: *const Tensor, a: *const Tensor, mode_: []const u8
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_linalg_qr_out(@ptrCast(&c_tensors), q.c_tensor,
                r.c_tensor,
                a.c_tensor,
                mode_.ptr, mode_.len);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn linalgSlogdet(
        a: *const Tensor
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_linalg_slogdet(@ptrCast(&c_tensors), a.c_tensor);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn linalgSlogdetOut(
        sign_: *const Tensor, logabsdet: *const Tensor, a: *const Tensor
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_linalg_slogdet_out(@ptrCast(&c_tensors), sign_.c_tensor,
                logabsdet.c_tensor,
                a.c_tensor);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn linalgSolve(
        a: *const Tensor, b: *const Tensor, left: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_solve(@ptrCast(&c_tensors), a.c_tensor,
                b.c_tensor,
                if (left)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgSolveEx(
        a: *const Tensor, b: *const Tensor, left: bool, check_errors: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_linalg_solve_ex(@ptrCast(&c_tensors), a.c_tensor,
                b.c_tensor,
                if (left)  1  else  0,
                if (check_errors)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn linalgSolveExOut(
        result: *const Tensor, info: *const Tensor, a: *const Tensor, b: *const Tensor, left: bool, check_errors: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_linalg_solve_ex_out(@ptrCast(&c_tensors), result.c_tensor,
                info.c_tensor,
                a.c_tensor,
                b.c_tensor,
                if (left)  1  else  0,
                if (check_errors)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn linalgSolveOut(
        out: *const Tensor, a: *const Tensor, b: *const Tensor, left: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_solve_out(@ptrCast(&c_tensors), out.c_tensor,
                a.c_tensor,
                b.c_tensor,
                if (left)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgSolveTriangular(
        self: *const Tensor, b: *const Tensor, upper: bool, left: bool, unitriangular: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_solve_triangular(@ptrCast(&c_tensors), self.c_tensor,
                b.c_tensor,
                if (upper)  1  else  0,
                if (left)  1  else  0,
                if (unitriangular)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgSolveTriangularOut(
        self: *const Tensor, out: *const Tensor, b: *const Tensor, upper: bool, left: bool, unitriangular: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_solve_triangular_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                b.c_tensor,
                if (upper)  1  else  0,
                if (left)  1  else  0,
                if (unitriangular)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgSvd(
        a: *const Tensor, full_matrices: bool, driver: []const u8
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_linalg_svd(@ptrCast(&c_tensors), a.c_tensor,
                if (full_matrices)  1  else  0,
                driver.ptr, driver.len);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn linalgSvdU(
        u: *const Tensor, s: *const Tensor, vh: *const Tensor, a: *const Tensor, full_matrices: bool, driver: []const u8
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_linalg_svd_u(@ptrCast(&c_tensors), u.c_tensor,
                s.c_tensor,
                vh.c_tensor,
                a.c_tensor,
                if (full_matrices)  1  else  0,
                driver.ptr, driver.len);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn linalgSvdvals(
        a: *const Tensor, driver: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_svdvals(@ptrCast(&c_tensors), a.c_tensor,
                driver.ptr, driver.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgSvdvalsOut(
        out: *const Tensor, a: *const Tensor, driver: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_svdvals_out(@ptrCast(&c_tensors), out.c_tensor,
                a.c_tensor,
                driver.ptr, driver.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgTensorinv(
        self: *const Tensor, ind: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_tensorinv(@ptrCast(&c_tensors), self.c_tensor,
                ind);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgTensorinvOut(
        self: *const Tensor, out: *const Tensor, ind: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_tensorinv_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                ind);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgTensorsolve(
        self: *const Tensor, other: *const Tensor, dims: ?[]i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_tensorsolve(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor,
                dims.ptr, @intCast(dims.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgTensorsolveOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor, dims: ?[]i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_tensorsolve_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor,
                dims.ptr, @intCast(dims.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgVander(
        x: *const Tensor, n: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_vander(@ptrCast(&c_tensors), x.c_tensor,
                n orelse 0, (n == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgVecdot(
        x: *const Tensor, y: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_vecdot(@ptrCast(&c_tensors), x.c_tensor,
                y.c_tensor,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linalgVecdotOut(
        out: *const Tensor, x: *const Tensor, y: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linalg_vecdot_out(@ptrCast(&c_tensors), out.c_tensor,
                x.c_tensor,
                y.c_tensor,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linear(
        self: *const Tensor, weight: *const Tensor, bias: ?*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linear(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linearOut(
        self: *const Tensor, out: *const Tensor, weight: *const Tensor, bias: ?*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linear_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linspace(
        start: Scalar, end: Scalar, steps: i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linspace(@ptrCast(&c_tensors), start.into().c_scalar,
                end.into().c_scalar,
                steps,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linspaceOut(
        out: *const Tensor, start: Scalar, end: Scalar, steps: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linspace_out(@ptrCast(&c_tensors), out.c_tensor,
                start.into().c_scalar,
                end.into().c_scalar,
                steps);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linspaceScalarTensor(
        start: Scalar, end: *const Tensor, steps: i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linspace_scalar_tensor(@ptrCast(&c_tensors), start.into().c_scalar,
                end.c_tensor,
                steps,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linspaceScalarTensorOut(
        out: *const Tensor, start: Scalar, end: *const Tensor, steps: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linspace_scalar_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                start.into().c_scalar,
                end.c_tensor,
                steps);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linspaceTensorScalar(
        start: *const Tensor, end: Scalar, steps: i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linspace_tensor_scalar(@ptrCast(&c_tensors), start.c_tensor,
                end.into().c_scalar,
                steps,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linspaceTensorScalarOut(
        out: *const Tensor, start: *const Tensor, end: Scalar, steps: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linspace_tensor_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                start.c_tensor,
                end.into().c_scalar,
                steps);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linspaceTensorTensor(
        start: *const Tensor, end: *const Tensor, steps: i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linspace_tensor_tensor(@ptrCast(&c_tensors), start.c_tensor,
                end.c_tensor,
                steps,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn linspaceTensorTensorOut(
        out: *const Tensor, start: *const Tensor, end: *const Tensor, steps: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_linspace_tensor_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                start.c_tensor,
                end.c_tensor,
                steps);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn log(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_log(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn log10(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_log10(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn log10_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_log10_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn log10Out(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_log10_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn log1p(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_log1p(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn log1p_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_log1p_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn log1pOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_log1p_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn log2(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_log2(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn log2_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_log2_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn log2Out(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_log2_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn log_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_log_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logNormal(
        self: *const Tensor, mean_: f64, std_dev: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_log_normal(@ptrCast(&c_tensors), self.c_tensor,
                mean_,
                std_dev);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logNormal_(
        self: *Tensor, mean_: f64, std_dev: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_log_normal_(@ptrCast(&c_tensors), self.c_tensor,
                mean_,
                std_dev);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logNormalOut(
        self: *const Tensor, out: *const Tensor, mean_: f64, std_dev: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_log_normal_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                mean_,
                std_dev);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_log_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logSigmoid(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_log_sigmoid(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logSigmoidBackward(
        self: *const Tensor, grad_output: *const Tensor, buffer: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_log_sigmoid_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                buffer.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logSigmoidBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor, buffer: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_log_sigmoid_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                buffer.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logSigmoidOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_log_sigmoid_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logSoftmax(
        self: *const Tensor, dim_: i64, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_log_softmax(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logSoftmaxIntOut(
        self: *const Tensor, out: *const Tensor, dim_: i64, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_log_softmax_int_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logaddexp(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logaddexp(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logaddexp2(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logaddexp2(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logaddexp2Out(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logaddexp2_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logaddexpOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logaddexp_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logcumsumexp(
        self: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logcumsumexp(@ptrCast(&c_tensors), self.c_tensor,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logcumsumexpOut(
        self: *const Tensor, out: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logcumsumexp_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logdet(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logdet(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logicalAnd(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logical_and(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logicalAnd_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logical_and_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logicalAndOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logical_and_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logicalNot(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logical_not(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logicalNot_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logical_not_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logicalNotOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logical_not_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logicalOr(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logical_or(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logicalOr_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logical_or_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logicalOrOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logical_or_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logicalXor(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logical_xor(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logicalXor_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logical_xor_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logicalXorOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logical_xor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logit(
        self: *const Tensor, eps: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logit(@ptrCast(&c_tensors), self.c_tensor,
                eps orelse std.math.nan, (eps == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logit_(
        self: *Tensor, eps: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logit_(@ptrCast(&c_tensors), self.c_tensor,
                eps orelse std.math.nan, (eps == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logitBackward(
        self: *const Tensor, grad_output: *const Tensor, eps: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logit_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                eps orelse std.math.nan, (eps == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logitBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor, eps: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logit_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                eps orelse std.math.nan, (eps == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logitOut(
        self: *const Tensor, out: *const Tensor, eps: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logit_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                eps orelse std.math.nan, (eps == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logspace(
        start: Scalar, end: Scalar, steps: i64, base: f64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logspace(@ptrCast(&c_tensors), start.into().c_scalar,
                end.into().c_scalar,
                steps,
                base,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logspaceOut(
        out: *const Tensor, start: Scalar, end: Scalar, steps: i64, base: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logspace_out(@ptrCast(&c_tensors), out.c_tensor,
                start.into().c_scalar,
                end.into().c_scalar,
                steps,
                base);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logspaceScalarTensor(
        start: Scalar, end: *const Tensor, steps: i64, base: f64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logspace_scalar_tensor(@ptrCast(&c_tensors), start.into().c_scalar,
                end.c_tensor,
                steps,
                base,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logspaceScalarTensorOut(
        out: *const Tensor, start: Scalar, end: *const Tensor, steps: i64, base: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logspace_scalar_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                start.into().c_scalar,
                end.c_tensor,
                steps,
                base);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logspaceTensorScalar(
        start: *const Tensor, end: Scalar, steps: i64, base: f64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logspace_tensor_scalar(@ptrCast(&c_tensors), start.c_tensor,
                end.into().c_scalar,
                steps,
                base,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logspaceTensorScalarOut(
        out: *const Tensor, start: *const Tensor, end: Scalar, steps: i64, base: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logspace_tensor_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                start.c_tensor,
                end.into().c_scalar,
                steps,
                base);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logspaceTensorTensor(
        start: *const Tensor, end: *const Tensor, steps: i64, base: f64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logspace_tensor_tensor(@ptrCast(&c_tensors), start.c_tensor,
                end.c_tensor,
                steps,
                base,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logspaceTensorTensorOut(
        out: *const Tensor, start: *const Tensor, end: *const Tensor, steps: i64, base: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logspace_tensor_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                start.c_tensor,
                end.c_tensor,
                steps,
                base);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logsumexp(
        self: *const Tensor, dim_: []i64, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logsumexp(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn logsumexpOut(
        self: *const Tensor, out: *const Tensor, dim_: []i64, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_logsumexp_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn lstm(
        self: *const Tensor, hx: []*const Tensor, params: []*const Tensor, has_biases: bool, num_layers: i64, dropout_: f64, train: bool, bidirectional: bool, batch_first: bool
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_lstm(@ptrCast(&c_tensors), self.c_tensor,
                ptrList(hx).ptr, @intCast(hx.len),
                ptrList(params).ptr, @intCast(params.len),
                if (has_biases)  1  else  0,
                num_layers,
                dropout_,
                if (train)  1  else  0,
                if (bidirectional)  1  else  0,
                if (batch_first)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn lstmCell(
        self: *const Tensor, hx: []*const Tensor, w_ih: *const Tensor, w_hh: *const Tensor, b_ih: ?*const Tensor, b_hh: ?*const Tensor
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_lstm_cell(@ptrCast(&c_tensors), self.c_tensor,
                ptrList(hx).ptr, @intCast(hx.len),
                w_ih.c_tensor,
                w_hh.c_tensor,
                if (b_ih != null) b_ih.?.c_tensor else null,
                if (b_hh != null) b_hh.?.c_tensor else null);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn lstmData(
        data_: *const Tensor, batch_sizes: *const Tensor, hx: []*const Tensor, params: []*const Tensor, has_biases: bool, num_layers: i64, dropout_: f64, train: bool, bidirectional: bool
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_lstm_data(@ptrCast(&c_tensors), data_.c_tensor,
                batch_sizes.c_tensor,
                ptrList(hx).ptr, @intCast(hx.len),
                ptrList(params).ptr, @intCast(params.len),
                if (has_biases)  1  else  0,
                num_layers,
                dropout_,
                if (train)  1  else  0,
                if (bidirectional)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn lstmMpsBackward(
        self: *const Tensor, out0: *const Tensor, out1: []*const Tensor, out2: []*const Tensor, grad_y: ?*const Tensor, grad_hy: ?*const Tensor, grad_cy: ?*const Tensor, z_state: *const Tensor, cell_state_fwd: *const Tensor, layersoutputs: *const Tensor, hx: []*const Tensor, params: []*const Tensor, has_biases: bool, num_layers: i64, dropout_: f64, train: bool, bidirectional: bool, batch_first: bool
    ) void {
        __c.atg_lstm_mps_backward(out0.c_tensor,
                ptrList(out1).ptr, @intCast(out1.len),
                ptrList(out2).ptr, @intCast(out2.len),
                if (grad_y != null) grad_y.?.c_tensor else null,
                if (grad_hy != null) grad_hy.?.c_tensor else null,
                if (grad_cy != null) grad_cy.?.c_tensor else null,
                z_state.c_tensor,
                cell_state_fwd.c_tensor,
                self.c_tensor,
                layersoutputs.c_tensor,
                ptrList(hx).ptr, @intCast(hx.len),
                ptrList(params).ptr, @intCast(params.len),
                if (has_biases)  1  else  0,
                num_layers,
                dropout_,
                if (train)  1  else  0,
                if (bidirectional)  1  else  0,
                if (batch_first)  1  else  0);
        torch.readAndCleanError();
        return;
    }

    pub fn lt(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_lt(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn lt_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_lt_(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn ltScalarOut(
        self: *const Tensor, out: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_lt_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn ltTensor(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_lt_tensor(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn ltTensor_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_lt_tensor_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn ltTensorOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_lt_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn luSolve(
        self: *const Tensor, lu_data: *const Tensor, lu_pivots: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_lu_solve(@ptrCast(&c_tensors), self.c_tensor,
                lu_data.c_tensor,
                lu_pivots.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn luSolveOut(
        self: *const Tensor, out: *const Tensor, lu_data: *const Tensor, lu_pivots: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_lu_solve_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                lu_data.c_tensor,
                lu_pivots.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn luUnpack(
        lu_data: *const Tensor, lu_pivots: *const Tensor, unpack_data: bool, unpack_pivots: bool
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_lu_unpack(@ptrCast(&c_tensors), lu_data.c_tensor,
                lu_pivots.c_tensor,
                if (unpack_data)  1  else  0,
                if (unpack_pivots)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn luUnpackOut(
        p: *const Tensor, l: *const Tensor, u: *const Tensor, lu_data: *const Tensor, lu_pivots: *const Tensor, unpack_data: bool, unpack_pivots: bool
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_lu_unpack_out(@ptrCast(&c_tensors), p.c_tensor,
                l.c_tensor,
                u.c_tensor,
                lu_data.c_tensor,
                lu_pivots.c_tensor,
                if (unpack_data)  1  else  0,
                if (unpack_pivots)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn marginRankingLoss(
        input1: *const Tensor, input2: *const Tensor, target: *const Tensor, margin: f64, reduction: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_margin_ranking_loss(@ptrCast(&c_tensors), input1.c_tensor,
                input2.c_tensor,
                target.c_tensor,
                margin,
                reduction.to_int());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn maskedFill(
        self: *const Tensor, mask: *const Tensor, value: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_masked_fill(@ptrCast(&c_tensors), self.c_tensor,
                mask.c_tensor,
                value.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn maskedFill_(
        self: *Tensor, mask: *const Tensor, value: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_masked_fill_(@ptrCast(&c_tensors), self.c_tensor,
                mask.c_tensor,
                value.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn maskedFillScalarOut(
        self: *const Tensor, out: *const Tensor, mask: *const Tensor, value: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_masked_fill_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                mask.c_tensor,
                value.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn maskedFillTensor(
        self: *const Tensor, mask: *const Tensor, value: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_masked_fill_tensor(@ptrCast(&c_tensors), self.c_tensor,
                mask.c_tensor,
                value.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn maskedFillTensor_(
        self: *Tensor, mask: *const Tensor, value: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_masked_fill_tensor_(@ptrCast(&c_tensors), self.c_tensor,
                mask.c_tensor,
                value.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn maskedFillTensorOut(
        self: *const Tensor, out: *const Tensor, mask: *const Tensor, value: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_masked_fill_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                mask.c_tensor,
                value.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn maskedScatter(
        self: *const Tensor, mask: *const Tensor, source: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_masked_scatter(@ptrCast(&c_tensors), self.c_tensor,
                mask.c_tensor,
                source.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn maskedScatter_(
        self: *Tensor, mask: *const Tensor, source: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_masked_scatter_(@ptrCast(&c_tensors), self.c_tensor,
                mask.c_tensor,
                source.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn maskedScatterBackward(
        grad_output: *const Tensor, mask: *const Tensor, sizes: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_masked_scatter_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                mask.c_tensor,
                sizes.ptr, @intCast(sizes.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn maskedScatterOut(
        self: *const Tensor, out: *const Tensor, mask: *const Tensor, source: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_masked_scatter_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                mask.c_tensor,
                source.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn maskedSelect(
        self: *const Tensor, mask: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_masked_select(@ptrCast(&c_tensors), self.c_tensor,
                mask.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn maskedSelectBackward(
        self: *const Tensor, gradient: *const Tensor, mask: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_masked_select_backward(@ptrCast(&c_tensors), gradient.c_tensor,
                self.c_tensor,
                mask.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn maskedSelectOut(
        self: *const Tensor, out: *const Tensor, mask: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_masked_select_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                mask.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn matmul(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_matmul(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn matmulOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_matmul_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn matrixExp(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_matrix_exp(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn matrixExpBackward(
        self: *const Tensor, gradient: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_matrix_exp_backward(@ptrCast(&c_tensors), self.c_tensor,
                gradient.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn matrixH(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_matrix_h(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn matrixPower(
        self: *const Tensor, n: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_matrix_power(@ptrCast(&c_tensors), self.c_tensor,
                n);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn matrixPowerOut(
        self: *const Tensor, out: *const Tensor, n: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_matrix_power_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                n);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn max(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_max(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn maxDim(
        self: *const Tensor, dim_: i64, keepdim: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_max_dim(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn maxDimMax(
        self: *const Tensor, max_: *const Tensor, max_values: *const Tensor, dim_: i64, keepdim: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_max_dim_max(@ptrCast(&c_tensors), max_.c_tensor,
                max_values.c_tensor,
                self.c_tensor,
                dim_,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn maxOther(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_max_other(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn maxOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_max_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn maxPool1d(
        self: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, dilation: []i64, ceil_mode: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_max_pool1d(@ptrCast(&c_tensors), self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (ceil_mode)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn maxPool1dWithIndices(
        self: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, dilation: []i64, ceil_mode: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_max_pool1d_with_indices(@ptrCast(&c_tensors), self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (ceil_mode)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn maxPool2d(
        self: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, dilation: []i64, ceil_mode: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_max_pool2d(@ptrCast(&c_tensors), self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (ceil_mode)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn maxPool2dBackward(
        self: *const Tensor, grad_output: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, dilation: []i64, ceil_mode: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_max_pool2d_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (ceil_mode)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn maxPool2dBackwardOut(
        self: *const Tensor, out: *const Tensor, grad_output: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, dilation: []i64, ceil_mode: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_max_pool2d_backward_out(@ptrCast(&c_tensors), out.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (ceil_mode)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn maxPool2dWithIndices(
        self: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, dilation: []i64, ceil_mode: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_max_pool2d_with_indices(@ptrCast(&c_tensors), self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (ceil_mode)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn maxPool2dWithIndicesBackward(
        self: *const Tensor, grad_output: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, dilation: []i64, ceil_mode: bool, indices_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_max_pool2d_with_indices_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (ceil_mode)  1  else  0,
                indices_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn maxPool2dWithIndicesBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, dilation: []i64, ceil_mode: bool, indices_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_max_pool2d_with_indices_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (ceil_mode)  1  else  0,
                indices_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn maxPool2dWithIndicesOut(
        self: *const Tensor, out: *const Tensor, indices_: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, dilation: []i64, ceil_mode: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_max_pool2d_with_indices_out(@ptrCast(&c_tensors), out.c_tensor,
                indices_.c_tensor,
                self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (ceil_mode)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn maxPool3d(
        self: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, dilation: []i64, ceil_mode: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_max_pool3d(@ptrCast(&c_tensors), self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (ceil_mode)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn maxPool3dWithIndices(
        self: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, dilation: []i64, ceil_mode: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_max_pool3d_with_indices(@ptrCast(&c_tensors), self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (ceil_mode)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn maxPool3dWithIndicesBackward(
        self: *const Tensor, grad_output: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, dilation: []i64, ceil_mode: bool, indices_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_max_pool3d_with_indices_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (ceil_mode)  1  else  0,
                indices_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn maxPool3dWithIndicesBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, dilation: []i64, ceil_mode: bool, indices_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_max_pool3d_with_indices_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (ceil_mode)  1  else  0,
                indices_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn maxPool3dWithIndicesOut(
        self: *const Tensor, out: *const Tensor, indices_: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, dilation: []i64, ceil_mode: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_max_pool3d_with_indices_out(@ptrCast(&c_tensors), out.c_tensor,
                indices_.c_tensor,
                self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (ceil_mode)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn maxUnaryOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_max_unary_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn maxUnpool2d(
        self: *const Tensor, indices_: *const Tensor, output_size: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_max_unpool2d(@ptrCast(&c_tensors), self.c_tensor,
                indices_.c_tensor,
                output_size.ptr, @intCast(output_size.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn maxUnpool2dOut(
        self: *const Tensor, out: *const Tensor, indices_: *const Tensor, output_size: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_max_unpool2d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                indices_.c_tensor,
                output_size.ptr, @intCast(output_size.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn maxUnpool3d(
        self: *const Tensor, indices_: *const Tensor, output_size: []i64, stride_: []i64, padding: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_max_unpool3d(@ptrCast(&c_tensors), self.c_tensor,
                indices_.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn maxUnpool3dOut(
        self: *const Tensor, out: *const Tensor, indices_: *const Tensor, output_size: []i64, stride_: []i64, padding: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_max_unpool3d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                indices_.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn maximum(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_maximum(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn maximumOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_maximum_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mean(
        self: *const Tensor, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mean(@ptrCast(&c_tensors), self.c_tensor,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn meanDim(
        self: *const Tensor, dim_: ?[]i64, keepdim: bool, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mean_dim(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn meanOut(
        self: *const Tensor, out: *const Tensor, dim_: ?[]i64, keepdim: bool, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mean_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn median(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_median(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn medianDim(
        self: *const Tensor, dim_: i64, keepdim: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_median_dim(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn medianDimValues(
        self: *const Tensor, values_: *const Tensor, indices_: *const Tensor, dim_: i64, keepdim: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_median_dim_values(@ptrCast(&c_tensors), values_.c_tensor,
                indices_.c_tensor,
                self.c_tensor,
                dim_,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn medianOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_median_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn meshgrid(
        tensors: []*const Tensor
    ) []Tensor {
        const c_tensors = __c.atg_meshgrid(ptrList(tensors).ptr, @intCast(tensors.len));
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn meshgridIndexing(
        tensors: []*const Tensor, indexing: []const u8
    ) []Tensor {
        const c_tensors = __c.atg_meshgrid_indexing(ptrList(tensors).ptr, @intCast(tensors.len),
                indexing.ptr, indexing.len);
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn mh(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mh(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn min(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_min(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn minDim(
        self: *const Tensor, dim_: i64, keepdim: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_min_dim(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn minDimMin(
        self: *const Tensor, min_: *const Tensor, min_indices: *const Tensor, dim_: i64, keepdim: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_min_dim_min(@ptrCast(&c_tensors), min_.c_tensor,
                min_indices.c_tensor,
                self.c_tensor,
                dim_,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn minOther(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_min_other(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn minOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_min_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn minUnaryOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_min_unary_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn minimum(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_minimum(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn minimumOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_minimum_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn miopenBatchNorm(
        self: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, running_mean: ?*const Tensor, running_var: ?*const Tensor, training: bool, exponential_average_factor: f64, epsilon: f64
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_miopen_batch_norm(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                if (running_mean != null) running_mean.?.c_tensor else null,
                if (running_var != null) running_var.?.c_tensor else null,
                if (training)  1  else  0,
                exponential_average_factor,
                epsilon);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn miopenBatchNormBackward(
        self: *const Tensor, grad_output: *const Tensor, weight: *const Tensor, running_mean: ?*const Tensor, running_var: ?*const Tensor, save_mean: ?*const Tensor, save_var: ?*const Tensor, epsilon: f64
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_miopen_batch_norm_backward(@ptrCast(&c_tensors), self.c_tensor,
                grad_output.c_tensor,
                weight.c_tensor,
                if (running_mean != null) running_mean.?.c_tensor else null,
                if (running_var != null) running_var.?.c_tensor else null,
                if (save_mean != null) save_mean.?.c_tensor else null,
                if (save_var != null) save_var.?.c_tensor else null,
                epsilon);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn miopenBatchNormBackwardOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, out2: *const Tensor, grad_output: *const Tensor, weight: *const Tensor, running_mean: ?*const Tensor, running_var: ?*const Tensor, save_mean: ?*const Tensor, save_var: ?*const Tensor, epsilon: f64
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_miopen_batch_norm_backward_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                out2.c_tensor,
                self.c_tensor,
                grad_output.c_tensor,
                weight.c_tensor,
                if (running_mean != null) running_mean.?.c_tensor else null,
                if (running_var != null) running_var.?.c_tensor else null,
                if (save_mean != null) save_mean.?.c_tensor else null,
                if (save_var != null) save_var.?.c_tensor else null,
                epsilon);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn miopenBatchNormOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, out2: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, running_mean: ?*const Tensor, running_var: ?*const Tensor, training: bool, exponential_average_factor: f64, epsilon: f64
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_miopen_batch_norm_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                out2.c_tensor,
                self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                if (running_mean != null) running_mean.?.c_tensor else null,
                if (running_var != null) running_var.?.c_tensor else null,
                if (training)  1  else  0,
                exponential_average_factor,
                epsilon);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn miopenConvolution(
        self: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, padding: []i64, stride_: []i64, dilation: []i64, groups: i64, benchmark: bool, deterministic: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_miopen_convolution(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                padding.ptr, @intCast(padding.len),
                stride_.ptr, @intCast(stride_.len),
                dilation.ptr, @intCast(dilation.len),
                groups,
                if (benchmark)  1  else  0,
                if (deterministic)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn miopenConvolutionAddRelu(
        self: *const Tensor, weight: *const Tensor, z: *const Tensor, alpha: Scalar, bias: ?*const Tensor, stride_: []i64, padding: []i64, dilation: []i64, groups: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_miopen_convolution_add_relu(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                z.c_tensor,
                alpha.into().c_scalar,
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                groups);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn miopenConvolutionOut(
        self: *const Tensor, out: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, padding: []i64, stride_: []i64, dilation: []i64, groups: i64, benchmark: bool, deterministic: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_miopen_convolution_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                padding.ptr, @intCast(padding.len),
                stride_.ptr, @intCast(stride_.len),
                dilation.ptr, @intCast(dilation.len),
                groups,
                if (benchmark)  1  else  0,
                if (deterministic)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn miopenConvolutionRelu(
        self: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, stride_: []i64, padding: []i64, dilation: []i64, groups: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_miopen_convolution_relu(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                groups);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn miopenConvolutionTranspose(
        self: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, padding: []i64, output_padding: []i64, stride_: []i64, dilation: []i64, groups: i64, benchmark: bool, deterministic: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_miopen_convolution_transpose(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                padding.ptr, @intCast(padding.len),
                output_padding.ptr, @intCast(output_padding.len),
                stride_.ptr, @intCast(stride_.len),
                dilation.ptr, @intCast(dilation.len),
                groups,
                if (benchmark)  1  else  0,
                if (deterministic)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn miopenConvolutionTransposeOut(
        self: *const Tensor, out: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, padding: []i64, output_padding: []i64, stride_: []i64, dilation: []i64, groups: i64, benchmark: bool, deterministic: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_miopen_convolution_transpose_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                padding.ptr, @intCast(padding.len),
                output_padding.ptr, @intCast(output_padding.len),
                stride_.ptr, @intCast(stride_.len),
                dilation.ptr, @intCast(dilation.len),
                groups,
                if (benchmark)  1  else  0,
                if (deterministic)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn miopenDepthwiseConvolution(
        self: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, padding: []i64, stride_: []i64, dilation: []i64, groups: i64, benchmark: bool, deterministic: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_miopen_depthwise_convolution(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                padding.ptr, @intCast(padding.len),
                stride_.ptr, @intCast(stride_.len),
                dilation.ptr, @intCast(dilation.len),
                groups,
                if (benchmark)  1  else  0,
                if (deterministic)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn miopenDepthwiseConvolutionOut(
        self: *const Tensor, out: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, padding: []i64, stride_: []i64, dilation: []i64, groups: i64, benchmark: bool, deterministic: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_miopen_depthwise_convolution_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                padding.ptr, @intCast(padding.len),
                stride_.ptr, @intCast(stride_.len),
                dilation.ptr, @intCast(dilation.len),
                groups,
                if (benchmark)  1  else  0,
                if (deterministic)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn miopenRnn(
        self: *const Tensor, weight: []*const Tensor, weight_stride0: i64, hx: *const Tensor, cx: ?*const Tensor, mode_: i64, hidden_size: i64, num_layers: i64, batch_first: bool, dropout_: f64, train: bool, bidirectional: bool, batch_sizes: []i64, dropout_state: ?*const Tensor
    ) [5]Tensor {
        var c_tensors = [_]C_tensor{null} ** 5;
        __c.atg_miopen_rnn(@ptrCast(&c_tensors), self.c_tensor,
                ptrList(weight).ptr, @intCast(weight.len),
                weight_stride0,
                hx.c_tensor,
                if (cx != null) cx.?.c_tensor else null,
                mode_,
                hidden_size,
                num_layers,
                if (batch_first)  1  else  0,
                dropout_,
                if (train)  1  else  0,
                if (bidirectional)  1  else  0,
                batch_sizes.ptr, @intCast(batch_sizes.len),
                if (dropout_state != null) dropout_state.?.c_tensor else null);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }, Tensor { .c_tensor = c_tensors[3] }, Tensor { .c_tensor = c_tensors[4] }};
    }

    pub fn miopenRnnOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, out2: *const Tensor, out3: *const Tensor, out4: *const Tensor, weight: []*const Tensor, weight_stride0: i64, hx: *const Tensor, cx: ?*const Tensor, mode_: i64, hidden_size: i64, num_layers: i64, batch_first: bool, dropout_: f64, train: bool, bidirectional: bool, batch_sizes: []i64, dropout_state: ?*const Tensor
    ) [5]Tensor {
        var c_tensors = [_]C_tensor{null} ** 5;
        __c.atg_miopen_rnn_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                out2.c_tensor,
                out3.c_tensor,
                out4.c_tensor,
                self.c_tensor,
                ptrList(weight).ptr, @intCast(weight.len),
                weight_stride0,
                hx.c_tensor,
                if (cx != null) cx.?.c_tensor else null,
                mode_,
                hidden_size,
                num_layers,
                if (batch_first)  1  else  0,
                dropout_,
                if (train)  1  else  0,
                if (bidirectional)  1  else  0,
                batch_sizes.ptr, @intCast(batch_sizes.len),
                if (dropout_state != null) dropout_state.?.c_tensor else null);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }, Tensor { .c_tensor = c_tensors[3] }, Tensor { .c_tensor = c_tensors[4] }};
    }

    pub fn mish(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mish(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mish_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mish_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mishBackward(
        self: *const Tensor, grad_output: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mish_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mishOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mish_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mkldnnAdaptiveAvgPool2d(
        self: *const Tensor, output_size: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mkldnn_adaptive_avg_pool2d(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mkldnnAdaptiveAvgPool2dBackward(
        self: *const Tensor, grad_output: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mkldnn_adaptive_avg_pool2d_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mkldnnAdaptiveAvgPool2dBackwardOut(
        self: *const Tensor, out: *const Tensor, grad_output: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mkldnn_adaptive_avg_pool2d_backward_out(@ptrCast(&c_tensors), out.c_tensor,
                grad_output.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mkldnnAdaptiveAvgPool2dOut(
        self: *const Tensor, out: *const Tensor, output_size: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mkldnn_adaptive_avg_pool2d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                output_size.ptr, @intCast(output_size.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mkldnnConvolution(
        self: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, padding: []i64, stride_: []i64, dilation: []i64, groups: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mkldnn_convolution(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                padding.ptr, @intCast(padding.len),
                stride_.ptr, @intCast(stride_.len),
                dilation.ptr, @intCast(dilation.len),
                groups);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mkldnnConvolutionOut(
        self: *const Tensor, out: *const Tensor, weight: *const Tensor, bias: ?*const Tensor, padding: []i64, stride_: []i64, dilation: []i64, groups: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mkldnn_convolution_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null,
                padding.ptr, @intCast(padding.len),
                stride_.ptr, @intCast(stride_.len),
                dilation.ptr, @intCast(dilation.len),
                groups);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mkldnnLinear(
        self: *const Tensor, weight: *const Tensor, bias: ?*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mkldnn_linear(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mkldnnLinearBackwardInput(
        input_size: []i64, grad_output: *const Tensor, weight: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mkldnn_linear_backward_input(@ptrCast(&c_tensors), input_size.ptr, @intCast(input_size.len),
                grad_output.c_tensor,
                weight.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mkldnnLinearBackwardInputOut(
        out: *const Tensor, input_size: []i64, grad_output: *const Tensor, weight: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mkldnn_linear_backward_input_out(@ptrCast(&c_tensors), out.c_tensor,
                input_size.ptr, @intCast(input_size.len),
                grad_output.c_tensor,
                weight.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mkldnnLinearBackwardWeights(
        self: *const Tensor, grad_output: *const Tensor, weight: *const Tensor, bias_defined: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_mkldnn_linear_backward_weights(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                weight.c_tensor,
                if (bias_defined)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn mkldnnLinearBackwardWeightsOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, grad_output: *const Tensor, weight: *const Tensor, bias_defined: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_mkldnn_linear_backward_weights_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                weight.c_tensor,
                if (bias_defined)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn mkldnnLinearOut(
        self: *const Tensor, out: *const Tensor, weight: *const Tensor, bias: ?*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mkldnn_linear_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                weight.c_tensor,
                if (bias != null) bias.?.c_tensor else null);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mkldnnMaxPool2d(
        self: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, dilation: []i64, ceil_mode: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mkldnn_max_pool2d(@ptrCast(&c_tensors), self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (ceil_mode)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mkldnnMaxPool2dBackward(
        self: *const Tensor, grad_output: *const Tensor, output: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, dilation: []i64, ceil_mode: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mkldnn_max_pool2d_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                output.c_tensor,
                self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (ceil_mode)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mkldnnMaxPool2dBackwardOut(
        self: *const Tensor, out: *const Tensor, grad_output: *const Tensor, output: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, dilation: []i64, ceil_mode: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mkldnn_max_pool2d_backward_out(@ptrCast(&c_tensors), out.c_tensor,
                grad_output.c_tensor,
                output.c_tensor,
                self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (ceil_mode)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mkldnnMaxPool2dOut(
        self: *const Tensor, out: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, dilation: []i64, ceil_mode: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mkldnn_max_pool2d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (ceil_mode)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mkldnnMaxPool3d(
        self: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, dilation: []i64, ceil_mode: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mkldnn_max_pool3d(@ptrCast(&c_tensors), self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (ceil_mode)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mkldnnMaxPool3dBackward(
        self: *const Tensor, grad_output: *const Tensor, output: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, dilation: []i64, ceil_mode: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mkldnn_max_pool3d_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                output.c_tensor,
                self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (ceil_mode)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mkldnnMaxPool3dBackwardOut(
        self: *const Tensor, out: *const Tensor, grad_output: *const Tensor, output: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, dilation: []i64, ceil_mode: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mkldnn_max_pool3d_backward_out(@ptrCast(&c_tensors), out.c_tensor,
                grad_output.c_tensor,
                output.c_tensor,
                self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (ceil_mode)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mkldnnMaxPool3dOut(
        self: *const Tensor, out: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, dilation: []i64, ceil_mode: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mkldnn_max_pool3d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (ceil_mode)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mkldnnReorderConv2dWeight(
        self: *const Tensor, padding: []i64, stride_: []i64, dilation: []i64, groups: i64, input_size: ?[]i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mkldnn_reorder_conv2d_weight(@ptrCast(&c_tensors), self.c_tensor,
                padding.ptr, @intCast(padding.len),
                stride_.ptr, @intCast(stride_.len),
                dilation.ptr, @intCast(dilation.len),
                groups,
                input_size.ptr, @intCast(input_size.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mkldnnReorderConv2dWeightOut(
        self: *const Tensor, out: *const Tensor, padding: []i64, stride_: []i64, dilation: []i64, groups: i64, input_size: ?[]i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mkldnn_reorder_conv2d_weight_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                padding.ptr, @intCast(padding.len),
                stride_.ptr, @intCast(stride_.len),
                dilation.ptr, @intCast(dilation.len),
                groups,
                input_size.ptr, @intCast(input_size.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mkldnnReorderConv3dWeight(
        self: *const Tensor, padding: []i64, stride_: []i64, dilation: []i64, groups: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mkldnn_reorder_conv3d_weight(@ptrCast(&c_tensors), self.c_tensor,
                padding.ptr, @intCast(padding.len),
                stride_.ptr, @intCast(stride_.len),
                dilation.ptr, @intCast(dilation.len),
                groups);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mkldnnReorderConv3dWeightOut(
        self: *const Tensor, out: *const Tensor, padding: []i64, stride_: []i64, dilation: []i64, groups: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mkldnn_reorder_conv3d_weight_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                padding.ptr, @intCast(padding.len),
                stride_.ptr, @intCast(stride_.len),
                dilation.ptr, @intCast(dilation.len),
                groups);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mkldnnRnnLayer(
        self: *const Tensor, weight0: *const Tensor, weight1: *const Tensor, weight2: *const Tensor, weight3: *const Tensor, hx_: *const Tensor, cx_: *const Tensor, reverse: bool, batch_sizes: []i64, mode_: i64, hidden_size: i64, num_layers: i64, has_biases: bool, bidirectional: bool, batch_first: bool, train: bool
    ) [4]Tensor {
        var c_tensors = [_]C_tensor{null} ** 4;
        __c.atg_mkldnn_rnn_layer(@ptrCast(&c_tensors), self.c_tensor,
                weight0.c_tensor,
                weight1.c_tensor,
                weight2.c_tensor,
                weight3.c_tensor,
                hx_.c_tensor,
                cx_.c_tensor,
                if (reverse)  1  else  0,
                batch_sizes.ptr, @intCast(batch_sizes.len),
                mode_,
                hidden_size,
                num_layers,
                if (has_biases)  1  else  0,
                if (bidirectional)  1  else  0,
                if (batch_first)  1  else  0,
                if (train)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }, Tensor { .c_tensor = c_tensors[3] }};
    }

    pub fn mkldnnRnnLayerBackward(
        self: *const Tensor, weight1: *const Tensor, weight2: *const Tensor, weight3: *const Tensor, weight4: *const Tensor, hx_: *const Tensor, cx_tmp: *const Tensor, output: *const Tensor, hy_: *const Tensor, cy_: *const Tensor, grad_output: ?*const Tensor, grad_hy: ?*const Tensor, grad_cy: ?*const Tensor, reverse: bool, mode_: i64, hidden_size: i64, num_layers: i64, has_biases: bool, train: bool, bidirectional: bool, batch_sizes: []i64, batch_first: bool, workspace: *const Tensor
    ) [7]Tensor {
        var c_tensors = [_]C_tensor{null} ** 7;
        __c.atg_mkldnn_rnn_layer_backward(@ptrCast(&c_tensors), self.c_tensor,
                weight1.c_tensor,
                weight2.c_tensor,
                weight3.c_tensor,
                weight4.c_tensor,
                hx_.c_tensor,
                cx_tmp.c_tensor,
                output.c_tensor,
                hy_.c_tensor,
                cy_.c_tensor,
                if (grad_output != null) grad_output.?.c_tensor else null,
                if (grad_hy != null) grad_hy.?.c_tensor else null,
                if (grad_cy != null) grad_cy.?.c_tensor else null,
                if (reverse)  1  else  0,
                mode_,
                hidden_size,
                num_layers,
                if (has_biases)  1  else  0,
                if (train)  1  else  0,
                if (bidirectional)  1  else  0,
                batch_sizes.ptr, @intCast(batch_sizes.len),
                if (batch_first)  1  else  0,
                workspace.c_tensor);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }, Tensor { .c_tensor = c_tensors[3] }, Tensor { .c_tensor = c_tensors[4] }, Tensor { .c_tensor = c_tensors[5] }, Tensor { .c_tensor = c_tensors[6] }};
    }

    pub fn mkldnnRnnLayerBackwardOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, out2: *const Tensor, out3: *const Tensor, out4: *const Tensor, out5: *const Tensor, out6: *const Tensor, weight1: *const Tensor, weight2: *const Tensor, weight3: *const Tensor, weight4: *const Tensor, hx_: *const Tensor, cx_tmp: *const Tensor, output: *const Tensor, hy_: *const Tensor, cy_: *const Tensor, grad_output: ?*const Tensor, grad_hy: ?*const Tensor, grad_cy: ?*const Tensor, reverse: bool, mode_: i64, hidden_size: i64, num_layers: i64, has_biases: bool, train: bool, bidirectional: bool, batch_sizes: []i64, batch_first: bool, workspace: *const Tensor
    ) [7]Tensor {
        var c_tensors = [_]C_tensor{null} ** 7;
        __c.atg_mkldnn_rnn_layer_backward_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                out2.c_tensor,
                out3.c_tensor,
                out4.c_tensor,
                out5.c_tensor,
                out6.c_tensor,
                self.c_tensor,
                weight1.c_tensor,
                weight2.c_tensor,
                weight3.c_tensor,
                weight4.c_tensor,
                hx_.c_tensor,
                cx_tmp.c_tensor,
                output.c_tensor,
                hy_.c_tensor,
                cy_.c_tensor,
                if (grad_output != null) grad_output.?.c_tensor else null,
                if (grad_hy != null) grad_hy.?.c_tensor else null,
                if (grad_cy != null) grad_cy.?.c_tensor else null,
                if (reverse)  1  else  0,
                mode_,
                hidden_size,
                num_layers,
                if (has_biases)  1  else  0,
                if (train)  1  else  0,
                if (bidirectional)  1  else  0,
                batch_sizes.ptr, @intCast(batch_sizes.len),
                if (batch_first)  1  else  0,
                workspace.c_tensor);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }, Tensor { .c_tensor = c_tensors[3] }, Tensor { .c_tensor = c_tensors[4] }, Tensor { .c_tensor = c_tensors[5] }, Tensor { .c_tensor = c_tensors[6] }};
    }

    pub fn mkldnnRnnLayerOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, out2: *const Tensor, out3: *const Tensor, weight0: *const Tensor, weight1: *const Tensor, weight2: *const Tensor, weight3: *const Tensor, hx_: *const Tensor, cx_: *const Tensor, reverse: bool, batch_sizes: []i64, mode_: i64, hidden_size: i64, num_layers: i64, has_biases: bool, bidirectional: bool, batch_first: bool, train: bool
    ) [4]Tensor {
        var c_tensors = [_]C_tensor{null} ** 4;
        __c.atg_mkldnn_rnn_layer_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                out2.c_tensor,
                out3.c_tensor,
                self.c_tensor,
                weight0.c_tensor,
                weight1.c_tensor,
                weight2.c_tensor,
                weight3.c_tensor,
                hx_.c_tensor,
                cx_.c_tensor,
                if (reverse)  1  else  0,
                batch_sizes.ptr, @intCast(batch_sizes.len),
                mode_,
                hidden_size,
                num_layers,
                if (has_biases)  1  else  0,
                if (bidirectional)  1  else  0,
                if (batch_first)  1  else  0,
                if (train)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }, Tensor { .c_tensor = c_tensors[3] }};
    }

    pub fn mm(
        self: *const Tensor, mat2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mm(@ptrCast(&c_tensors), self.c_tensor,
                mat2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mmOut(
        self: *const Tensor, out: *const Tensor, mat2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mm_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                mat2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mode(
        self: *const Tensor, dim_: i64, keepdim: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_mode(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn modeValues(
        self: *const Tensor, values_: *const Tensor, indices_: *const Tensor, dim_: i64, keepdim: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_mode_values(@ptrCast(&c_tensors), values_.c_tensor,
                indices_.c_tensor,
                self.c_tensor,
                dim_,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn moveaxis(
        self: *const Tensor, source: []i64, destination: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_moveaxis(@ptrCast(&c_tensors), self.c_tensor,
                source.ptr, @intCast(source.len),
                destination.ptr, @intCast(destination.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn moveaxisInt(
        self: *const Tensor, source: i64, destination: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_moveaxis_int(@ptrCast(&c_tensors), self.c_tensor,
                source,
                destination);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn movedim(
        self: *const Tensor, source: []i64, destination: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_movedim(@ptrCast(&c_tensors), self.c_tensor,
                source.ptr, @intCast(source.len),
                destination.ptr, @intCast(destination.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn movedimInt(
        self: *const Tensor, source: i64, destination: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_movedim_int(@ptrCast(&c_tensors), self.c_tensor,
                source,
                destination);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mseLoss(
        self: *const Tensor, target: *const Tensor, reduction: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mse_loss(@ptrCast(&c_tensors), self.c_tensor,
                target.c_tensor,
                reduction.to_int());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mseLossBackward(
        self: *const Tensor, grad_output: *const Tensor, target: *const Tensor, reduction: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mse_loss_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                target.c_tensor,
                reduction.to_int());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mseLossBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor, target: *const Tensor, reduction: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mse_loss_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                target.c_tensor,
                reduction.to_int());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mseLossOut(
        self: *const Tensor, out: *const Tensor, target: *const Tensor, reduction: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mse_loss_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                target.c_tensor,
                reduction.to_int());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn msort(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_msort(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn msortOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_msort_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mt(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mt(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mul(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mul(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mul_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mul_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mulOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mul_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mulScalar(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mul_scalar(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mulScalar_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mul_scalar_(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mulScalarOut(
        self: *const Tensor, out: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mul_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn multiMarginLossBackward(
        self: *const Tensor, grad_output: *const Tensor, target: *const Tensor, p: Scalar, margin: Scalar, weight: ?*const Tensor, reduction: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_multi_margin_loss_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                target.c_tensor,
                p.into().c_scalar,
                margin.into().c_scalar,
                if (weight != null) weight.?.c_tensor else null,
                reduction.to_int());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn multiMarginLossBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor, target: *const Tensor, p: Scalar, margin: Scalar, weight: ?*const Tensor, reduction: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_multi_margin_loss_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                target.c_tensor,
                p.into().c_scalar,
                margin.into().c_scalar,
                if (weight != null) weight.?.c_tensor else null,
                reduction.to_int());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn multilabelMarginLoss(
        self: *const Tensor, target: *const Tensor, reduction: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_multilabel_margin_loss(@ptrCast(&c_tensors), self.c_tensor,
                target.c_tensor,
                reduction.to_int());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn multilabelMarginLossBackward(
        self: *const Tensor, grad_output: *const Tensor, target: *const Tensor, reduction: i64, is_target: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_multilabel_margin_loss_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                target.c_tensor,
                reduction.to_int(),
                is_target.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn multilabelMarginLossBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor, target: *const Tensor, reduction: i64, is_target: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_multilabel_margin_loss_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                target.c_tensor,
                reduction.to_int(),
                is_target.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn multilabelMarginLossOut(
        self: *const Tensor, out: *const Tensor, target: *const Tensor, reduction: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_multilabel_margin_loss_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                target.c_tensor,
                reduction.to_int());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn multinomial(
        self: *const Tensor, num_samples: i64, replacement: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_multinomial(@ptrCast(&c_tensors), self.c_tensor,
                num_samples,
                if (replacement)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn multinomialOut(
        self: *const Tensor, out: *const Tensor, num_samples: i64, replacement: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_multinomial_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                num_samples,
                if (replacement)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn multiply(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_multiply(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn multiply_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_multiply_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn multiplyOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_multiply_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn multiplyScalar(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_multiply_scalar(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn multiplyScalar_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_multiply_scalar_(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mv(
        self: *const Tensor, vec: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mv(@ptrCast(&c_tensors), self.c_tensor,
                vec.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mvOut(
        self: *const Tensor, out: *const Tensor, vec: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mv_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                vec.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mvlgamma(
        self: *const Tensor, p: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mvlgamma(@ptrCast(&c_tensors), self.c_tensor,
                p);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mvlgamma_(
        self: *Tensor, p: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mvlgamma_(@ptrCast(&c_tensors), self.c_tensor,
                p);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn mvlgammaOut(
        self: *const Tensor, out: *const Tensor, p: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_mvlgamma_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                p);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nanToNum(
        self: *const Tensor, nan: ?f64, posinf: ?f64, neginf: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nan_to_num(@ptrCast(&c_tensors), self.c_tensor,
                nan orelse std.math.nan, (nan == null),
                posinf orelse std.math.nan, (posinf == null),
                neginf orelse std.math.nan, (neginf == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nanToNum_(
        self: *Tensor, nan: ?f64, posinf: ?f64, neginf: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nan_to_num_(@ptrCast(&c_tensors), self.c_tensor,
                nan orelse std.math.nan, (nan == null),
                posinf orelse std.math.nan, (posinf == null),
                neginf orelse std.math.nan, (neginf == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nanToNumOut(
        self: *const Tensor, out: *const Tensor, nan: ?f64, posinf: ?f64, neginf: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nan_to_num_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                nan orelse std.math.nan, (nan == null),
                posinf orelse std.math.nan, (posinf == null),
                neginf orelse std.math.nan, (neginf == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nanmean(
        self: *const Tensor, dim_: ?[]i64, keepdim: bool, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nanmean(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nanmeanOut(
        self: *const Tensor, out: *const Tensor, dim_: ?[]i64, keepdim: bool, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nanmean_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nanmedian(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nanmedian(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nanmedianDim(
        self: *const Tensor, dim_: i64, keepdim: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_nanmedian_dim(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn nanmedianDimValues(
        self: *const Tensor, values_: *const Tensor, indices_: *const Tensor, dim_: i64, keepdim: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_nanmedian_dim_values(@ptrCast(&c_tensors), values_.c_tensor,
                indices_.c_tensor,
                self.c_tensor,
                dim_,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn nanmedianOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nanmedian_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nanquantile(
        self: *const Tensor, q: *const Tensor, dim_: ?i64, keepdim: bool, interpolation: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nanquantile(@ptrCast(&c_tensors), self.c_tensor,
                q.c_tensor,
                dim_ orelse 0, (dim_ == null),
                if (keepdim)  1  else  0,
                interpolation.ptr, interpolation.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nanquantileOut(
        self: *const Tensor, out: *const Tensor, q: *const Tensor, dim_: ?i64, keepdim: bool, interpolation: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nanquantile_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                q.c_tensor,
                dim_ orelse 0, (dim_ == null),
                if (keepdim)  1  else  0,
                interpolation.ptr, interpolation.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nanquantileScalar(
        self: *const Tensor, q: f64, dim_: ?i64, keepdim: bool, interpolation: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nanquantile_scalar(@ptrCast(&c_tensors), self.c_tensor,
                q,
                dim_ orelse 0, (dim_ == null),
                if (keepdim)  1  else  0,
                interpolation.ptr, interpolation.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nanquantileScalarOut(
        self: *const Tensor, out: *const Tensor, q: f64, dim_: ?i64, keepdim: bool, interpolation: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nanquantile_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                q,
                dim_ orelse 0, (dim_ == null),
                if (keepdim)  1  else  0,
                interpolation.ptr, interpolation.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nansum(
        self: *const Tensor, dim_: ?[]i64, keepdim: bool, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nansum(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nansumOut(
        self: *const Tensor, out: *const Tensor, dim_: ?[]i64, keepdim: bool, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nansum_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn narrow(
        self: *const Tensor, dim_: i64, start: i64, length: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_narrow(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                start,
                length);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn narrowCopy(
        self: *const Tensor, dim_: i64, start: i64, length: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_narrow_copy(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                start,
                length);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn narrowCopyOut(
        self: *const Tensor, out: *const Tensor, dim_: i64, start: i64, length: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_narrow_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_,
                start,
                length);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn narrowTensor(
        self: *const Tensor, dim_: i64, start: *const Tensor, length: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_narrow_tensor(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                start.c_tensor,
                length);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nativeBatchNorm(
        self: *const Tensor, weight: ?*const Tensor, bias: ?*const Tensor, running_mean: ?*const Tensor, running_var: ?*const Tensor, training: bool, momentum: f64, eps: f64
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_native_batch_norm(@ptrCast(&c_tensors), self.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                if (bias != null) bias.?.c_tensor else null,
                if (running_mean != null) running_mean.?.c_tensor else null,
                if (running_var != null) running_var.?.c_tensor else null,
                if (training)  1  else  0,
                momentum,
                eps);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn nativeBatchNormOut(
        self: *const Tensor, out: *const Tensor, save_mean: *const Tensor, save_invstd: *const Tensor, weight: ?*const Tensor, bias: ?*const Tensor, running_mean: ?*const Tensor, running_var: ?*const Tensor, training: bool, momentum: f64, eps: f64
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_native_batch_norm_out(@ptrCast(&c_tensors), out.c_tensor,
                save_mean.c_tensor,
                save_invstd.c_tensor,
                self.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                if (bias != null) bias.?.c_tensor else null,
                if (running_mean != null) running_mean.?.c_tensor else null,
                if (running_var != null) running_var.?.c_tensor else null,
                if (training)  1  else  0,
                momentum,
                eps);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn nativeChannelShuffle(
        self: *const Tensor, groups: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_native_channel_shuffle(@ptrCast(&c_tensors), self.c_tensor,
                groups);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nativeDropout(
        self: *const Tensor, p: f64, train: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_native_dropout(@ptrCast(&c_tensors), self.c_tensor,
                p,
                if (train)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn nativeDropoutBackward(
        grad_output: *const Tensor, mask: *const Tensor, scale: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_native_dropout_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                mask.c_tensor,
                scale);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nativeDropoutBackwardOut(
        out: *const Tensor, grad_output: *const Tensor, mask: *const Tensor, scale: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_native_dropout_backward_out(@ptrCast(&c_tensors), out.c_tensor,
                grad_output.c_tensor,
                mask.c_tensor,
                scale);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nativeDropoutOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, p: f64, train: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_native_dropout_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                self.c_tensor,
                p,
                if (train)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn nativeGroupNorm(
        self: *const Tensor, weight: ?*const Tensor, bias: ?*const Tensor, n: i64, c: i64, hxw: i64, group: i64, eps: f64
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_native_group_norm(@ptrCast(&c_tensors), self.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                if (bias != null) bias.?.c_tensor else null,
                n,
                c,
                hxw,
                group,
                eps);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn nativeGroupNormOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, out2: *const Tensor, weight: ?*const Tensor, bias: ?*const Tensor, n: i64, c: i64, hxw: i64, group: i64, eps: f64
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_native_group_norm_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                out2.c_tensor,
                self.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                if (bias != null) bias.?.c_tensor else null,
                n,
                c,
                hxw,
                group,
                eps);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn nativeLayerNorm(
        self: *const Tensor, normalized_shape: []i64, weight: ?*const Tensor, bias: ?*const Tensor, eps: f64
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_native_layer_norm(@ptrCast(&c_tensors), self.c_tensor,
                normalized_shape.ptr, @intCast(normalized_shape.len),
                if (weight != null) weight.?.c_tensor else null,
                if (bias != null) bias.?.c_tensor else null,
                eps);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn nativeLayerNormOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, out2: *const Tensor, normalized_shape: []i64, weight: ?*const Tensor, bias: ?*const Tensor, eps: f64
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_native_layer_norm_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                out2.c_tensor,
                self.c_tensor,
                normalized_shape.ptr, @intCast(normalized_shape.len),
                if (weight != null) weight.?.c_tensor else null,
                if (bias != null) bias.?.c_tensor else null,
                eps);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn nativeNorm(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_native_norm(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nativeNormOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_native_norm_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nativeNormScalaroptDimDtype(
        self: *const Tensor, p: Scalar, dim_: []i64, keepdim: bool, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_native_norm_scalaropt_dim_dtype(@ptrCast(&c_tensors), self.c_tensor,
                p.into().c_scalar,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nativeNormScalaroptDimDtypeOut(
        self: *const Tensor, out: *const Tensor, p: Scalar, dim_: []i64, keepdim: bool, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_native_norm_scalaropt_dim_dtype_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                p.into().c_scalar,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn ne(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_ne(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn ne_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_ne_(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn neScalarOut(
        self: *const Tensor, out: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_ne_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn neTensor(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_ne_tensor(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn neTensor_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_ne_tensor_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn neTensorOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_ne_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn neg(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_neg(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn neg_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_neg_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn negOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_neg_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn negative(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_negative(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn negative2(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_negative_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn negativeOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_negative_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nestedToPaddedTensor(
        self: *const Tensor, padding: f64, output_size: ?[]i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nested_to_padded_tensor(@ptrCast(&c_tensors), self.c_tensor,
                padding,
                output_size.ptr, @intCast(output_size.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn newEmpty(
        self: *const Tensor, size_: []i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_new_empty(@ptrCast(&c_tensors), self.c_tensor,
                size_.ptr, @intCast(size_.len),
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn newEmptyOut(
        self: *const Tensor, out: *const Tensor, size_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_new_empty_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                size_.ptr, @intCast(size_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn newEmptyStrided(
        self: *const Tensor, size_: []i64, stride_: []i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_new_empty_strided(@ptrCast(&c_tensors), self.c_tensor,
                size_.ptr, @intCast(size_.len),
                stride_.ptr, @intCast(stride_.len),
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn newEmptyStridedOut(
        self: *const Tensor, out: *const Tensor, size_: []i64, stride_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_new_empty_strided_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                size_.ptr, @intCast(size_.len),
                stride_.ptr, @intCast(stride_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn newFull(
        self: *const Tensor, size_: []i64, fill_value: Scalar, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_new_full(@ptrCast(&c_tensors), self.c_tensor,
                size_.ptr, @intCast(size_.len),
                fill_value.into().c_scalar,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn newFullOut(
        self: *const Tensor, out: *const Tensor, size_: []i64, fill_value: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_new_full_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                size_.ptr, @intCast(size_.len),
                fill_value.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn newOnes(
        self: *const Tensor, size_: []i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_new_ones(@ptrCast(&c_tensors), self.c_tensor,
                size_.ptr, @intCast(size_.len),
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn newOnesOut(
        self: *const Tensor, out: *const Tensor, size_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_new_ones_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                size_.ptr, @intCast(size_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn newZeros(
        self: *const Tensor, size_: []i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_new_zeros(@ptrCast(&c_tensors), self.c_tensor,
                size_.ptr, @intCast(size_.len),
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn newZerosOut(
        self: *const Tensor, out: *const Tensor, size_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_new_zeros_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                size_.ptr, @intCast(size_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nextafter(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nextafter(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nextafter_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nextafter_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nextafterOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nextafter_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nllLoss(
        self: *const Tensor, target: *const Tensor, weight: ?*const Tensor, reduction: i64, ignore_index: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nll_loss(@ptrCast(&c_tensors), self.c_tensor,
                target.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                reduction.to_int(),
                ignore_index);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nllLoss2d(
        self: *const Tensor, target: *const Tensor, weight: ?*const Tensor, reduction: i64, ignore_index: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nll_loss2d(@ptrCast(&c_tensors), self.c_tensor,
                target.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                reduction.to_int(),
                ignore_index);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nllLoss2dBackward(
        self: *const Tensor, grad_output: *const Tensor, target: *const Tensor, weight: ?*const Tensor, reduction: i64, ignore_index: i64, total_weight: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nll_loss2d_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                target.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                reduction.to_int(),
                ignore_index,
                total_weight.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nllLoss2dBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor, target: *const Tensor, weight: ?*const Tensor, reduction: i64, ignore_index: i64, total_weight: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nll_loss2d_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                target.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                reduction.to_int(),
                ignore_index,
                total_weight.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nllLoss2dOut(
        self: *const Tensor, out: *const Tensor, target: *const Tensor, weight: ?*const Tensor, reduction: i64, ignore_index: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nll_loss2d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                target.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                reduction.to_int(),
                ignore_index);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nllLossBackward(
        self: *const Tensor, grad_output: *const Tensor, target: *const Tensor, weight: ?*const Tensor, reduction: i64, ignore_index: i64, total_weight: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nll_loss_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                target.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                reduction.to_int(),
                ignore_index,
                total_weight.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nllLossBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor, target: *const Tensor, weight: ?*const Tensor, reduction: i64, ignore_index: i64, total_weight: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nll_loss_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                target.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                reduction.to_int(),
                ignore_index,
                total_weight.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nllLossNd(
        self: *const Tensor, target: *const Tensor, weight: ?*const Tensor, reduction: i64, ignore_index: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nll_loss_nd(@ptrCast(&c_tensors), self.c_tensor,
                target.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                reduction.to_int(),
                ignore_index);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nllLossOut(
        self: *const Tensor, out: *const Tensor, target: *const Tensor, weight: ?*const Tensor, reduction: i64, ignore_index: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nll_loss_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                target.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                reduction.to_int(),
                ignore_index);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nonzero(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nonzero(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nonzeroNumpy(
        self: *const Tensor, 
    ) []Tensor {
        const c_tensors = __c.atg_nonzero_numpy(self.c_tensor);
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn nonzeroOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nonzero_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nonzeroStatic(
        self: *const Tensor, size_: i64, fill_value: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nonzero_static(@ptrCast(&c_tensors), self.c_tensor,
                size_,
                fill_value);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nonzeroStaticOut(
        self: *const Tensor, out: *const Tensor, size_: i64, fill_value: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nonzero_static_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                size_,
                fill_value);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn norm(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_norm(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn normDtypeOut(
        self: *const Tensor, out: *const Tensor, p: Scalar, dim_: []i64, keepdim: bool, dtype: Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_norm_dtype_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                p.into().c_scalar,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0,
                dtype.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn normExceptDim(
        v: *const Tensor, pow_: i64, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_norm_except_dim(@ptrCast(&c_tensors), v.c_tensor,
                pow_,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn normOut(
        self: *const Tensor, out: *const Tensor, p: Scalar, dim_: []i64, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_norm_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                p.into().c_scalar,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn normScalarOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_norm_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn normScalaroptDim(
        self: *const Tensor, p: Scalar, dim_: []i64, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_norm_scalaropt_dim(@ptrCast(&c_tensors), self.c_tensor,
                p.into().c_scalar,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn normScalaroptDimDtype(
        self: *const Tensor, p: Scalar, dim_: []i64, keepdim: bool, dtype: Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_norm_scalaropt_dim_dtype(@ptrCast(&c_tensors), self.c_tensor,
                p.into().c_scalar,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0,
                dtype.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn normScalaroptDtype(
        self: *const Tensor, p: Scalar, dtype: Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_norm_scalaropt_dtype(@ptrCast(&c_tensors), self.c_tensor,
                p.into().c_scalar,
                dtype.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn normScalaroptDtypeOut(
        self: *const Tensor, out: *const Tensor, p: Scalar, dtype: Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_norm_scalaropt_dtype_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                p.into().c_scalar,
                dtype.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn normal_(
        self: *Tensor, mean_: f64, std_dev: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_normal_(@ptrCast(&c_tensors), self.c_tensor,
                mean_,
                std_dev);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn normalFunctional(
        self: *const Tensor, mean_: f64, std_dev: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_normal_functional(@ptrCast(&c_tensors), self.c_tensor,
                mean_,
                std_dev);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn notEqual(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_not_equal(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn notEqual_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_not_equal_(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn notEqualScalarOut(
        self: *const Tensor, out: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_not_equal_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn notEqualTensor(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_not_equal_tensor(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn notEqualTensor_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_not_equal_tensor_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn notEqualTensorOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_not_equal_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nuclearNorm(
        self: *const Tensor, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nuclear_norm(@ptrCast(&c_tensors), self.c_tensor,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nuclearNormDim(
        self: *const Tensor, dim_: []i64, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nuclear_norm_dim(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nuclearNormDimOut(
        self: *const Tensor, out: *const Tensor, dim_: []i64, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nuclear_norm_dim_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn nuclearNormOut(
        self: *const Tensor, out: *const Tensor, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_nuclear_norm_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn numpyT(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_numpy_t(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn oneHot(
        self: *const Tensor, num_classes: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_one_hot(@ptrCast(&c_tensors), self.c_tensor,
                num_classes);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn ones(
        size_: []i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_ones(@ptrCast(&c_tensors), size_.ptr, @intCast(size_.len),
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn onesLike(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_ones_like(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn onesLikeOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_ones_like_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn onesOut(
        out: *const Tensor, size_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_ones_out(@ptrCast(&c_tensors), out.c_tensor,
                size_.ptr, @intCast(size_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn orgqr(
        self: *const Tensor, input2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_orgqr(@ptrCast(&c_tensors), self.c_tensor,
                input2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn orgqrOut(
        self: *const Tensor, out: *const Tensor, input2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_orgqr_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                input2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn ormqr(
        self: *const Tensor, input2: *const Tensor, input3: *const Tensor, left: bool, transpose_: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_ormqr(@ptrCast(&c_tensors), self.c_tensor,
                input2.c_tensor,
                input3.c_tensor,
                if (left)  1  else  0,
                if (transpose_)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn ormqrOut(
        self: *const Tensor, out: *const Tensor, input2: *const Tensor, input3: *const Tensor, left: bool, transpose_: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_ormqr_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                input2.c_tensor,
                input3.c_tensor,
                if (left)  1  else  0,
                if (transpose_)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn outer(
        self: *const Tensor, vec2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_outer(@ptrCast(&c_tensors), self.c_tensor,
                vec2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn outerOut(
        self: *const Tensor, out: *const Tensor, vec2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_outer_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                vec2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn outputNr(
        self: *const Tensor, 
    ) i64 {
        const return_ = __c.atg_output_nr(self.c_tensor);
       torch.readAndCleanError();
        return return_;
    }

    pub fn pad(
        self: *const Tensor, padding: []i64, mode_: []const u8, value: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_pad(@ptrCast(&c_tensors), self.c_tensor,
                padding.ptr, @intCast(padding.len),
                mode_.ptr, mode_.len,
                value orelse std.math.nan, (value == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn padSequence(
        sequences: []*const Tensor, batch_first: bool, padding_value: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_pad_sequence(@ptrCast(&c_tensors), ptrList(sequences).ptr, @intCast(sequences.len),
                if (batch_first)  1  else  0,
                padding_value);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn pairwiseDistance(
        x1: *const Tensor, x2: *const Tensor, p: f64, eps: f64, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_pairwise_distance(@ptrCast(&c_tensors), x1.c_tensor,
                x2.c_tensor,
                p,
                eps,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn pdist(
        self: *const Tensor, p: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_pdist(@ptrCast(&c_tensors), self.c_tensor,
                p);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn permute(
        self: *const Tensor, dims: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_permute(@ptrCast(&c_tensors), self.c_tensor,
                dims.ptr, @intCast(dims.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn permuteCopy(
        self: *const Tensor, dims: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_permute_copy(@ptrCast(&c_tensors), self.c_tensor,
                dims.ptr, @intCast(dims.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn permuteCopyOut(
        self: *const Tensor, out: *const Tensor, dims: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_permute_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dims.ptr, @intCast(dims.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn pinMemory(
        self: *const Tensor, device_: Device
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_pin_memory(@ptrCast(&c_tensors), self.c_tensor,
                device_.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn pinverse(
        self: *const Tensor, rcond: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_pinverse(@ptrCast(&c_tensors), self.c_tensor,
                rcond);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn pixelShuffle(
        self: *const Tensor, upscale_factor: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_pixel_shuffle(@ptrCast(&c_tensors), self.c_tensor,
                upscale_factor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn pixelShuffleOut(
        self: *const Tensor, out: *const Tensor, upscale_factor: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_pixel_shuffle_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                upscale_factor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn pixelUnshuffle(
        self: *const Tensor, downscale_factor: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_pixel_unshuffle(@ptrCast(&c_tensors), self.c_tensor,
                downscale_factor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn pixelUnshuffleOut(
        self: *const Tensor, out: *const Tensor, downscale_factor: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_pixel_unshuffle_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                downscale_factor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn poisson(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_poisson(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn poissonNllLoss(
        self: *const Tensor, target: *const Tensor, log_input: bool, full_: bool, eps: f64, reduction: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_poisson_nll_loss(@ptrCast(&c_tensors), self.c_tensor,
                target.c_tensor,
                if (log_input)  1  else  0,
                if (full_)  1  else  0,
                eps,
                reduction.to_int());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn poissonOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_poisson_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn polar(
        abs_: *const Tensor, angle_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_polar(@ptrCast(&c_tensors), abs_.c_tensor,
                angle_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn polarOut(
        out: *const Tensor, abs_: *const Tensor, angle_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_polar_out(@ptrCast(&c_tensors), out.c_tensor,
                abs_.c_tensor,
                angle_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn polygamma(
        self: *const Tensor, n: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_polygamma(@ptrCast(&c_tensors), n,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn polygamma_(
        self: *Tensor, n: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_polygamma_(@ptrCast(&c_tensors), self.c_tensor,
                n);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn polygammaOut(
        self: *const Tensor, out: *const Tensor, n: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_polygamma_out(@ptrCast(&c_tensors), out.c_tensor,
                n,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn positive(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_positive(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn pow(
        self: *const Tensor, exponent: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_pow(@ptrCast(&c_tensors), self.c_tensor,
                exponent.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn pow2(
        self: *Tensor, exponent: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_pow_(@ptrCast(&c_tensors), self.c_tensor,
                exponent.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn powScalar(
        self_scalar: Scalar, exponent: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_pow_scalar(@ptrCast(&c_tensors), self_scalar.into().c_scalar,
                exponent.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn powScalarOut(
        out: *const Tensor, self_scalar: Scalar, exponent: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_pow_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self_scalar.into().c_scalar,
                exponent.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn powTensor_(
        self: *Tensor, exponent: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_pow_tensor_(@ptrCast(&c_tensors), self.c_tensor,
                exponent.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn powTensorScalar(
        self: *const Tensor, exponent: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_pow_tensor_scalar(@ptrCast(&c_tensors), self.c_tensor,
                exponent.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn powTensorScalarOut(
        self: *const Tensor, out: *const Tensor, exponent: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_pow_tensor_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                exponent.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn powTensorTensorOut(
        self: *const Tensor, out: *const Tensor, exponent: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_pow_tensor_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                exponent.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn prelu(
        self: *const Tensor, weight: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_prelu(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn prod(
        self: *const Tensor, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_prod(@ptrCast(&c_tensors), self.c_tensor,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn prodDimInt(
        self: *const Tensor, dim_: i64, keepdim: bool, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_prod_dim_int(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                if (keepdim)  1  else  0,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn prodIntOut(
        self: *const Tensor, out: *const Tensor, dim_: i64, keepdim: bool, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_prod_int_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_,
                if (keepdim)  1  else  0,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn prodOut(
        self: *const Tensor, out: *const Tensor, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_prod_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn put(
        self: *const Tensor, index_: *const Tensor, source: *const Tensor, accumulate: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_put(@ptrCast(&c_tensors), self.c_tensor,
                index_.c_tensor,
                source.c_tensor,
                if (accumulate)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn put_(
        self: *Tensor, index_: *const Tensor, source: *const Tensor, accumulate: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_put_(@ptrCast(&c_tensors), self.c_tensor,
                index_.c_tensor,
                source.c_tensor,
                if (accumulate)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn putOut(
        self: *const Tensor, out: *const Tensor, index_: *const Tensor, source: *const Tensor, accumulate: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_put_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                index_.c_tensor,
                source.c_tensor,
                if (accumulate)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn qPerChannelAxis(
        self: *const Tensor, 
    ) i64 {
        const return_ = __c.atg_q_per_channel_axis(self.c_tensor);
       torch.readAndCleanError();
        return return_;
    }

    pub fn qPerChannelScales(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_q_per_channel_scales(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn qPerChannelScalesOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_q_per_channel_scales_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn qPerChannelZeroPoints(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_q_per_channel_zero_points(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn qPerChannelZeroPointsOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_q_per_channel_zero_points_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn qScale(
        self: *const Tensor, 
    ) f64 {
        const return_ = __c.atg_q_scale(self.c_tensor);
       torch.readAndCleanError();
        return return_;
    }

    pub fn qZeroPoint(
        self: *const Tensor, 
    ) i64 {
        const return_ = __c.atg_q_zero_point(self.c_tensor);
       torch.readAndCleanError();
        return return_;
    }

    pub fn qr(
        self: *const Tensor, some: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_qr(@ptrCast(&c_tensors), self.c_tensor,
                if (some)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn qrQ(
        self: *const Tensor, q: *const Tensor, r: *const Tensor, some: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_qr_q(@ptrCast(&c_tensors), q.c_tensor,
                r.c_tensor,
                self.c_tensor,
                if (some)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn quantile(
        self: *const Tensor, q: *const Tensor, dim_: ?i64, keepdim: bool, interpolation: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_quantile(@ptrCast(&c_tensors), self.c_tensor,
                q.c_tensor,
                dim_ orelse 0, (dim_ == null),
                if (keepdim)  1  else  0,
                interpolation.ptr, interpolation.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn quantileOut(
        self: *const Tensor, out: *const Tensor, q: *const Tensor, dim_: ?i64, keepdim: bool, interpolation: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_quantile_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                q.c_tensor,
                dim_ orelse 0, (dim_ == null),
                if (keepdim)  1  else  0,
                interpolation.ptr, interpolation.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn quantileScalar(
        self: *const Tensor, q: f64, dim_: ?i64, keepdim: bool, interpolation: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_quantile_scalar(@ptrCast(&c_tensors), self.c_tensor,
                q,
                dim_ orelse 0, (dim_ == null),
                if (keepdim)  1  else  0,
                interpolation.ptr, interpolation.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn quantileScalarOut(
        self: *const Tensor, out: *const Tensor, q: f64, dim_: ?i64, keepdim: bool, interpolation: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_quantile_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                q,
                dim_ orelse 0, (dim_ == null),
                if (keepdim)  1  else  0,
                interpolation.ptr, interpolation.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn quantizePerChannel(
        self: *const Tensor, scales: *const Tensor, zero_points: *const Tensor, axis: i64, dtype: Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_quantize_per_channel(@ptrCast(&c_tensors), self.c_tensor,
                scales.c_tensor,
                zero_points.c_tensor,
                axis,
                dtype.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn quantizePerChannelOut(
        self: *const Tensor, out: *const Tensor, scales: *const Tensor, zero_points: *const Tensor, axis: i64, dtype: Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_quantize_per_channel_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                scales.c_tensor,
                zero_points.c_tensor,
                axis,
                dtype.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn quantizePerTensor(
        self: *const Tensor, scale: f64, zero_point: i64, dtype: Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_quantize_per_tensor(@ptrCast(&c_tensors), self.c_tensor,
                scale,
                zero_point,
                dtype.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn quantizePerTensorDynamic(
        self: *const Tensor, dtype: Kind, reduce_range: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_quantize_per_tensor_dynamic(@ptrCast(&c_tensors), self.c_tensor,
                dtype.cInt(),
                if (reduce_range)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn quantizePerTensorDynamicOut(
        self: *const Tensor, out: *const Tensor, dtype: Kind, reduce_range: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_quantize_per_tensor_dynamic_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dtype.cInt(),
                if (reduce_range)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn quantizePerTensorOut(
        self: *const Tensor, out: *const Tensor, scale: f64, zero_point: i64, dtype: Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_quantize_per_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                scale,
                zero_point,
                dtype.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn quantizePerTensorTensorQparams(
        self: *const Tensor, scale: *const Tensor, zero_point: *const Tensor, dtype: Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_quantize_per_tensor_tensor_qparams(@ptrCast(&c_tensors), self.c_tensor,
                scale.c_tensor,
                zero_point.c_tensor,
                dtype.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn quantizePerTensorTensorQparamsOut(
        self: *const Tensor, out: *const Tensor, scale: *const Tensor, zero_point: *const Tensor, dtype: Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_quantize_per_tensor_tensor_qparams_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                scale.c_tensor,
                zero_point.c_tensor,
                dtype.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn quantizePerTensorTensors(
        tensors: []*const Tensor, scales: *const Tensor, zero_points: *const Tensor, dtype: Kind
    ) []Tensor {
        const c_tensors = __c.atg_quantize_per_tensor_tensors(ptrList(tensors).ptr, @intCast(tensors.len),
                scales.c_tensor,
                zero_points.c_tensor,
                dtype.cInt());
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn quantizePerTensorTensorsOut(
        out: []*const Tensor, tensors: []*const Tensor, scales: *const Tensor, zero_points: *const Tensor, dtype: Kind
    ) void {
        __c.atg_quantize_per_tensor_tensors_out(ptrList(out).ptr, @intCast(out.len),
                ptrList(tensors).ptr, @intCast(tensors.len),
                scales.c_tensor,
                zero_points.c_tensor,
                dtype.cInt());
        torch.readAndCleanError();
        return;
    }

    pub fn quantizedBatchNorm(
        self: *const Tensor, weight: ?*const Tensor, bias: ?*const Tensor, mean_: *const Tensor, var_: *const Tensor, eps: f64, output_scale: f64, output_zero_point: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_quantized_batch_norm(@ptrCast(&c_tensors), self.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                if (bias != null) bias.?.c_tensor else null,
                mean_.c_tensor,
                var_.c_tensor,
                eps,
                output_scale,
                output_zero_point);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn quantizedBatchNormOut(
        self: *const Tensor, out: *const Tensor, weight: ?*const Tensor, bias: ?*const Tensor, mean_: *const Tensor, var_: *const Tensor, eps: f64, output_scale: f64, output_zero_point: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_quantized_batch_norm_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                if (weight != null) weight.?.c_tensor else null,
                if (bias != null) bias.?.c_tensor else null,
                mean_.c_tensor,
                var_.c_tensor,
                eps,
                output_scale,
                output_zero_point);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn quantizedGruCell(
        self: *const Tensor, hx: *const Tensor, w_ih: *const Tensor, w_hh: *const Tensor, b_ih: *const Tensor, b_hh: *const Tensor, packed_ih: *const Tensor, packed_hh: *const Tensor, col_offsets_ih: *const Tensor, col_offsets_hh: *const Tensor, scale_ih: Scalar, scale_hh: Scalar, zero_point_ih: Scalar, zero_point_hh: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_quantized_gru_cell(@ptrCast(&c_tensors), self.c_tensor,
                hx.c_tensor,
                w_ih.c_tensor,
                w_hh.c_tensor,
                b_ih.c_tensor,
                b_hh.c_tensor,
                packed_ih.c_tensor,
                packed_hh.c_tensor,
                col_offsets_ih.c_tensor,
                col_offsets_hh.c_tensor,
                scale_ih.into().c_scalar,
                scale_hh.into().c_scalar,
                zero_point_ih.into().c_scalar,
                zero_point_hh.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn quantizedLstmCell(
        self: *const Tensor, hx: []*const Tensor, w_ih: *const Tensor, w_hh: *const Tensor, b_ih: *const Tensor, b_hh: *const Tensor, packed_ih: *const Tensor, packed_hh: *const Tensor, col_offsets_ih: *const Tensor, col_offsets_hh: *const Tensor, scale_ih: Scalar, scale_hh: Scalar, zero_point_ih: Scalar, zero_point_hh: Scalar
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_quantized_lstm_cell(@ptrCast(&c_tensors), self.c_tensor,
                ptrList(hx).ptr, @intCast(hx.len),
                w_ih.c_tensor,
                w_hh.c_tensor,
                b_ih.c_tensor,
                b_hh.c_tensor,
                packed_ih.c_tensor,
                packed_hh.c_tensor,
                col_offsets_ih.c_tensor,
                col_offsets_hh.c_tensor,
                scale_ih.into().c_scalar,
                scale_hh.into().c_scalar,
                zero_point_ih.into().c_scalar,
                zero_point_hh.into().c_scalar);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn quantizedMaxPool1d(
        self: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, dilation: []i64, ceil_mode: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_quantized_max_pool1d(@ptrCast(&c_tensors), self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (ceil_mode)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn quantizedMaxPool1dOut(
        self: *const Tensor, out: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, dilation: []i64, ceil_mode: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_quantized_max_pool1d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (ceil_mode)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn quantizedMaxPool2d(
        self: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, dilation: []i64, ceil_mode: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_quantized_max_pool2d(@ptrCast(&c_tensors), self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (ceil_mode)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn quantizedMaxPool2dOut(
        self: *const Tensor, out: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, dilation: []i64, ceil_mode: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_quantized_max_pool2d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (ceil_mode)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn quantizedMaxPool3d(
        self: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, dilation: []i64, ceil_mode: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_quantized_max_pool3d(@ptrCast(&c_tensors), self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (ceil_mode)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn quantizedMaxPool3dOut(
        self: *const Tensor, out: *const Tensor, kernel_size: []i64, stride_: []i64, padding: []i64, dilation: []i64, ceil_mode: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_quantized_max_pool3d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len),
                if (ceil_mode)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn quantizedRnnReluCell(
        self: *const Tensor, hx: *const Tensor, w_ih: *const Tensor, w_hh: *const Tensor, b_ih: *const Tensor, b_hh: *const Tensor, packed_ih: *const Tensor, packed_hh: *const Tensor, col_offsets_ih: *const Tensor, col_offsets_hh: *const Tensor, scale_ih: Scalar, scale_hh: Scalar, zero_point_ih: Scalar, zero_point_hh: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_quantized_rnn_relu_cell(@ptrCast(&c_tensors), self.c_tensor,
                hx.c_tensor,
                w_ih.c_tensor,
                w_hh.c_tensor,
                b_ih.c_tensor,
                b_hh.c_tensor,
                packed_ih.c_tensor,
                packed_hh.c_tensor,
                col_offsets_ih.c_tensor,
                col_offsets_hh.c_tensor,
                scale_ih.into().c_scalar,
                scale_hh.into().c_scalar,
                zero_point_ih.into().c_scalar,
                zero_point_hh.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn quantizedRnnTanhCell(
        self: *const Tensor, hx: *const Tensor, w_ih: *const Tensor, w_hh: *const Tensor, b_ih: *const Tensor, b_hh: *const Tensor, packed_ih: *const Tensor, packed_hh: *const Tensor, col_offsets_ih: *const Tensor, col_offsets_hh: *const Tensor, scale_ih: Scalar, scale_hh: Scalar, zero_point_ih: Scalar, zero_point_hh: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_quantized_rnn_tanh_cell(@ptrCast(&c_tensors), self.c_tensor,
                hx.c_tensor,
                w_ih.c_tensor,
                w_hh.c_tensor,
                b_ih.c_tensor,
                b_hh.c_tensor,
                packed_ih.c_tensor,
                packed_hh.c_tensor,
                col_offsets_ih.c_tensor,
                col_offsets_hh.c_tensor,
                scale_ih.into().c_scalar,
                scale_hh.into().c_scalar,
                zero_point_ih.into().c_scalar,
                zero_point_hh.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn rad2deg(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_rad2deg(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn rad2deg_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_rad2deg_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn rad2degOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_rad2deg_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn rand(
        size_: []i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_rand(@ptrCast(&c_tensors), size_.ptr, @intCast(size_.len),
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn randLike(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_rand_like(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn randLikeOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_rand_like_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn randOut(
        out: *const Tensor, size_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_rand_out(@ptrCast(&c_tensors), out.c_tensor,
                size_.ptr, @intCast(size_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn randint(
        high: i64, size_: []i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_randint(@ptrCast(&c_tensors), high,
                size_.ptr, @intCast(size_.len),
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn randintLike(
        self: *const Tensor, high: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_randint_like(@ptrCast(&c_tensors), self.c_tensor,
                high);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn randintLikeLowDtype(
        self: *const Tensor, low: i64, high: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_randint_like_low_dtype(@ptrCast(&c_tensors), self.c_tensor,
                low,
                high);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn randintLikeLowDtypeOut(
        self: *const Tensor, out: *const Tensor, low: i64, high: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_randint_like_low_dtype_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                low,
                high);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn randintLikeOut(
        self: *const Tensor, out: *const Tensor, high: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_randint_like_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                high);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn randintLow(
        low: i64, high: i64, size_: []i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_randint_low(@ptrCast(&c_tensors), low,
                high,
                size_.ptr, @intCast(size_.len),
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn randintLowOut(
        out: *const Tensor, low: i64, high: i64, size_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_randint_low_out(@ptrCast(&c_tensors), out.c_tensor,
                low,
                high,
                size_.ptr, @intCast(size_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn randintOut(
        out: *const Tensor, high: i64, size_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_randint_out(@ptrCast(&c_tensors), out.c_tensor,
                high,
                size_.ptr, @intCast(size_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn randn(
        size_: []i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_randn(@ptrCast(&c_tensors), size_.ptr, @intCast(size_.len),
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn randnLike(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_randn_like(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn randnLikeOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_randn_like_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn randnOut(
        out: *const Tensor, size_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_randn_out(@ptrCast(&c_tensors), out.c_tensor,
                size_.ptr, @intCast(size_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn random(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_random(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn random_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_random_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn randomFrom(
        self: *const Tensor, from: i64, to_: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_random_from(@ptrCast(&c_tensors), self.c_tensor,
                from,
                to_ orelse 0, (to_ == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn randomFrom_(
        self: *Tensor, from: i64, to_: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_random_from_(@ptrCast(&c_tensors), self.c_tensor,
                from,
                to_ orelse 0, (to_ == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn randomFromOut(
        self: *const Tensor, out: *const Tensor, from: i64, to_: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_random_from_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                from,
                to_ orelse 0, (to_ == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn randomOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_random_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn randomTo(
        self: *const Tensor, to_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_random_to(@ptrCast(&c_tensors), self.c_tensor,
                to_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn randomTo_(
        self: *Tensor, to_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_random_to_(@ptrCast(&c_tensors), self.c_tensor,
                to_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn randomToOut(
        self: *const Tensor, out: *const Tensor, to_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_random_to_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                to_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn randperm(
        n: i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_randperm(@ptrCast(&c_tensors), n,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn randpermOut(
        out: *const Tensor, n: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_randperm_out(@ptrCast(&c_tensors), out.c_tensor,
                n);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn range(
        start: Scalar, end: Scalar, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_range(@ptrCast(&c_tensors), start.into().c_scalar,
                end.into().c_scalar,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn rangeOut(
        out: *const Tensor, start: Scalar, end: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_range_out(@ptrCast(&c_tensors), out.c_tensor,
                start.into().c_scalar,
                end.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn rangeOut_(
        out: *const Tensor, start: Scalar, end: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_range_out_(@ptrCast(&c_tensors), out.c_tensor,
                start.into().c_scalar,
                end.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn rangeStep(
        start: Scalar, end: Scalar, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_range_step(@ptrCast(&c_tensors), start.into().c_scalar,
                end.into().c_scalar,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn ravel(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_ravel(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn real(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_real(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn reciprocal(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_reciprocal(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn reciprocal_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_reciprocal_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn reciprocalOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_reciprocal_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn reflectionPad1d(
        self: *const Tensor, padding: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_reflection_pad1d(@ptrCast(&c_tensors), self.c_tensor,
                padding.ptr, @intCast(padding.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn reflectionPad1dBackward(
        self: *const Tensor, grad_output: *const Tensor, padding: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_reflection_pad1d_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                padding.ptr, @intCast(padding.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn reflectionPad1dBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor, padding: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_reflection_pad1d_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                padding.ptr, @intCast(padding.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn reflectionPad1dOut(
        self: *const Tensor, out: *const Tensor, padding: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_reflection_pad1d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                padding.ptr, @intCast(padding.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn reflectionPad2d(
        self: *const Tensor, padding: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_reflection_pad2d(@ptrCast(&c_tensors), self.c_tensor,
                padding.ptr, @intCast(padding.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn reflectionPad2dBackward(
        self: *const Tensor, grad_output: *const Tensor, padding: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_reflection_pad2d_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                padding.ptr, @intCast(padding.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn reflectionPad2dBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor, padding: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_reflection_pad2d_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                padding.ptr, @intCast(padding.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn reflectionPad2dOut(
        self: *const Tensor, out: *const Tensor, padding: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_reflection_pad2d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                padding.ptr, @intCast(padding.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn reflectionPad3d(
        self: *const Tensor, padding: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_reflection_pad3d(@ptrCast(&c_tensors), self.c_tensor,
                padding.ptr, @intCast(padding.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn reflectionPad3dBackward(
        self: *const Tensor, grad_output: *const Tensor, padding: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_reflection_pad3d_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                padding.ptr, @intCast(padding.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn reflectionPad3dBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor, padding: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_reflection_pad3d_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                padding.ptr, @intCast(padding.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn reflectionPad3dOut(
        self: *const Tensor, out: *const Tensor, padding: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_reflection_pad3d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                padding.ptr, @intCast(padding.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn relu(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_relu(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn relu6(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_relu6(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn relu6_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_relu6_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn relu_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_relu_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn reluOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_relu_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn remainder(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_remainder(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn remainder_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_remainder_(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn remainderScalarOut(
        self: *const Tensor, out: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_remainder_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn remainderScalarTensor(
        self_scalar: Scalar, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_remainder_scalar_tensor(@ptrCast(&c_tensors), self_scalar.into().c_scalar,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn remainderScalarTensorOut(
        out: *const Tensor, self_scalar: Scalar, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_remainder_scalar_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self_scalar.into().c_scalar,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn remainderTensor(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_remainder_tensor(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn remainderTensor_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_remainder_tensor_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn remainderTensorOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_remainder_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn renorm(
        self: *const Tensor, p: Scalar, dim_: i64, maxnorm: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_renorm(@ptrCast(&c_tensors), self.c_tensor,
                p.into().c_scalar,
                dim_,
                maxnorm.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn renorm_(
        self: *Tensor, p: Scalar, dim_: i64, maxnorm: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_renorm_(@ptrCast(&c_tensors), self.c_tensor,
                p.into().c_scalar,
                dim_,
                maxnorm.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn renormOut(
        self: *const Tensor, out: *const Tensor, p: Scalar, dim_: i64, maxnorm: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_renorm_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                p.into().c_scalar,
                dim_,
                maxnorm.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn repeat(
        self: *const Tensor, repeats: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_repeat(@ptrCast(&c_tensors), self.c_tensor,
                repeats.ptr, @intCast(repeats.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn repeatInterleave(
        repeats: *const Tensor, output_size: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_repeat_interleave(@ptrCast(&c_tensors), repeats.c_tensor,
                output_size orelse 0, (output_size == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn repeatInterleaveSelfInt(
        self: *const Tensor, repeats: i64, dim_: ?i64, output_size: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_repeat_interleave_self_int(@ptrCast(&c_tensors), self.c_tensor,
                repeats,
                dim_ orelse 0, (dim_ == null),
                output_size orelse 0, (output_size == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn repeatInterleaveSelfTensor(
        self: *const Tensor, repeats: *const Tensor, dim_: ?i64, output_size: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_repeat_interleave_self_tensor(@ptrCast(&c_tensors), self.c_tensor,
                repeats.c_tensor,
                dim_ orelse 0, (dim_ == null),
                output_size orelse 0, (output_size == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn repeatInterleaveTensorOut(
        out: *const Tensor, repeats: *const Tensor, output_size: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_repeat_interleave_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                repeats.c_tensor,
                output_size orelse 0, (output_size == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn repeatOut(
        self: *const Tensor, out: *const Tensor, repeats: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_repeat_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                repeats.ptr, @intCast(repeats.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn replicationPad1d(
        self: *const Tensor, padding: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_replication_pad1d(@ptrCast(&c_tensors), self.c_tensor,
                padding.ptr, @intCast(padding.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn replicationPad1dBackward(
        self: *const Tensor, grad_output: *const Tensor, padding: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_replication_pad1d_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                padding.ptr, @intCast(padding.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn replicationPad1dBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor, padding: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_replication_pad1d_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                padding.ptr, @intCast(padding.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn replicationPad1dOut(
        self: *const Tensor, out: *const Tensor, padding: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_replication_pad1d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                padding.ptr, @intCast(padding.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn replicationPad2d(
        self: *const Tensor, padding: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_replication_pad2d(@ptrCast(&c_tensors), self.c_tensor,
                padding.ptr, @intCast(padding.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn replicationPad2dBackward(
        self: *const Tensor, grad_output: *const Tensor, padding: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_replication_pad2d_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                padding.ptr, @intCast(padding.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn replicationPad2dBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor, padding: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_replication_pad2d_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                padding.ptr, @intCast(padding.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn replicationPad2dOut(
        self: *const Tensor, out: *const Tensor, padding: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_replication_pad2d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                padding.ptr, @intCast(padding.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn replicationPad3d(
        self: *const Tensor, padding: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_replication_pad3d(@ptrCast(&c_tensors), self.c_tensor,
                padding.ptr, @intCast(padding.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn replicationPad3dBackward(
        self: *const Tensor, grad_output: *const Tensor, padding: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_replication_pad3d_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                padding.ptr, @intCast(padding.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn replicationPad3dBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor, padding: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_replication_pad3d_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                padding.ptr, @intCast(padding.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn replicationPad3dOut(
        self: *const Tensor, out: *const Tensor, padding: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_replication_pad3d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                padding.ptr, @intCast(padding.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn requiresGrad_(
        self: *Tensor, requires_grad: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_requires_grad_(@ptrCast(&c_tensors), self.c_tensor,
                if (requires_grad)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn reshape(
        self: *const Tensor, shape: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_reshape(@ptrCast(&c_tensors), self.c_tensor,
                shape.ptr, @intCast(shape.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn reshapeAs(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_reshape_as(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn resize(
        self: *const Tensor, size_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_resize(@ptrCast(&c_tensors), self.c_tensor,
                size_.ptr, @intCast(size_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn resize_(
        self: *Tensor, size_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_resize_(@ptrCast(&c_tensors), self.c_tensor,
                size_.ptr, @intCast(size_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn resizeAs(
        self: *const Tensor, the_template: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_resize_as(@ptrCast(&c_tensors), self.c_tensor,
                the_template.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn resizeAs_(
        self: *Tensor, the_template: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_resize_as_(@ptrCast(&c_tensors), self.c_tensor,
                the_template.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn resizeAsOut(
        self: *const Tensor, out: *const Tensor, the_template: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_resize_as_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                the_template.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn resizeAsSparse(
        self: *const Tensor, the_template: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_resize_as_sparse(@ptrCast(&c_tensors), self.c_tensor,
                the_template.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn resizeAsSparse_(
        self: *Tensor, the_template: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_resize_as_sparse_(@ptrCast(&c_tensors), self.c_tensor,
                the_template.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn resizeAsSparseOut(
        self: *const Tensor, out: *const Tensor, the_template: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_resize_as_sparse_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                the_template.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn resizeOut(
        self: *const Tensor, out: *const Tensor, size_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_resize_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                size_.ptr, @intCast(size_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn resolveConj(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_resolve_conj(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn resolveNeg(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_resolve_neg(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn retainsGrad(
        self: *const Tensor, 
    ) bool {
        const return_ = __c.atg_retains_grad(self.c_tensor);
       torch.readAndCleanError();
        return return_ != 0;
    }

    pub fn rnnRelu(
        self: *const Tensor, hx: *const Tensor, params: []*const Tensor, has_biases: bool, num_layers: i64, dropout_: f64, train: bool, bidirectional: bool, batch_first: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_rnn_relu(@ptrCast(&c_tensors), self.c_tensor,
                hx.c_tensor,
                ptrList(params).ptr, @intCast(params.len),
                if (has_biases)  1  else  0,
                num_layers,
                dropout_,
                if (train)  1  else  0,
                if (bidirectional)  1  else  0,
                if (batch_first)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn rnnReluCell(
        self: *const Tensor, hx: *const Tensor, w_ih: *const Tensor, w_hh: *const Tensor, b_ih: ?*const Tensor, b_hh: ?*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_rnn_relu_cell(@ptrCast(&c_tensors), self.c_tensor,
                hx.c_tensor,
                w_ih.c_tensor,
                w_hh.c_tensor,
                if (b_ih != null) b_ih.?.c_tensor else null,
                if (b_hh != null) b_hh.?.c_tensor else null);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn rnnReluData(
        data_: *const Tensor, batch_sizes: *const Tensor, hx: *const Tensor, params: []*const Tensor, has_biases: bool, num_layers: i64, dropout_: f64, train: bool, bidirectional: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_rnn_relu_data(@ptrCast(&c_tensors), data_.c_tensor,
                batch_sizes.c_tensor,
                hx.c_tensor,
                ptrList(params).ptr, @intCast(params.len),
                if (has_biases)  1  else  0,
                num_layers,
                dropout_,
                if (train)  1  else  0,
                if (bidirectional)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn rnnTanh(
        self: *const Tensor, hx: *const Tensor, params: []*const Tensor, has_biases: bool, num_layers: i64, dropout_: f64, train: bool, bidirectional: bool, batch_first: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_rnn_tanh(@ptrCast(&c_tensors), self.c_tensor,
                hx.c_tensor,
                ptrList(params).ptr, @intCast(params.len),
                if (has_biases)  1  else  0,
                num_layers,
                dropout_,
                if (train)  1  else  0,
                if (bidirectional)  1  else  0,
                if (batch_first)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn rnnTanhCell(
        self: *const Tensor, hx: *const Tensor, w_ih: *const Tensor, w_hh: *const Tensor, b_ih: ?*const Tensor, b_hh: ?*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_rnn_tanh_cell(@ptrCast(&c_tensors), self.c_tensor,
                hx.c_tensor,
                w_ih.c_tensor,
                w_hh.c_tensor,
                if (b_ih != null) b_ih.?.c_tensor else null,
                if (b_hh != null) b_hh.?.c_tensor else null);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn rnnTanhData(
        data_: *const Tensor, batch_sizes: *const Tensor, hx: *const Tensor, params: []*const Tensor, has_biases: bool, num_layers: i64, dropout_: f64, train: bool, bidirectional: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_rnn_tanh_data(@ptrCast(&c_tensors), data_.c_tensor,
                batch_sizes.c_tensor,
                hx.c_tensor,
                ptrList(params).ptr, @intCast(params.len),
                if (has_biases)  1  else  0,
                num_layers,
                dropout_,
                if (train)  1  else  0,
                if (bidirectional)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn roll(
        self: *const Tensor, shifts: []i64, dims: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_roll(@ptrCast(&c_tensors), self.c_tensor,
                shifts.ptr, @intCast(shifts.len),
                dims.ptr, @intCast(dims.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn rollOut(
        self: *const Tensor, out: *const Tensor, shifts: []i64, dims: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_roll_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                shifts.ptr, @intCast(shifts.len),
                dims.ptr, @intCast(dims.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn rot90(
        self: *const Tensor, k: i64, dims: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_rot90(@ptrCast(&c_tensors), self.c_tensor,
                k,
                dims.ptr, @intCast(dims.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn rot90Out(
        self: *const Tensor, out: *const Tensor, k: i64, dims: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_rot90_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                k,
                dims.ptr, @intCast(dims.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn round(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_round(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn round_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_round_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn roundDecimals(
        self: *const Tensor, decimals: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_round_decimals(@ptrCast(&c_tensors), self.c_tensor,
                decimals);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn roundDecimals_(
        self: *Tensor, decimals: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_round_decimals_(@ptrCast(&c_tensors), self.c_tensor,
                decimals);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn roundDecimalsOut(
        self: *const Tensor, out: *const Tensor, decimals: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_round_decimals_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                decimals);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn roundOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_round_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn rowIndices(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_row_indices(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn rowIndicesCopy(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_row_indices_copy(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn rowIndicesCopyOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_row_indices_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn rowStack(
        tensors: []*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_row_stack(@ptrCast(&c_tensors), ptrList(tensors).ptr, @intCast(tensors.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn rowStackOut(
        out: *const Tensor, tensors: []*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_row_stack_out(@ptrCast(&c_tensors), out.c_tensor,
                ptrList(tensors).ptr, @intCast(tensors.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn rrelu(
        self: *const Tensor, training: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_rrelu(@ptrCast(&c_tensors), self.c_tensor,
                if (training)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn rrelu_(
        self: *Tensor, training: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_rrelu_(@ptrCast(&c_tensors), self.c_tensor,
                if (training)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn rreluWithNoise(
        self: *const Tensor, noise: *const Tensor, training: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_rrelu_with_noise(@ptrCast(&c_tensors), self.c_tensor,
                noise.c_tensor,
                if (training)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn rreluWithNoise_(
        self: *Tensor, noise: *const Tensor, training: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_rrelu_with_noise_(@ptrCast(&c_tensors), self.c_tensor,
                noise.c_tensor,
                if (training)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn rreluWithNoiseBackward(
        self: *const Tensor, grad_output: *const Tensor, noise: *const Tensor, lower: Scalar, upper: Scalar, training: bool, self_is_result: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_rrelu_with_noise_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                noise.c_tensor,
                lower.into().c_scalar,
                upper.into().c_scalar,
                if (training)  1  else  0,
                if (self_is_result)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn rreluWithNoiseBackwardOut(
        self: *const Tensor, out: *const Tensor, grad_output: *const Tensor, noise: *const Tensor, lower: Scalar, upper: Scalar, training: bool, self_is_result: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_rrelu_with_noise_backward_out(@ptrCast(&c_tensors), out.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                noise.c_tensor,
                lower.into().c_scalar,
                upper.into().c_scalar,
                if (training)  1  else  0,
                if (self_is_result)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn rreluWithNoiseOut(
        self: *const Tensor, out: *const Tensor, noise: *const Tensor, training: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_rrelu_with_noise_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                noise.c_tensor,
                if (training)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn rsqrt(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_rsqrt(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn rsqrt_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_rsqrt_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn rsqrtOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_rsqrt_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn rsub(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_rsub(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn rsubScalar(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_rsub_scalar(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn rsubScalarOut(
        self: *const Tensor, out: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_rsub_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn rsubTensorOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_rsub_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn scalarTensor(
        s: Scalar, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_scalar_tensor(@ptrCast(&c_tensors), s.into().c_scalar,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn scalarTensorOut(
        out: *const Tensor, s: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_scalar_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                s.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn scaledDotProductAttention(
        query: *const Tensor, key: *const Tensor, value: *const Tensor, attn_mask: ?*const Tensor, dropout_p: f64, is_causal: bool, scale: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_scaled_dot_product_attention(@ptrCast(&c_tensors), query.c_tensor,
                key.c_tensor,
                value.c_tensor,
                if (attn_mask != null) attn_mask.?.c_tensor else null,
                dropout_p,
                if (is_causal)  1  else  0,
                scale orelse std.math.nan, (scale == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn scatter(
        self: *const Tensor, dim_: i64, index_: *const Tensor, src: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_scatter(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                index_.c_tensor,
                src.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn scatter_(
        self: *Tensor, dim_: i64, index_: *const Tensor, src: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_scatter_(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                index_.c_tensor,
                src.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn scatterAdd(
        self: *const Tensor, dim_: i64, index_: *const Tensor, src: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_scatter_add(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                index_.c_tensor,
                src.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn scatterAdd_(
        self: *Tensor, dim_: i64, index_: *const Tensor, src: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_scatter_add_(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                index_.c_tensor,
                src.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn scatterAddOut(
        self: *const Tensor, out: *const Tensor, dim_: i64, index_: *const Tensor, src: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_scatter_add_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_,
                index_.c_tensor,
                src.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn scatterReduce(
        self: *const Tensor, dim_: i64, index_: *const Tensor, src: *const Tensor, reduce: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_scatter_reduce(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                index_.c_tensor,
                src.c_tensor,
                reduce.ptr, reduce.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn scatterReduce_(
        self: *Tensor, dim_: i64, index_: *const Tensor, src: *const Tensor, reduce: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_scatter_reduce_(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                index_.c_tensor,
                src.c_tensor,
                reduce.ptr, reduce.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn scatterReduceOut(
        self: *const Tensor, out: *const Tensor, dim_: i64, index_: *const Tensor, src: *const Tensor, reduce: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_scatter_reduce_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_,
                index_.c_tensor,
                src.c_tensor,
                reduce.ptr, reduce.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn scatterSrcOut(
        self: *const Tensor, out: *const Tensor, dim_: i64, index_: *const Tensor, src: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_scatter_src_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_,
                index_.c_tensor,
                src.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn scatterValue(
        self: *const Tensor, dim_: i64, index_: *const Tensor, value: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_scatter_value(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                index_.c_tensor,
                value.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn scatterValue_(
        self: *Tensor, dim_: i64, index_: *const Tensor, value: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_scatter_value_(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                index_.c_tensor,
                value.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn scatterValueOut(
        self: *const Tensor, out: *const Tensor, dim_: i64, index_: *const Tensor, value: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_scatter_value_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_,
                index_.c_tensor,
                value.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn scatterValueReduce(
        self: *const Tensor, dim_: i64, index_: *const Tensor, value: Scalar, reduce: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_scatter_value_reduce(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                index_.c_tensor,
                value.into().c_scalar,
                reduce.ptr, reduce.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn scatterValueReduce_(
        self: *Tensor, dim_: i64, index_: *const Tensor, value: Scalar, reduce: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_scatter_value_reduce_(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                index_.c_tensor,
                value.into().c_scalar,
                reduce.ptr, reduce.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn scatterValueReduceOut(
        self: *const Tensor, out: *const Tensor, dim_: i64, index_: *const Tensor, value: Scalar, reduce: []const u8
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_scatter_value_reduce_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_,
                index_.c_tensor,
                value.into().c_scalar,
                reduce.ptr, reduce.len);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn searchsorted(
        self: *const Tensor, sorted_sequence: *const Tensor, out_int32: bool, right: bool, side: []const u8, sorter: ?*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_searchsorted(@ptrCast(&c_tensors), sorted_sequence.c_tensor,
                self.c_tensor,
                if (out_int32)  1  else  0,
                if (right)  1  else  0,
                side.ptr, side.len,
                if (sorter != null) sorter.?.c_tensor else null);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn searchsortedScalar(
        sorted_sequence: *const Tensor, self_scalar: Scalar, out_int32: bool, right: bool, side: []const u8, sorter: ?*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_searchsorted_scalar(@ptrCast(&c_tensors), sorted_sequence.c_tensor,
                self_scalar.into().c_scalar,
                if (out_int32)  1  else  0,
                if (right)  1  else  0,
                side.ptr, side.len,
                if (sorter != null) sorter.?.c_tensor else null);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn searchsortedScalarOut(
        out: *const Tensor, sorted_sequence: *const Tensor, self_scalar: Scalar, out_int32: bool, right: bool, side: []const u8, sorter: ?*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_searchsorted_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                sorted_sequence.c_tensor,
                self_scalar.into().c_scalar,
                if (out_int32)  1  else  0,
                if (right)  1  else  0,
                side.ptr, side.len,
                if (sorter != null) sorter.?.c_tensor else null);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn searchsortedTensorOut(
        self: *const Tensor, out: *const Tensor, sorted_sequence: *const Tensor, out_int32: bool, right: bool, side: []const u8, sorter: ?*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_searchsorted_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                sorted_sequence.c_tensor,
                self.c_tensor,
                if (out_int32)  1  else  0,
                if (right)  1  else  0,
                side.ptr, side.len,
                if (sorter != null) sorter.?.c_tensor else null);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn segmentReduce(
        data_: *const Tensor, reduce: []const u8, lengths: ?*const Tensor, indices_: ?*const Tensor, offsets: ?*const Tensor, axis: i64, unsafe: bool, initial: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_segment_reduce(@ptrCast(&c_tensors), data_.c_tensor,
                reduce.ptr, reduce.len,
                if (lengths != null) lengths.?.c_tensor else null,
                if (indices_ != null) indices_.?.c_tensor else null,
                if (offsets != null) offsets.?.c_tensor else null,
                axis,
                if (unsafe)  1  else  0,
                initial.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn segmentReduceOut(
        out: *const Tensor, data_: *const Tensor, reduce: []const u8, lengths: ?*const Tensor, indices_: ?*const Tensor, offsets: ?*const Tensor, axis: i64, unsafe: bool, initial: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_segment_reduce_out(@ptrCast(&c_tensors), out.c_tensor,
                data_.c_tensor,
                reduce.ptr, reduce.len,
                if (lengths != null) lengths.?.c_tensor else null,
                if (indices_ != null) indices_.?.c_tensor else null,
                if (offsets != null) offsets.?.c_tensor else null,
                axis,
                if (unsafe)  1  else  0,
                initial.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn select(
        self: *const Tensor, dim_: i64, index_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_select(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                index_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn selectBackward(
        grad_output: *const Tensor, input_sizes: []i64, dim_: i64, index_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_select_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                input_sizes.ptr, @intCast(input_sizes.len),
                dim_,
                index_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn selectBackwardOut(
        out: *const Tensor, grad_output: *const Tensor, input_sizes: []i64, dim_: i64, index_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_select_backward_out(@ptrCast(&c_tensors), out.c_tensor,
                grad_output.c_tensor,
                input_sizes.ptr, @intCast(input_sizes.len),
                dim_,
                index_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn selectCopy(
        self: *const Tensor, dim_: i64, index_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_select_copy(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                index_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn selectCopyIntOut(
        self: *const Tensor, out: *const Tensor, dim_: i64, index_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_select_copy_int_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_,
                index_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn selectScatter(
        self: *const Tensor, src: *const Tensor, dim_: i64, index_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_select_scatter(@ptrCast(&c_tensors), self.c_tensor,
                src.c_tensor,
                dim_,
                index_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn selectScatterOut(
        self: *const Tensor, out: *const Tensor, src: *const Tensor, dim_: i64, index_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_select_scatter_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                src.c_tensor,
                dim_,
                index_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn selu(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_selu(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn selu_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_selu_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn set(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_set(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn set_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_set_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn setData(
        self: *Tensor, new_data: *const Tensor
    ) void {
        __c.atg_set_data(self.c_tensor,
                new_data.c_tensor);
        torch.readAndCleanError();
        return;
    }

    pub fn setOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_set_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn setRequiresGrad(
        self: *const Tensor, r: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_set_requires_grad(@ptrCast(&c_tensors), self.c_tensor,
                if (r)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn setSourceTensor(
        self: *const Tensor, source: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_set_source_tensor(@ptrCast(&c_tensors), self.c_tensor,
                source.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn setSourceTensor_(
        self: *Tensor, source: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_set_source_tensor_(@ptrCast(&c_tensors), self.c_tensor,
                source.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn setSourceTensorOut(
        self: *const Tensor, out: *const Tensor, source: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_set_source_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                source.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn setSourceTensorStorageOffset_(
        self: *Tensor, source: *const Tensor, storage_offset: i64, size_: []i64, stride_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_set_source_tensor_storage_offset_(@ptrCast(&c_tensors), self.c_tensor,
                source.c_tensor,
                storage_offset,
                size_.ptr, @intCast(size_.len),
                stride_.ptr, @intCast(stride_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sgn(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sgn(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sgn_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sgn_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sgnOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sgn_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sigmoid(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sigmoid(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sigmoid_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sigmoid_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sigmoidBackward(
        grad_output: *const Tensor, output: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sigmoid_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                output.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sigmoidBackwardGradInput(
        grad_input: *const Tensor, grad_output: *const Tensor, output: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sigmoid_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                output.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sigmoidOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sigmoid_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sign(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sign(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sign2(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sign_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn signOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sign_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn signbit(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_signbit(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn signbitOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_signbit_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn silu(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_silu(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn silu_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_silu_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn siluBackward(
        self: *const Tensor, grad_output: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_silu_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn siluBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_silu_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn siluOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_silu_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sin(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sin(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sin_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sin_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sinOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sin_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sinc(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sinc(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sinc_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sinc_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sincOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sinc_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sinh(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sinh(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sinh_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sinh_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sinhOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sinh_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn slice(
        self: *const Tensor, dim_: i64, start: ?i64, end: ?i64, step: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_slice(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                start orelse 0, (start == null),
                end orelse 0, (end == null),
                step);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sliceBackward(
        grad_output: *const Tensor, input_sizes: []i64, dim_: i64, start: i64, end: i64, step: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_slice_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                input_sizes.ptr, @intCast(input_sizes.len),
                dim_,
                start,
                end,
                step);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sliceBackwardOut(
        out: *const Tensor, grad_output: *const Tensor, input_sizes: []i64, dim_: i64, start: i64, end: i64, step: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_slice_backward_out(@ptrCast(&c_tensors), out.c_tensor,
                grad_output.c_tensor,
                input_sizes.ptr, @intCast(input_sizes.len),
                dim_,
                start,
                end,
                step);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sliceCopy(
        self: *const Tensor, dim_: i64, start: ?i64, end: ?i64, step: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_slice_copy(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                start orelse 0, (start == null),
                end orelse 0, (end == null),
                step);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sliceCopyTensorOut(
        self: *const Tensor, out: *const Tensor, dim_: i64, start: ?i64, end: ?i64, step: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_slice_copy_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_,
                start orelse 0, (start == null),
                end orelse 0, (end == null),
                step);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sliceInverse(
        self: *const Tensor, src: *const Tensor, dim_: i64, start: ?i64, end: ?i64, step: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_slice_inverse(@ptrCast(&c_tensors), self.c_tensor,
                src.c_tensor,
                dim_,
                start orelse 0, (start == null),
                end orelse 0, (end == null),
                step);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sliceScatter(
        self: *const Tensor, src: *const Tensor, dim_: i64, start: ?i64, end: ?i64, step: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_slice_scatter(@ptrCast(&c_tensors), self.c_tensor,
                src.c_tensor,
                dim_,
                start orelse 0, (start == null),
                end orelse 0, (end == null),
                step);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sliceScatterOut(
        self: *const Tensor, out: *const Tensor, src: *const Tensor, dim_: i64, start: ?i64, end: ?i64, step: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_slice_scatter_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                src.c_tensor,
                dim_,
                start orelse 0, (start == null),
                end orelse 0, (end == null),
                step);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn slogdet(
        self: *const Tensor, 
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_slogdet(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn slogdetOut(
        self: *const Tensor, sign_: *const Tensor, logabsdet: *const Tensor
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_slogdet_out(@ptrCast(&c_tensors), sign_.c_tensor,
                logabsdet.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn slowConv3d(
        self: *const Tensor, weight: *const Tensor, kernel_size: []i64, bias: ?*const Tensor, stride_: []i64, padding: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_slow_conv3d(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn slowConv3dOut(
        self: *const Tensor, out: *const Tensor, weight: *const Tensor, kernel_size: []i64, bias: ?*const Tensor, stride_: []i64, padding: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_slow_conv3d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                weight.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn slowConvDilated2d(
        self: *const Tensor, weight: *const Tensor, kernel_size: []i64, bias: ?*const Tensor, stride_: []i64, padding: []i64, dilation: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_slow_conv_dilated2d(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn slowConvDilated2dOut(
        self: *const Tensor, out: *const Tensor, weight: *const Tensor, kernel_size: []i64, bias: ?*const Tensor, stride_: []i64, padding: []i64, dilation: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_slow_conv_dilated2d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                weight.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn slowConvDilated3d(
        self: *const Tensor, weight: *const Tensor, kernel_size: []i64, bias: ?*const Tensor, stride_: []i64, padding: []i64, dilation: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_slow_conv_dilated3d(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn slowConvDilated3dOut(
        self: *const Tensor, out: *const Tensor, weight: *const Tensor, kernel_size: []i64, bias: ?*const Tensor, stride_: []i64, padding: []i64, dilation: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_slow_conv_dilated3d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                weight.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                dilation.ptr, @intCast(dilation.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn slowConvTranspose2d(
        self: *const Tensor, weight: *const Tensor, kernel_size: []i64, bias: ?*const Tensor, stride_: []i64, padding: []i64, output_padding: []i64, dilation: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_slow_conv_transpose2d(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                output_padding.ptr, @intCast(output_padding.len),
                dilation.ptr, @intCast(dilation.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn slowConvTranspose2dOut(
        self: *const Tensor, out: *const Tensor, weight: *const Tensor, kernel_size: []i64, bias: ?*const Tensor, stride_: []i64, padding: []i64, output_padding: []i64, dilation: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_slow_conv_transpose2d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                weight.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                output_padding.ptr, @intCast(output_padding.len),
                dilation.ptr, @intCast(dilation.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn slowConvTranspose3d(
        self: *const Tensor, weight: *const Tensor, kernel_size: []i64, bias: ?*const Tensor, stride_: []i64, padding: []i64, output_padding: []i64, dilation: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_slow_conv_transpose3d(@ptrCast(&c_tensors), self.c_tensor,
                weight.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                output_padding.ptr, @intCast(output_padding.len),
                dilation.ptr, @intCast(dilation.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn slowConvTranspose3dOut(
        self: *const Tensor, out: *const Tensor, weight: *const Tensor, kernel_size: []i64, bias: ?*const Tensor, stride_: []i64, padding: []i64, output_padding: []i64, dilation: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_slow_conv_transpose3d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                weight.c_tensor,
                kernel_size.ptr, @intCast(kernel_size.len),
                if (bias != null) bias.?.c_tensor else null,
                stride_.ptr, @intCast(stride_.len),
                padding.ptr, @intCast(padding.len),
                output_padding.ptr, @intCast(output_padding.len),
                dilation.ptr, @intCast(dilation.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn smm(
        self: *const Tensor, mat2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_smm(@ptrCast(&c_tensors), self.c_tensor,
                mat2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn smoothL1Loss(
        self: *const Tensor, target: *const Tensor, reduction: i64, beta: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_smooth_l1_loss(@ptrCast(&c_tensors), self.c_tensor,
                target.c_tensor,
                reduction.to_int(),
                beta);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn smoothL1LossBackward(
        self: *const Tensor, grad_output: *const Tensor, target: *const Tensor, reduction: i64, beta: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_smooth_l1_loss_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                target.c_tensor,
                reduction.to_int(),
                beta);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn smoothL1LossBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor, target: *const Tensor, reduction: i64, beta: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_smooth_l1_loss_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                target.c_tensor,
                reduction.to_int(),
                beta);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn smoothL1LossOut(
        self: *const Tensor, out: *const Tensor, target: *const Tensor, reduction: i64, beta: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_smooth_l1_loss_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                target.c_tensor,
                reduction.to_int(),
                beta);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn softMarginLoss(
        self: *const Tensor, target: *const Tensor, reduction: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_soft_margin_loss(@ptrCast(&c_tensors), self.c_tensor,
                target.c_tensor,
                reduction.to_int());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn softMarginLossBackward(
        self: *const Tensor, grad_output: *const Tensor, target: *const Tensor, reduction: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_soft_margin_loss_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                target.c_tensor,
                reduction.to_int());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn softMarginLossBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor, target: *const Tensor, reduction: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_soft_margin_loss_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                target.c_tensor,
                reduction.to_int());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn softMarginLossOut(
        self: *const Tensor, out: *const Tensor, target: *const Tensor, reduction: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_soft_margin_loss_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                target.c_tensor,
                reduction.to_int());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn softmax(
        self: *const Tensor, dim_: i64, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_softmax(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn softmaxIntOut(
        self: *const Tensor, out: *const Tensor, dim_: i64, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_softmax_int_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn softplus(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_softplus(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn softplusBackward(
        self: *const Tensor, grad_output: *const Tensor, beta: Scalar, threshold_: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_softplus_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                beta.into().c_scalar,
                threshold_.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn softplusBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor, beta: Scalar, threshold_: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_softplus_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                beta.into().c_scalar,
                threshold_.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn softplusOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_softplus_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn softshrink(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_softshrink(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn softshrinkBackward(
        self: *const Tensor, grad_output: *const Tensor, lambd: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_softshrink_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                lambd.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn softshrinkBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor, lambd: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_softshrink_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                lambd.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn softshrinkOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_softshrink_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sort(
        self: *const Tensor, dim_: i64, descending: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_sort(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                if (descending)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn sortStable(
        self: *const Tensor, stable: bool, dim_: i64, descending: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_sort_stable(@ptrCast(&c_tensors), self.c_tensor,
                if (stable)  1  else  0,
                dim_,
                if (descending)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn sortValues(
        self: *const Tensor, values_: *const Tensor, indices_: *const Tensor, dim_: i64, descending: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_sort_values(@ptrCast(&c_tensors), values_.c_tensor,
                indices_.c_tensor,
                self.c_tensor,
                dim_,
                if (descending)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn sortValuesStable(
        self: *const Tensor, values_: *const Tensor, indices_: *const Tensor, stable: bool, dim_: i64, descending: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_sort_values_stable(@ptrCast(&c_tensors), values_.c_tensor,
                indices_.c_tensor,
                self.c_tensor,
                if (stable)  1  else  0,
                dim_,
                if (descending)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn sparseBscTensor(
        ccol_indices_: *const Tensor, row_indices_: *const Tensor, values_: *const Tensor, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sparse_bsc_tensor(@ptrCast(&c_tensors), ccol_indices_.c_tensor,
                row_indices_.c_tensor,
                values_.c_tensor,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sparseBscTensorCcolRowValueSize(
        ccol_indices_: *const Tensor, row_indices_: *const Tensor, values_: *const Tensor, size_: []i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sparse_bsc_tensor_ccol_row_value_size(@ptrCast(&c_tensors), ccol_indices_.c_tensor,
                row_indices_.c_tensor,
                values_.c_tensor,
                size_.ptr, @intCast(size_.len),
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sparseBsrTensor(
        crow_indices_: *const Tensor, col_indices_: *const Tensor, values_: *const Tensor, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sparse_bsr_tensor(@ptrCast(&c_tensors), crow_indices_.c_tensor,
                col_indices_.c_tensor,
                values_.c_tensor,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sparseBsrTensorCrowColValueSize(
        crow_indices_: *const Tensor, col_indices_: *const Tensor, values_: *const Tensor, size_: []i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sparse_bsr_tensor_crow_col_value_size(@ptrCast(&c_tensors), crow_indices_.c_tensor,
                col_indices_.c_tensor,
                values_.c_tensor,
                size_.ptr, @intCast(size_.len),
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sparseCompressedTensor(
        compressed_indices: *const Tensor, plain_indices: *const Tensor, values_: *const Tensor, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sparse_compressed_tensor(@ptrCast(&c_tensors), compressed_indices.c_tensor,
                plain_indices.c_tensor,
                values_.c_tensor,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sparseCompressedTensorCompPlainValueSize(
        compressed_indices: *const Tensor, plain_indices: *const Tensor, values_: *const Tensor, size_: []i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sparse_compressed_tensor_comp_plain_value_size(@ptrCast(&c_tensors), compressed_indices.c_tensor,
                plain_indices.c_tensor,
                values_.c_tensor,
                size_.ptr, @intCast(size_.len),
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sparseCooTensor(
        size_: []i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sparse_coo_tensor(@ptrCast(&c_tensors), size_.ptr, @intCast(size_.len),
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sparseCooTensorIndices(
        indices_: *const Tensor, values_: *const Tensor, options_: TensorOptions, is_coalesced_: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sparse_coo_tensor_indices(@ptrCast(&c_tensors), indices_.c_tensor,
                values_.c_tensor,
                options_.kind.cInt(), options_.device.cInt(),
                if (is_coalesced_)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sparseCooTensorIndicesSize(
        indices_: *const Tensor, values_: *const Tensor, size_: []i64, options_: TensorOptions, is_coalesced_: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sparse_coo_tensor_indices_size(@ptrCast(&c_tensors), indices_.c_tensor,
                values_.c_tensor,
                size_.ptr, @intCast(size_.len),
                options_.kind.cInt(), options_.device.cInt(),
                if (is_coalesced_)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sparseCooTensorSizeOut(
        out: *const Tensor, size_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sparse_coo_tensor_size_out(@ptrCast(&c_tensors), out.c_tensor,
                size_.ptr, @intCast(size_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sparseCscTensor(
        ccol_indices_: *const Tensor, row_indices_: *const Tensor, values_: *const Tensor, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sparse_csc_tensor(@ptrCast(&c_tensors), ccol_indices_.c_tensor,
                row_indices_.c_tensor,
                values_.c_tensor,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sparseCscTensorCcolRowValueSize(
        ccol_indices_: *const Tensor, row_indices_: *const Tensor, values_: *const Tensor, size_: []i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sparse_csc_tensor_ccol_row_value_size(@ptrCast(&c_tensors), ccol_indices_.c_tensor,
                row_indices_.c_tensor,
                values_.c_tensor,
                size_.ptr, @intCast(size_.len),
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sparseCsrTensor(
        crow_indices_: *const Tensor, col_indices_: *const Tensor, values_: *const Tensor, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sparse_csr_tensor(@ptrCast(&c_tensors), crow_indices_.c_tensor,
                col_indices_.c_tensor,
                values_.c_tensor,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sparseCsrTensorCrowColValueSize(
        crow_indices_: *const Tensor, col_indices_: *const Tensor, values_: *const Tensor, size_: []i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sparse_csr_tensor_crow_col_value_size(@ptrCast(&c_tensors), crow_indices_.c_tensor,
                col_indices_.c_tensor,
                values_.c_tensor,
                size_.ptr, @intCast(size_.len),
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sparseDim(
        self: *const Tensor, 
    ) i64 {
        const return_ = __c.atg_sparse_dim(self.c_tensor);
       torch.readAndCleanError();
        return return_;
    }

    pub fn sparseMask(
        self: *const Tensor, mask: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sparse_mask(@ptrCast(&c_tensors), self.c_tensor,
                mask.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sparseMaskOut(
        self: *const Tensor, out: *const Tensor, mask: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sparse_mask_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                mask.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sparseResize(
        self: *const Tensor, size_: []i64, sparse_dim_: i64, dense_dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sparse_resize(@ptrCast(&c_tensors), self.c_tensor,
                size_.ptr, @intCast(size_.len),
                sparse_dim_,
                dense_dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sparseResize_(
        self: *Tensor, size_: []i64, sparse_dim_: i64, dense_dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sparse_resize_(@ptrCast(&c_tensors), self.c_tensor,
                size_.ptr, @intCast(size_.len),
                sparse_dim_,
                dense_dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sparseResizeAndClear(
        self: *const Tensor, size_: []i64, sparse_dim_: i64, dense_dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sparse_resize_and_clear(@ptrCast(&c_tensors), self.c_tensor,
                size_.ptr, @intCast(size_.len),
                sparse_dim_,
                dense_dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sparseResizeAndClear_(
        self: *Tensor, size_: []i64, sparse_dim_: i64, dense_dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sparse_resize_and_clear_(@ptrCast(&c_tensors), self.c_tensor,
                size_.ptr, @intCast(size_.len),
                sparse_dim_,
                dense_dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sparseResizeAndClearOut(
        self: *const Tensor, out: *const Tensor, size_: []i64, sparse_dim_: i64, dense_dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sparse_resize_and_clear_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                size_.ptr, @intCast(size_.len),
                sparse_dim_,
                dense_dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sparseResizeOut(
        self: *const Tensor, out: *const Tensor, size_: []i64, sparse_dim_: i64, dense_dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sparse_resize_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                size_.ptr, @intCast(size_.len),
                sparse_dim_,
                dense_dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sparseSampledAddmm(
        self: *const Tensor, mat1: *const Tensor, mat2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sparse_sampled_addmm(@ptrCast(&c_tensors), self.c_tensor,
                mat1.c_tensor,
                mat2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sparseSampledAddmmOut(
        self: *const Tensor, out: *const Tensor, mat1: *const Tensor, mat2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sparse_sampled_addmm_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                mat1.c_tensor,
                mat2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialAiryAi(
        x: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_airy_ai(@ptrCast(&c_tensors), x.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialAiryAiOut(
        out: *const Tensor, x: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_airy_ai_out(@ptrCast(&c_tensors), out.c_tensor,
                x.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialBesselJ0(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_bessel_j0(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialBesselJ0Out(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_bessel_j0_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialBesselJ1(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_bessel_j1(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialBesselJ1Out(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_bessel_j1_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialBesselY0(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_bessel_y0(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialBesselY0Out(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_bessel_y0_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialBesselY1(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_bessel_y1(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialBesselY1Out(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_bessel_y1_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialChebyshevPolynomialT(
        x: *const Tensor, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_chebyshev_polynomial_t(@ptrCast(&c_tensors), x.c_tensor,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialChebyshevPolynomialTNScalar(
        x: *const Tensor, n: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_chebyshev_polynomial_t_n_scalar(@ptrCast(&c_tensors), x.c_tensor,
                n.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialChebyshevPolynomialTNScalarOut(
        out: *const Tensor, x: *const Tensor, n: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_chebyshev_polynomial_t_n_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                x.c_tensor,
                n.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialChebyshevPolynomialTOut(
        out: *const Tensor, x: *const Tensor, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_chebyshev_polynomial_t_out(@ptrCast(&c_tensors), out.c_tensor,
                x.c_tensor,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialChebyshevPolynomialTXScalar(
        x: Scalar, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_chebyshev_polynomial_t_x_scalar(@ptrCast(&c_tensors), x.into().c_scalar,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialChebyshevPolynomialTXScalarOut(
        out: *const Tensor, x: Scalar, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_chebyshev_polynomial_t_x_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                x.into().c_scalar,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialChebyshevPolynomialU(
        x: *const Tensor, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_chebyshev_polynomial_u(@ptrCast(&c_tensors), x.c_tensor,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialChebyshevPolynomialUNScalar(
        x: *const Tensor, n: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_chebyshev_polynomial_u_n_scalar(@ptrCast(&c_tensors), x.c_tensor,
                n.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialChebyshevPolynomialUNScalarOut(
        out: *const Tensor, x: *const Tensor, n: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_chebyshev_polynomial_u_n_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                x.c_tensor,
                n.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialChebyshevPolynomialUOut(
        out: *const Tensor, x: *const Tensor, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_chebyshev_polynomial_u_out(@ptrCast(&c_tensors), out.c_tensor,
                x.c_tensor,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialChebyshevPolynomialUXScalar(
        x: Scalar, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_chebyshev_polynomial_u_x_scalar(@ptrCast(&c_tensors), x.into().c_scalar,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialChebyshevPolynomialUXScalarOut(
        out: *const Tensor, x: Scalar, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_chebyshev_polynomial_u_x_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                x.into().c_scalar,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialChebyshevPolynomialV(
        x: *const Tensor, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_chebyshev_polynomial_v(@ptrCast(&c_tensors), x.c_tensor,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialChebyshevPolynomialVNScalar(
        x: *const Tensor, n: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_chebyshev_polynomial_v_n_scalar(@ptrCast(&c_tensors), x.c_tensor,
                n.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialChebyshevPolynomialVNScalarOut(
        out: *const Tensor, x: *const Tensor, n: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_chebyshev_polynomial_v_n_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                x.c_tensor,
                n.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialChebyshevPolynomialVOut(
        out: *const Tensor, x: *const Tensor, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_chebyshev_polynomial_v_out(@ptrCast(&c_tensors), out.c_tensor,
                x.c_tensor,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialChebyshevPolynomialVXScalar(
        x: Scalar, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_chebyshev_polynomial_v_x_scalar(@ptrCast(&c_tensors), x.into().c_scalar,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialChebyshevPolynomialVXScalarOut(
        out: *const Tensor, x: Scalar, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_chebyshev_polynomial_v_x_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                x.into().c_scalar,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialChebyshevPolynomialW(
        x: *const Tensor, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_chebyshev_polynomial_w(@ptrCast(&c_tensors), x.c_tensor,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialChebyshevPolynomialWNScalar(
        x: *const Tensor, n: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_chebyshev_polynomial_w_n_scalar(@ptrCast(&c_tensors), x.c_tensor,
                n.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialChebyshevPolynomialWNScalarOut(
        out: *const Tensor, x: *const Tensor, n: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_chebyshev_polynomial_w_n_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                x.c_tensor,
                n.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialChebyshevPolynomialWOut(
        out: *const Tensor, x: *const Tensor, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_chebyshev_polynomial_w_out(@ptrCast(&c_tensors), out.c_tensor,
                x.c_tensor,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialChebyshevPolynomialWXScalar(
        x: Scalar, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_chebyshev_polynomial_w_x_scalar(@ptrCast(&c_tensors), x.into().c_scalar,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialChebyshevPolynomialWXScalarOut(
        out: *const Tensor, x: Scalar, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_chebyshev_polynomial_w_x_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                x.into().c_scalar,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialDigamma(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_digamma(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialDigammaOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_digamma_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialEntr(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_entr(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialEntrOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_entr_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialErf(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_erf(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialErfOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_erf_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialErfc(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_erfc(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialErfcOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_erfc_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialErfcx(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_erfcx(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialErfcxOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_erfcx_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialErfinv(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_erfinv(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialErfinvOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_erfinv_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialExp2(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_exp2(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialExp2Out(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_exp2_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialExpit(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_expit(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialExpitOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_expit_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialExpm1(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_expm1(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialExpm1Out(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_expm1_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialGammainc(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_gammainc(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialGammaincOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_gammainc_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialGammaincc(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_gammaincc(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialGammainccOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_gammaincc_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialGammaln(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_gammaln(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialGammalnOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_gammaln_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialHermitePolynomialH(
        x: *const Tensor, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_hermite_polynomial_h(@ptrCast(&c_tensors), x.c_tensor,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialHermitePolynomialHNScalar(
        x: *const Tensor, n: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_hermite_polynomial_h_n_scalar(@ptrCast(&c_tensors), x.c_tensor,
                n.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialHermitePolynomialHNScalarOut(
        out: *const Tensor, x: *const Tensor, n: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_hermite_polynomial_h_n_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                x.c_tensor,
                n.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialHermitePolynomialHOut(
        out: *const Tensor, x: *const Tensor, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_hermite_polynomial_h_out(@ptrCast(&c_tensors), out.c_tensor,
                x.c_tensor,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialHermitePolynomialHXScalar(
        x: Scalar, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_hermite_polynomial_h_x_scalar(@ptrCast(&c_tensors), x.into().c_scalar,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialHermitePolynomialHXScalarOut(
        out: *const Tensor, x: Scalar, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_hermite_polynomial_h_x_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                x.into().c_scalar,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialHermitePolynomialHe(
        x: *const Tensor, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_hermite_polynomial_he(@ptrCast(&c_tensors), x.c_tensor,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialHermitePolynomialHeNScalar(
        x: *const Tensor, n: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_hermite_polynomial_he_n_scalar(@ptrCast(&c_tensors), x.c_tensor,
                n.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialHermitePolynomialHeNScalarOut(
        out: *const Tensor, x: *const Tensor, n: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_hermite_polynomial_he_n_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                x.c_tensor,
                n.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialHermitePolynomialHeOut(
        out: *const Tensor, x: *const Tensor, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_hermite_polynomial_he_out(@ptrCast(&c_tensors), out.c_tensor,
                x.c_tensor,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialHermitePolynomialHeXScalar(
        x: Scalar, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_hermite_polynomial_he_x_scalar(@ptrCast(&c_tensors), x.into().c_scalar,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialHermitePolynomialHeXScalarOut(
        out: *const Tensor, x: Scalar, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_hermite_polynomial_he_x_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                x.into().c_scalar,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialI0(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_i0(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialI0Out(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_i0_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialI0e(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_i0e(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialI0eOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_i0e_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialI1(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_i1(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialI1Out(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_i1_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialI1e(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_i1e(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialI1eOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_i1e_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialLaguerrePolynomialL(
        x: *const Tensor, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_laguerre_polynomial_l(@ptrCast(&c_tensors), x.c_tensor,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialLaguerrePolynomialLNScalar(
        x: *const Tensor, n: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_laguerre_polynomial_l_n_scalar(@ptrCast(&c_tensors), x.c_tensor,
                n.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialLaguerrePolynomialLNScalarOut(
        out: *const Tensor, x: *const Tensor, n: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_laguerre_polynomial_l_n_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                x.c_tensor,
                n.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialLaguerrePolynomialLOut(
        out: *const Tensor, x: *const Tensor, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_laguerre_polynomial_l_out(@ptrCast(&c_tensors), out.c_tensor,
                x.c_tensor,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialLaguerrePolynomialLXScalar(
        x: Scalar, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_laguerre_polynomial_l_x_scalar(@ptrCast(&c_tensors), x.into().c_scalar,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialLaguerrePolynomialLXScalarOut(
        out: *const Tensor, x: Scalar, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_laguerre_polynomial_l_x_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                x.into().c_scalar,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialLegendrePolynomialP(
        x: *const Tensor, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_legendre_polynomial_p(@ptrCast(&c_tensors), x.c_tensor,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialLegendrePolynomialPNScalar(
        x: *const Tensor, n: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_legendre_polynomial_p_n_scalar(@ptrCast(&c_tensors), x.c_tensor,
                n.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialLegendrePolynomialPNScalarOut(
        out: *const Tensor, x: *const Tensor, n: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_legendre_polynomial_p_n_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                x.c_tensor,
                n.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialLegendrePolynomialPOut(
        out: *const Tensor, x: *const Tensor, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_legendre_polynomial_p_out(@ptrCast(&c_tensors), out.c_tensor,
                x.c_tensor,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialLegendrePolynomialPXScalar(
        x: Scalar, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_legendre_polynomial_p_x_scalar(@ptrCast(&c_tensors), x.into().c_scalar,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialLegendrePolynomialPXScalarOut(
        out: *const Tensor, x: Scalar, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_legendre_polynomial_p_x_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                x.into().c_scalar,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialLog1p(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_log1p(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialLog1pOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_log1p_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialLogNdtr(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_log_ndtr(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialLogNdtrOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_log_ndtr_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialLogSoftmax(
        self: *const Tensor, dim_: i64, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_log_softmax(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialLogit(
        self: *const Tensor, eps: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_logit(@ptrCast(&c_tensors), self.c_tensor,
                eps orelse std.math.nan, (eps == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialLogitOut(
        self: *const Tensor, out: *const Tensor, eps: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_logit_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                eps orelse std.math.nan, (eps == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialLogsumexp(
        self: *const Tensor, dim_: []i64, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_logsumexp(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialLogsumexpOut(
        self: *const Tensor, out: *const Tensor, dim_: []i64, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_logsumexp_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialModifiedBesselI0(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_modified_bessel_i0(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialModifiedBesselI0Out(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_modified_bessel_i0_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialModifiedBesselI1(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_modified_bessel_i1(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialModifiedBesselI1Out(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_modified_bessel_i1_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialModifiedBesselK0(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_modified_bessel_k0(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialModifiedBesselK0Out(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_modified_bessel_k0_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialModifiedBesselK1(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_modified_bessel_k1(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialModifiedBesselK1Out(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_modified_bessel_k1_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialMultigammaln(
        self: *const Tensor, p: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_multigammaln(@ptrCast(&c_tensors), self.c_tensor,
                p);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialMultigammalnOut(
        self: *const Tensor, out: *const Tensor, p: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_multigammaln_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                p);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialNdtr(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_ndtr(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialNdtrOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_ndtr_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialNdtri(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_ndtri(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialNdtriOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_ndtri_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialPolygamma(
        self: *const Tensor, n: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_polygamma(@ptrCast(&c_tensors), n,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialPolygammaOut(
        self: *const Tensor, out: *const Tensor, n: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_polygamma_out(@ptrCast(&c_tensors), out.c_tensor,
                n,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialPsi(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_psi(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialPsiOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_psi_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialRound(
        self: *const Tensor, decimals: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_round(@ptrCast(&c_tensors), self.c_tensor,
                decimals);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialRoundOut(
        self: *const Tensor, out: *const Tensor, decimals: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_round_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                decimals);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialScaledModifiedBesselK0(
        x: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_scaled_modified_bessel_k0(@ptrCast(&c_tensors), x.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialScaledModifiedBesselK0Out(
        out: *const Tensor, x: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_scaled_modified_bessel_k0_out(@ptrCast(&c_tensors), out.c_tensor,
                x.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialScaledModifiedBesselK1(
        x: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_scaled_modified_bessel_k1(@ptrCast(&c_tensors), x.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialScaledModifiedBesselK1Out(
        out: *const Tensor, x: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_scaled_modified_bessel_k1_out(@ptrCast(&c_tensors), out.c_tensor,
                x.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialShiftedChebyshevPolynomialT(
        x: *const Tensor, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_shifted_chebyshev_polynomial_t(@ptrCast(&c_tensors), x.c_tensor,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialShiftedChebyshevPolynomialTNScalar(
        x: *const Tensor, n: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_shifted_chebyshev_polynomial_t_n_scalar(@ptrCast(&c_tensors), x.c_tensor,
                n.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialShiftedChebyshevPolynomialTNScalarOut(
        out: *const Tensor, x: *const Tensor, n: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_shifted_chebyshev_polynomial_t_n_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                x.c_tensor,
                n.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialShiftedChebyshevPolynomialTOut(
        out: *const Tensor, x: *const Tensor, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_shifted_chebyshev_polynomial_t_out(@ptrCast(&c_tensors), out.c_tensor,
                x.c_tensor,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialShiftedChebyshevPolynomialTXScalar(
        x: Scalar, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_shifted_chebyshev_polynomial_t_x_scalar(@ptrCast(&c_tensors), x.into().c_scalar,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialShiftedChebyshevPolynomialTXScalarOut(
        out: *const Tensor, x: Scalar, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_shifted_chebyshev_polynomial_t_x_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                x.into().c_scalar,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialShiftedChebyshevPolynomialU(
        x: *const Tensor, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_shifted_chebyshev_polynomial_u(@ptrCast(&c_tensors), x.c_tensor,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialShiftedChebyshevPolynomialUNScalar(
        x: *const Tensor, n: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_shifted_chebyshev_polynomial_u_n_scalar(@ptrCast(&c_tensors), x.c_tensor,
                n.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialShiftedChebyshevPolynomialUNScalarOut(
        out: *const Tensor, x: *const Tensor, n: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_shifted_chebyshev_polynomial_u_n_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                x.c_tensor,
                n.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialShiftedChebyshevPolynomialUOut(
        out: *const Tensor, x: *const Tensor, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_shifted_chebyshev_polynomial_u_out(@ptrCast(&c_tensors), out.c_tensor,
                x.c_tensor,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialShiftedChebyshevPolynomialUXScalar(
        x: Scalar, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_shifted_chebyshev_polynomial_u_x_scalar(@ptrCast(&c_tensors), x.into().c_scalar,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialShiftedChebyshevPolynomialUXScalarOut(
        out: *const Tensor, x: Scalar, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_shifted_chebyshev_polynomial_u_x_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                x.into().c_scalar,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialShiftedChebyshevPolynomialV(
        x: *const Tensor, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_shifted_chebyshev_polynomial_v(@ptrCast(&c_tensors), x.c_tensor,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialShiftedChebyshevPolynomialVNScalar(
        x: *const Tensor, n: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_shifted_chebyshev_polynomial_v_n_scalar(@ptrCast(&c_tensors), x.c_tensor,
                n.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialShiftedChebyshevPolynomialVNScalarOut(
        out: *const Tensor, x: *const Tensor, n: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_shifted_chebyshev_polynomial_v_n_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                x.c_tensor,
                n.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialShiftedChebyshevPolynomialVOut(
        out: *const Tensor, x: *const Tensor, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_shifted_chebyshev_polynomial_v_out(@ptrCast(&c_tensors), out.c_tensor,
                x.c_tensor,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialShiftedChebyshevPolynomialVXScalar(
        x: Scalar, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_shifted_chebyshev_polynomial_v_x_scalar(@ptrCast(&c_tensors), x.into().c_scalar,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialShiftedChebyshevPolynomialVXScalarOut(
        out: *const Tensor, x: Scalar, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_shifted_chebyshev_polynomial_v_x_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                x.into().c_scalar,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialShiftedChebyshevPolynomialW(
        x: *const Tensor, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_shifted_chebyshev_polynomial_w(@ptrCast(&c_tensors), x.c_tensor,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialShiftedChebyshevPolynomialWNScalar(
        x: *const Tensor, n: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_shifted_chebyshev_polynomial_w_n_scalar(@ptrCast(&c_tensors), x.c_tensor,
                n.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialShiftedChebyshevPolynomialWNScalarOut(
        out: *const Tensor, x: *const Tensor, n: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_shifted_chebyshev_polynomial_w_n_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                x.c_tensor,
                n.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialShiftedChebyshevPolynomialWOut(
        out: *const Tensor, x: *const Tensor, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_shifted_chebyshev_polynomial_w_out(@ptrCast(&c_tensors), out.c_tensor,
                x.c_tensor,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialShiftedChebyshevPolynomialWXScalar(
        x: Scalar, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_shifted_chebyshev_polynomial_w_x_scalar(@ptrCast(&c_tensors), x.into().c_scalar,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialShiftedChebyshevPolynomialWXScalarOut(
        out: *const Tensor, x: Scalar, n: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_shifted_chebyshev_polynomial_w_x_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                x.into().c_scalar,
                n.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialSinc(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_sinc(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialSincOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_sinc_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialSoftmax(
        self: *const Tensor, dim_: i64, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_softmax(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialSphericalBesselJ0(
        x: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_spherical_bessel_j0(@ptrCast(&c_tensors), x.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialSphericalBesselJ0Out(
        out: *const Tensor, x: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_spherical_bessel_j0_out(@ptrCast(&c_tensors), out.c_tensor,
                x.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialXlog1py(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_xlog1py(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialXlog1pyOtherScalar(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_xlog1py_other_scalar(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialXlog1pyOtherScalarOut(
        self: *const Tensor, out: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_xlog1py_other_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialXlog1pyOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_xlog1py_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialXlog1pySelfScalar(
        self_scalar: Scalar, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_xlog1py_self_scalar(@ptrCast(&c_tensors), self_scalar.into().c_scalar,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialXlog1pySelfScalarOut(
        out: *const Tensor, self_scalar: Scalar, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_xlog1py_self_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self_scalar.into().c_scalar,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialXlogy(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_xlogy(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialXlogyOtherScalar(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_xlogy_other_scalar(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialXlogyOtherScalarOut(
        self: *const Tensor, out: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_xlogy_other_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialXlogyOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_xlogy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialXlogySelfScalar(
        self_scalar: Scalar, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_xlogy_self_scalar(@ptrCast(&c_tensors), self_scalar.into().c_scalar,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialXlogySelfScalarOut(
        out: *const Tensor, self_scalar: Scalar, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_xlogy_self_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self_scalar.into().c_scalar,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialZeta(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_zeta(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialZetaOtherScalar(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_zeta_other_scalar(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialZetaOtherScalarOut(
        self: *const Tensor, out: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_zeta_other_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialZetaOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_zeta_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialZetaSelfScalar(
        self_scalar: Scalar, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_zeta_self_scalar(@ptrCast(&c_tensors), self_scalar.into().c_scalar,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn specialZetaSelfScalarOut(
        out: *const Tensor, self_scalar: Scalar, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_special_zeta_self_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self_scalar.into().c_scalar,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn split(
        self: *const Tensor, split_size: i64, dim_: i64
    ) []Tensor {
        const c_tensors = __c.atg_split(self.c_tensor,
                split_size,
                dim_);
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn splitCopy(
        self: *const Tensor, split_size: i64, dim_: i64
    ) []Tensor {
        const c_tensors = __c.atg_split_copy(self.c_tensor,
                split_size,
                dim_);
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn splitCopyTensorOut(
        self: *const Tensor, out: []*const Tensor, split_size: i64, dim_: i64
    ) void {
        __c.atg_split_copy_tensor_out(ptrList(out).ptr, @intCast(out.len),
                self.c_tensor,
                split_size,
                dim_);
        torch.readAndCleanError();
        return;
    }

    pub fn splitSizes(
        self: *const Tensor, split_size: []i64, dim_: i64
    ) []Tensor {
        const c_tensors = __c.atg_split_sizes(self.c_tensor,
                split_size.ptr, @intCast(split_size.len),
                dim_);
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn splitWithSizes(
        self: *const Tensor, split_sizes_: []i64, dim_: i64
    ) []Tensor {
        const c_tensors = __c.atg_split_with_sizes(self.c_tensor,
                split_sizes_.ptr, @intCast(split_sizes_.len),
                dim_);
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn splitWithSizesCopy(
        self: *const Tensor, split_sizes_: []i64, dim_: i64
    ) []Tensor {
        const c_tensors = __c.atg_split_with_sizes_copy(self.c_tensor,
                split_sizes_.ptr, @intCast(split_sizes_.len),
                dim_);
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn splitWithSizesCopyOut(
        self: *const Tensor, out: []*const Tensor, split_sizes_: []i64, dim_: i64
    ) void {
        __c.atg_split_with_sizes_copy_out(ptrList(out).ptr, @intCast(out.len),
                self.c_tensor,
                split_sizes_.ptr, @intCast(split_sizes_.len),
                dim_);
        torch.readAndCleanError();
        return;
    }

    pub fn sqrt(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sqrt(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sqrt_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sqrt_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sqrtOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sqrt_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn square(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_square(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn square_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_square_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn squareOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_square_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn squeeze(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_squeeze(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn squeeze_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_squeeze_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn squeezeCopy(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_squeeze_copy(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn squeezeCopyDim(
        self: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_squeeze_copy_dim(@ptrCast(&c_tensors), self.c_tensor,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn squeezeCopyDimOut(
        self: *const Tensor, out: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_squeeze_copy_dim_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn squeezeCopyDims(
        self: *const Tensor, dim_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_squeeze_copy_dims(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn squeezeCopyDimsOut(
        self: *const Tensor, out: *const Tensor, dim_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_squeeze_copy_dims_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_.ptr, @intCast(dim_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn squeezeCopyOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_squeeze_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn squeezeDim(
        self: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_squeeze_dim(@ptrCast(&c_tensors), self.c_tensor,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn squeezeDim_(
        self: *Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_squeeze_dim_(@ptrCast(&c_tensors), self.c_tensor,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn squeezeDims(
        self: *const Tensor, dim_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_squeeze_dims(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn squeezeDims_(
        self: *Tensor, dim_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_squeeze_dims_(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sspaddmm(
        self: *const Tensor, mat1: *const Tensor, mat2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sspaddmm(@ptrCast(&c_tensors), self.c_tensor,
                mat1.c_tensor,
                mat2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sspaddmmOut(
        self: *const Tensor, out: *const Tensor, mat1: *const Tensor, mat2: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sspaddmm_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                mat1.c_tensor,
                mat2.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn stack(
        tensors: []*const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_stack(@ptrCast(&c_tensors), ptrList(tensors).ptr, @intCast(tensors.len),
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn stackOut(
        out: *const Tensor, tensors: []*const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_stack_out(@ptrCast(&c_tensors), out.c_tensor,
                ptrList(tensors).ptr, @intCast(tensors.len),
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn stdev(
        self: *const Tensor, unbiased: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_std(@ptrCast(&c_tensors), self.c_tensor,
                if (unbiased)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn stdCorrection(
        self: *const Tensor, dim_: ?[]i64, correction: Scalar, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_std_correction(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                correction.into().c_scalar,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn stdCorrectionOut(
        self: *const Tensor, out: *const Tensor, dim_: ?[]i64, correction: Scalar, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_std_correction_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                correction.into().c_scalar,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn stdDim(
        self: *const Tensor, dim_: ?[]i64, unbiased: bool, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_std_dim(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (unbiased)  1  else  0,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn stdMean(
        self: *const Tensor, unbiased: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_std_mean(@ptrCast(&c_tensors), self.c_tensor,
                if (unbiased)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn stdMeanCorrection(
        self: *const Tensor, dim_: ?[]i64, correction: Scalar, keepdim: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_std_mean_correction(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                correction.into().c_scalar,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn stdMeanCorrectionOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, dim_: ?[]i64, correction: Scalar, keepdim: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_std_mean_correction_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                correction.into().c_scalar,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn stdMeanDim(
        self: *const Tensor, dim_: ?[]i64, unbiased: bool, keepdim: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_std_mean_dim(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (unbiased)  1  else  0,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn stdOut(
        self: *const Tensor, out: *const Tensor, dim_: ?[]i64, unbiased: bool, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_std_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (unbiased)  1  else  0,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn stft(
        self: *const Tensor, n_fft: i64, hop_length: ?i64, win_length: ?i64, window: ?*const Tensor, normalized: bool, onesided: bool, return_complex: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_stft(@ptrCast(&c_tensors), self.c_tensor,
                n_fft,
                hop_length orelse 0, (hop_length == null),
                win_length orelse 0, (win_length == null),
                if (window != null) window.?.c_tensor else null,
                if (normalized)  1  else  0,
                if (onesided)  1  else  0,
                if (return_complex)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn stftCenter(
        self: *const Tensor, n_fft: i64, hop_length: ?i64, win_length: ?i64, window: ?*const Tensor, center: bool, pad_mode: []const u8, normalized: bool, onesided: bool, return_complex: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_stft_center(@ptrCast(&c_tensors), self.c_tensor,
                n_fft,
                hop_length orelse 0, (hop_length == null),
                win_length orelse 0, (win_length == null),
                if (window != null) window.?.c_tensor else null,
                if (center)  1  else  0,
                pad_mode.ptr, pad_mode.len,
                if (normalized)  1  else  0,
                if (onesided)  1  else  0,
                if (return_complex)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sub(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sub(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sub_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sub_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn subOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sub_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn subScalar(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sub_scalar(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn subScalar_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sub_scalar_(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn subScalarOut(
        self: *const Tensor, out: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sub_scalar_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn subtract(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_subtract(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn subtract_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_subtract_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn subtractOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_subtract_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn subtractScalar(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_subtract_scalar(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn subtractScalar_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_subtract_scalar_(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sum(
        self: *const Tensor, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sum(@ptrCast(&c_tensors), self.c_tensor,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sumDimIntlist(
        self: *const Tensor, dim_: ?[]i64, keepdim: bool, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sum_dim_intlist(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sumIntlistOut(
        self: *const Tensor, out: *const Tensor, dim_: ?[]i64, keepdim: bool, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sum_intlist_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (keepdim)  1  else  0,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sumOut(
        self: *const Tensor, out: *const Tensor, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sum_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn sumToSize(
        self: *const Tensor, size_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_sum_to_size(@ptrCast(&c_tensors), self.c_tensor,
                size_.ptr, @intCast(size_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn svd(
        self: *const Tensor, some: bool, compute_uv: bool
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_svd(@ptrCast(&c_tensors), self.c_tensor,
                if (some)  1  else  0,
                if (compute_uv)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn svdU(
        self: *const Tensor, u: *const Tensor, s: *const Tensor, v: *const Tensor, some: bool, compute_uv: bool
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_svd_u(@ptrCast(&c_tensors), u.c_tensor,
                s.c_tensor,
                v.c_tensor,
                self.c_tensor,
                if (some)  1  else  0,
                if (compute_uv)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn swapaxes(
        self: *const Tensor, axis0: i64, axis1: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_swapaxes(@ptrCast(&c_tensors), self.c_tensor,
                axis0,
                axis1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn swapaxes_(
        self: *Tensor, axis0: i64, axis1: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_swapaxes_(@ptrCast(&c_tensors), self.c_tensor,
                axis0,
                axis1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn swapdims(
        self: *const Tensor, dim0: i64, dim1: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_swapdims(@ptrCast(&c_tensors), self.c_tensor,
                dim0,
                dim1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn swapdims_(
        self: *Tensor, dim0: i64, dim1: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_swapdims_(@ptrCast(&c_tensors), self.c_tensor,
                dim0,
                dim1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn t(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_t(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn t_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_t_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn tCopy(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_t_copy(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn tCopyOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_t_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn take(
        self: *const Tensor, index_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_take(@ptrCast(&c_tensors), self.c_tensor,
                index_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn takeAlongDim(
        self: *const Tensor, indices_: *const Tensor, dim_: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_take_along_dim(@ptrCast(&c_tensors), self.c_tensor,
                indices_.c_tensor,
                dim_ orelse 0, (dim_ == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn takeAlongDimOut(
        self: *const Tensor, out: *const Tensor, indices_: *const Tensor, dim_: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_take_along_dim_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                indices_.c_tensor,
                dim_ orelse 0, (dim_ == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn takeOut(
        self: *const Tensor, out: *const Tensor, index_: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_take_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                index_.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn tan(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_tan(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn tan_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_tan_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn tanOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_tan_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn tanh(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_tanh(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn tanh_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_tanh_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn tanhBackward(
        grad_output: *const Tensor, output: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_tanh_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                output.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn tanhBackwardGradInput(
        grad_input: *const Tensor, grad_output: *const Tensor, output: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_tanh_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                output.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn tanhOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_tanh_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn tensorSplit(
        self: *const Tensor, sections: i64, dim_: i64
    ) []Tensor {
        const c_tensors = __c.atg_tensor_split(self.c_tensor,
                sections,
                dim_);
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn tensorSplitIndices(
        self: *const Tensor, indices_: []i64, dim_: i64
    ) []Tensor {
        const c_tensors = __c.atg_tensor_split_indices(self.c_tensor,
                indices_.ptr, @intCast(indices_.len),
                dim_);
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn tensorSplitTensorIndicesOrSections(
        self: *const Tensor, tensor_indices_or_sections: *const Tensor, dim_: i64
    ) []Tensor {
        const c_tensors = __c.atg_tensor_split_tensor_indices_or_sections(self.c_tensor,
                tensor_indices_or_sections.c_tensor,
                dim_);
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn tensordot(
        self: *const Tensor, other: *const Tensor, dims_self: []i64, dims_other: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_tensordot(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor,
                dims_self.ptr, @intCast(dims_self.len),
                dims_other.ptr, @intCast(dims_other.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn tensordotOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor, dims_self: []i64, dims_other: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_tensordot_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor,
                dims_self.ptr, @intCast(dims_self.len),
                dims_other.ptr, @intCast(dims_other.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn threshold(
        self: *const Tensor, threshold_: Scalar, value: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_threshold(@ptrCast(&c_tensors), self.c_tensor,
                threshold_.into().c_scalar,
                value.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn threshold2(
        self: *Tensor, threshold_: Scalar, value: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_threshold_(@ptrCast(&c_tensors), self.c_tensor,
                threshold_.into().c_scalar,
                value.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn thresholdBackward(
        self: *const Tensor, grad_output: *const Tensor, threshold_: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_threshold_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                self.c_tensor,
                threshold_.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn thresholdBackwardGradInput(
        self: *const Tensor, grad_input: *const Tensor, grad_output: *const Tensor, threshold_: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_threshold_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                self.c_tensor,
                threshold_.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn thresholdOut(
        self: *const Tensor, out: *const Tensor, threshold_: Scalar, value: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_threshold_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                threshold_.into().c_scalar,
                value.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn tile(
        self: *const Tensor, dims: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_tile(@ptrCast(&c_tensors), self.c_tensor,
                dims.ptr, @intCast(dims.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn to(
        self: *const Tensor, device_: Device
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_to(@ptrCast(&c_tensors), self.c_tensor,
                device_.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn toDense(
        self: *const Tensor, dtype: ?Kind, masked_grad: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_to_dense(@ptrCast(&c_tensors), self.c_tensor,
                dtype orelse -1,
                if (masked_grad)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn toDenseBackward(
        self: *const Tensor, gradient: *const Tensor, masked_grad: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_to_dense_backward(@ptrCast(&c_tensors), gradient.c_tensor,
                self.c_tensor,
                if (masked_grad)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn toDevice(
        self: *const Tensor, device_: Device, dtype: Kind, non_blocking: bool, copy_: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_to_device(@ptrCast(&c_tensors), self.c_tensor,
                device_.cInt(),
                dtype.cInt(),
                if (non_blocking)  1  else  0,
                if (copy_)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn toDtype(
        self: *const Tensor, dtype: Kind, non_blocking: bool, copy_: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_to_dtype(@ptrCast(&c_tensors), self.c_tensor,
                dtype.cInt(),
                if (non_blocking)  1  else  0,
                if (copy_)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn toDtypeLayout(
        self: *const Tensor, options_: TensorOptions, non_blocking: bool, copy_: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_to_dtype_layout(@ptrCast(&c_tensors), self.c_tensor,
                options_.kind.cInt(), options_.device.cInt(),
                if (non_blocking)  1  else  0,
                if (copy_)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn toMkldnn(
        self: *const Tensor, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_to_mkldnn(@ptrCast(&c_tensors), self.c_tensor,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn toMkldnnBackward(
        self: *const Tensor, gradient: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_to_mkldnn_backward(@ptrCast(&c_tensors), gradient.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn toMkldnnOut(
        self: *const Tensor, out: *const Tensor, dtype: ?Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_to_mkldnn_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dtype orelse -1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn toOther(
        self: *const Tensor, other: *const Tensor, non_blocking: bool, copy_: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_to_other(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor,
                if (non_blocking)  1  else  0,
                if (copy_)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn toPaddedTensor(
        self: *const Tensor, padding: f64, output_size: ?[]i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_to_padded_tensor(@ptrCast(&c_tensors), self.c_tensor,
                padding,
                output_size.ptr, @intCast(output_size.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn toPaddedTensorOut(
        self: *const Tensor, out: *const Tensor, padding: f64, output_size: ?[]i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_to_padded_tensor_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                padding,
                output_size.ptr, @intCast(output_size.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn toSparse(
        self: *const Tensor, layout: ?Layout, blocksize: ?[]i64, dense_dim_: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_to_sparse(@ptrCast(&c_tensors), self.c_tensor,
                layout orelse - 1,
                blocksize.ptr, @intCast(blocksize.len),
                dense_dim_ orelse 0, (dense_dim_ == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn toSparseBsc(
        self: *const Tensor, blocksize: []i64, dense_dim_: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_to_sparse_bsc(@ptrCast(&c_tensors), self.c_tensor,
                blocksize.ptr, @intCast(blocksize.len),
                dense_dim_ orelse 0, (dense_dim_ == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn toSparseBsr(
        self: *const Tensor, blocksize: []i64, dense_dim_: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_to_sparse_bsr(@ptrCast(&c_tensors), self.c_tensor,
                blocksize.ptr, @intCast(blocksize.len),
                dense_dim_ orelse 0, (dense_dim_ == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn toSparseCsc(
        self: *const Tensor, dense_dim_: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_to_sparse_csc(@ptrCast(&c_tensors), self.c_tensor,
                dense_dim_ orelse 0, (dense_dim_ == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn toSparseCsr(
        self: *const Tensor, dense_dim_: ?i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_to_sparse_csr(@ptrCast(&c_tensors), self.c_tensor,
                dense_dim_ orelse 0, (dense_dim_ == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn toSparseSparseDim(
        self: *const Tensor, sparse_dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_to_sparse_sparse_dim(@ptrCast(&c_tensors), self.c_tensor,
                sparse_dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn topk(
        self: *const Tensor, k: i64, dim_: i64, largest: bool, sorted: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_topk(@ptrCast(&c_tensors), self.c_tensor,
                k,
                dim_,
                if (largest)  1  else  0,
                if (sorted)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn topkValues(
        self: *const Tensor, values_: *const Tensor, indices_: *const Tensor, k: i64, dim_: i64, largest: bool, sorted: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_topk_values(@ptrCast(&c_tensors), values_.c_tensor,
                indices_.c_tensor,
                self.c_tensor,
                k,
                dim_,
                if (largest)  1  else  0,
                if (sorted)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn totype(
        self: *const Tensor, scalar_type: Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_totype(@ptrCast(&c_tensors), self.c_tensor,
                scalar_type.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn trace(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_trace(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn traceBackward(
        gradient: *const Tensor, sizes: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_trace_backward(@ptrCast(&c_tensors), gradient.c_tensor,
                sizes.ptr, @intCast(sizes.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn traceOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_trace_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn transpose(
        self: *const Tensor, dim0: i64, dim1: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_transpose(@ptrCast(&c_tensors), self.c_tensor,
                dim0,
                dim1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn transpose2(
        self: *Tensor, dim0: i64, dim1: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_transpose_(@ptrCast(&c_tensors), self.c_tensor,
                dim0,
                dim1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn transposeCopy(
        self: *const Tensor, dim0: i64, dim1: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_transpose_copy(@ptrCast(&c_tensors), self.c_tensor,
                dim0,
                dim1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn transposeCopyIntOut(
        self: *const Tensor, out: *const Tensor, dim0: i64, dim1: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_transpose_copy_int_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim0,
                dim1);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn trapezoid(
        y: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_trapezoid(@ptrCast(&c_tensors), y.c_tensor,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn trapezoidX(
        y: *const Tensor, x: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_trapezoid_x(@ptrCast(&c_tensors), y.c_tensor,
                x.c_tensor,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn trapz(
        y: *const Tensor, x: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_trapz(@ptrCast(&c_tensors), y.c_tensor,
                x.c_tensor,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn trapzDx(
        y: *const Tensor, dx: f64, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_trapz_dx(@ptrCast(&c_tensors), y.c_tensor,
                dx,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn triangularSolve(
        self: *const Tensor, a: *const Tensor, upper: bool, transpose_: bool, unitriangular: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_triangular_solve(@ptrCast(&c_tensors), self.c_tensor,
                a.c_tensor,
                if (upper)  1  else  0,
                if (transpose_)  1  else  0,
                if (unitriangular)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn triangularSolveX(
        self: *const Tensor, x: *const Tensor, m: *const Tensor, a: *const Tensor, upper: bool, transpose_: bool, unitriangular: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_triangular_solve_x(@ptrCast(&c_tensors), x.c_tensor,
                m.c_tensor,
                self.c_tensor,
                a.c_tensor,
                if (upper)  1  else  0,
                if (transpose_)  1  else  0,
                if (unitriangular)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn tril(
        self: *const Tensor, diagonal_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_tril(@ptrCast(&c_tensors), self.c_tensor,
                diagonal_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn tril_(
        self: *Tensor, diagonal_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_tril_(@ptrCast(&c_tensors), self.c_tensor,
                diagonal_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn trilIndices(
        row: i64, col: i64, offset: i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_tril_indices(@ptrCast(&c_tensors), row,
                col,
                offset,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn trilIndicesOut(
        out: *const Tensor, row: i64, col: i64, offset: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_tril_indices_out(@ptrCast(&c_tensors), out.c_tensor,
                row,
                col,
                offset);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn trilOut(
        self: *const Tensor, out: *const Tensor, diagonal_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_tril_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                diagonal_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn tripletMarginLoss(
        anchor: *const Tensor, positive_: *const Tensor, negative_: *const Tensor, margin: f64, p: f64, eps: f64, swap: bool, reduction: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_triplet_margin_loss(@ptrCast(&c_tensors), anchor.c_tensor,
                positive_.c_tensor,
                negative_.c_tensor,
                margin,
                p,
                eps,
                if (swap)  1  else  0,
                reduction.to_int());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn triu(
        self: *const Tensor, diagonal_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_triu(@ptrCast(&c_tensors), self.c_tensor,
                diagonal_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn triu_(
        self: *Tensor, diagonal_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_triu_(@ptrCast(&c_tensors), self.c_tensor,
                diagonal_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn triuIndices(
        row: i64, col: i64, offset: i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_triu_indices(@ptrCast(&c_tensors), row,
                col,
                offset,
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn triuIndicesOut(
        out: *const Tensor, row: i64, col: i64, offset: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_triu_indices_out(@ptrCast(&c_tensors), out.c_tensor,
                row,
                col,
                offset);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn triuOut(
        self: *const Tensor, out: *const Tensor, diagonal_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_triu_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                diagonal_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn trueDivide(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_true_divide(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn trueDivide_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_true_divide_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn trueDivideOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_true_divide_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn trueDivideScalar(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_true_divide_scalar(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn trueDivideScalar_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_true_divide_scalar_(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn trunc(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_trunc(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn trunc_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_trunc_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn truncOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_trunc_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn typeAs(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_type_as(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn unbind(
        self: *const Tensor, dim_: i64
    ) []Tensor {
        const c_tensors = __c.atg_unbind(self.c_tensor,
                dim_);
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn unbindCopy(
        self: *const Tensor, dim_: i64
    ) []Tensor {
        const c_tensors = __c.atg_unbind_copy(self.c_tensor,
                dim_);
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn unbindCopyIntOut(
        self: *const Tensor, out: []*const Tensor, dim_: i64
    ) void {
        __c.atg_unbind_copy_int_out(ptrList(out).ptr, @intCast(out.len),
                self.c_tensor,
                dim_);
        torch.readAndCleanError();
        return;
    }

    pub fn unflatten(
        self: *const Tensor, dim_: i64, sizes: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_unflatten(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                sizes.ptr, @intCast(sizes.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn unflattenDenseTensors(
        flat: *const Tensor, tensors: []*const Tensor
    ) []Tensor {
        const c_tensors = __c.atg_unflatten_dense_tensors(flat.c_tensor,
                ptrList(tensors).ptr, @intCast(tensors.len));
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn unfold(
        self: *const Tensor, dimension: i64, size_: i64, step: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_unfold(@ptrCast(&c_tensors), self.c_tensor,
                dimension,
                size_,
                step);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn unfoldBackward(
        grad_in: *const Tensor, input_sizes: []i64, dim_: i64, size_: i64, step: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_unfold_backward(@ptrCast(&c_tensors), grad_in.c_tensor,
                input_sizes.ptr, @intCast(input_sizes.len),
                dim_,
                size_,
                step);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn unfoldBackwardOut(
        out: *const Tensor, grad_in: *const Tensor, input_sizes: []i64, dim_: i64, size_: i64, step: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_unfold_backward_out(@ptrCast(&c_tensors), out.c_tensor,
                grad_in.c_tensor,
                input_sizes.ptr, @intCast(input_sizes.len),
                dim_,
                size_,
                step);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn unfoldCopy(
        self: *const Tensor, dimension: i64, size_: i64, step: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_unfold_copy(@ptrCast(&c_tensors), self.c_tensor,
                dimension,
                size_,
                step);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn unfoldCopyOut(
        self: *const Tensor, out: *const Tensor, dimension: i64, size_: i64, step: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_unfold_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dimension,
                size_,
                step);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn uniform(
        self: *const Tensor, from: f64, to_: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_uniform(@ptrCast(&c_tensors), self.c_tensor,
                from,
                to_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn uniform_(
        self: *Tensor, from: f64, to_: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_uniform_(@ptrCast(&c_tensors), self.c_tensor,
                from,
                to_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn uniformOut(
        self: *const Tensor, out: *const Tensor, from: f64, to_: f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_uniform_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                from,
                to_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn uniqueConsecutive(
        self: *const Tensor, return_inverse: bool, return_counts: bool, dim_: ?i64
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_unique_consecutive(@ptrCast(&c_tensors), self.c_tensor,
                if (return_inverse)  1  else  0,
                if (return_counts)  1  else  0,
                dim_ orelse 0, (dim_ == null));
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn uniqueConsecutiveOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, out2: *const Tensor, return_inverse: bool, return_counts: bool, dim_: ?i64
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_unique_consecutive_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                out2.c_tensor,
                self.c_tensor,
                if (return_inverse)  1  else  0,
                if (return_counts)  1  else  0,
                dim_ orelse 0, (dim_ == null));
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn uniqueDim(
        self: *const Tensor, dim_: i64, sorted: bool, return_inverse: bool, return_counts: bool
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_unique_dim(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                if (sorted)  1  else  0,
                if (return_inverse)  1  else  0,
                if (return_counts)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn uniqueDimConsecutive(
        self: *const Tensor, dim_: i64, return_inverse: bool, return_counts: bool
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_unique_dim_consecutive(@ptrCast(&c_tensors), self.c_tensor,
                dim_,
                if (return_inverse)  1  else  0,
                if (return_counts)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn uniqueDimConsecutiveOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, out2: *const Tensor, dim_: i64, return_inverse: bool, return_counts: bool
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_unique_dim_consecutive_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                out2.c_tensor,
                self.c_tensor,
                dim_,
                if (return_inverse)  1  else  0,
                if (return_counts)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn uniqueDimOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, out2: *const Tensor, dim_: i64, sorted: bool, return_inverse: bool, return_counts: bool
    ) [3]Tensor {
        var c_tensors = [_]C_tensor{null} ** 3;
        __c.atg_unique_dim_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                out2.c_tensor,
                self.c_tensor,
                dim_,
                if (sorted)  1  else  0,
                if (return_inverse)  1  else  0,
                if (return_counts)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }, Tensor { .c_tensor = c_tensors[2] }};
    }

    pub fn unsafeChunk(
        self: *const Tensor, chunks: i64, dim_: i64
    ) []Tensor {
        const c_tensors = __c.atg_unsafe_chunk(self.c_tensor,
                chunks,
                dim_);
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn unsafeSplit(
        self: *const Tensor, split_size: i64, dim_: i64
    ) []Tensor {
        const c_tensors = __c.atg_unsafe_split(self.c_tensor,
                split_size,
                dim_);
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn unsafeSplitTensorOut(
        self: *const Tensor, out: []*const Tensor, split_size: i64, dim_: i64
    ) void {
        __c.atg_unsafe_split_tensor_out(ptrList(out).ptr, @intCast(out.len),
                self.c_tensor,
                split_size,
                dim_);
        torch.readAndCleanError();
        return;
    }

    pub fn unsafeSplitWithSizes(
        self: *const Tensor, split_sizes_: []i64, dim_: i64
    ) []Tensor {
        const c_tensors = __c.atg_unsafe_split_with_sizes(self.c_tensor,
                split_sizes_.ptr, @intCast(split_sizes_.len),
                dim_);
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn unsafeSplitWithSizesOut(
        self: *const Tensor, out: []*const Tensor, split_sizes_: []i64, dim_: i64
    ) void {
        __c.atg_unsafe_split_with_sizes_out(ptrList(out).ptr, @intCast(out.len),
                self.c_tensor,
                split_sizes_.ptr, @intCast(split_sizes_.len),
                dim_);
        torch.readAndCleanError();
        return;
    }

    pub fn unsqueeze(
        self: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_unsqueeze(@ptrCast(&c_tensors), self.c_tensor,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn unsqueeze_(
        self: *Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_unsqueeze_(@ptrCast(&c_tensors), self.c_tensor,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn unsqueezeCopy(
        self: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_unsqueeze_copy(@ptrCast(&c_tensors), self.c_tensor,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn unsqueezeCopyOut(
        self: *const Tensor, out: *const Tensor, dim_: i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_unsqueeze_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleBicubic2d(
        self: *const Tensor, output_size: []i64, align_corners: bool, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_bicubic2d(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                if (align_corners)  1  else  0,
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleBicubic2dBackward(
        grad_output: *const Tensor, output_size: []i64, input_size: []i64, align_corners: bool, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_bicubic2d_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                input_size.ptr, @intCast(input_size.len),
                if (align_corners)  1  else  0,
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleBicubic2dBackwardGradInput(
        grad_input: *const Tensor, grad_output: *const Tensor, output_size: []i64, input_size: []i64, align_corners: bool, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_bicubic2d_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                input_size.ptr, @intCast(input_size.len),
                if (align_corners)  1  else  0,
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleBicubic2dOut(
        self: *const Tensor, out: *const Tensor, output_size: []i64, align_corners: bool, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_bicubic2d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                if (align_corners)  1  else  0,
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleBicubic2dVec(
        self: *const Tensor, output_size: ?[]i64, align_corners: bool, scale_factors: []f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_bicubic2d_vec(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                if (align_corners)  1  else  0,
                scale_factors.ptr, @intCast(scale_factors.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleBilinear2d(
        self: *const Tensor, output_size: []i64, align_corners: bool, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_bilinear2d(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                if (align_corners)  1  else  0,
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleBilinear2dBackward(
        grad_output: *const Tensor, output_size: []i64, input_size: []i64, align_corners: bool, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_bilinear2d_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                input_size.ptr, @intCast(input_size.len),
                if (align_corners)  1  else  0,
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleBilinear2dBackwardGradInput(
        grad_input: *const Tensor, grad_output: *const Tensor, output_size: []i64, input_size: []i64, align_corners: bool, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_bilinear2d_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                input_size.ptr, @intCast(input_size.len),
                if (align_corners)  1  else  0,
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleBilinear2dOut(
        self: *const Tensor, out: *const Tensor, output_size: []i64, align_corners: bool, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_bilinear2d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                if (align_corners)  1  else  0,
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleBilinear2dVec(
        self: *const Tensor, output_size: ?[]i64, align_corners: bool, scale_factors: []f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_bilinear2d_vec(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                if (align_corners)  1  else  0,
                scale_factors.ptr, @intCast(scale_factors.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleLinear1d(
        self: *const Tensor, output_size: []i64, align_corners: bool, scales: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_linear1d(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                if (align_corners)  1  else  0,
                scales orelse std.math.nan, (scales == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleLinear1dBackward(
        grad_output: *const Tensor, output_size: []i64, input_size: []i64, align_corners: bool, scales: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_linear1d_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                input_size.ptr, @intCast(input_size.len),
                if (align_corners)  1  else  0,
                scales orelse std.math.nan, (scales == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleLinear1dBackwardGradInput(
        grad_input: *const Tensor, grad_output: *const Tensor, output_size: []i64, input_size: []i64, align_corners: bool, scales: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_linear1d_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                input_size.ptr, @intCast(input_size.len),
                if (align_corners)  1  else  0,
                scales orelse std.math.nan, (scales == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleLinear1dOut(
        self: *const Tensor, out: *const Tensor, output_size: []i64, align_corners: bool, scales: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_linear1d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                if (align_corners)  1  else  0,
                scales orelse std.math.nan, (scales == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleLinear1dVec(
        self: *const Tensor, output_size: ?[]i64, align_corners: bool, scale_factors: []f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_linear1d_vec(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                if (align_corners)  1  else  0,
                scale_factors.ptr, @intCast(scale_factors.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleNearest1d(
        self: *const Tensor, output_size: []i64, scales: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_nearest1d(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                scales orelse std.math.nan, (scales == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleNearest1dBackward(
        grad_output: *const Tensor, output_size: []i64, input_size: []i64, scales: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_nearest1d_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                input_size.ptr, @intCast(input_size.len),
                scales orelse std.math.nan, (scales == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleNearest1dBackwardGradInput(
        grad_input: *const Tensor, grad_output: *const Tensor, output_size: []i64, input_size: []i64, scales: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_nearest1d_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                input_size.ptr, @intCast(input_size.len),
                scales orelse std.math.nan, (scales == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleNearest1dOut(
        self: *const Tensor, out: *const Tensor, output_size: []i64, scales: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_nearest1d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                scales orelse std.math.nan, (scales == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleNearest1dVec(
        self: *const Tensor, output_size: ?[]i64, scale_factors: []f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_nearest1d_vec(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                scale_factors.ptr, @intCast(scale_factors.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleNearest2d(
        self: *const Tensor, output_size: []i64, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_nearest2d(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleNearest2dBackward(
        grad_output: *const Tensor, output_size: []i64, input_size: []i64, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_nearest2d_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                input_size.ptr, @intCast(input_size.len),
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleNearest2dBackwardGradInput(
        grad_input: *const Tensor, grad_output: *const Tensor, output_size: []i64, input_size: []i64, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_nearest2d_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                input_size.ptr, @intCast(input_size.len),
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleNearest2dOut(
        self: *const Tensor, out: *const Tensor, output_size: []i64, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_nearest2d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleNearest2dVec(
        self: *const Tensor, output_size: ?[]i64, scale_factors: []f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_nearest2d_vec(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                scale_factors.ptr, @intCast(scale_factors.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleNearest3d(
        self: *const Tensor, output_size: []i64, scales_d: ?f64, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_nearest3d(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                scales_d orelse std.math.nan, (scales_d == null),
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleNearest3dBackward(
        grad_output: *const Tensor, output_size: []i64, input_size: []i64, scales_d: ?f64, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_nearest3d_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                input_size.ptr, @intCast(input_size.len),
                scales_d orelse std.math.nan, (scales_d == null),
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleNearest3dBackwardGradInput(
        grad_input: *const Tensor, grad_output: *const Tensor, output_size: []i64, input_size: []i64, scales_d: ?f64, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_nearest3d_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                input_size.ptr, @intCast(input_size.len),
                scales_d orelse std.math.nan, (scales_d == null),
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleNearest3dOut(
        self: *const Tensor, out: *const Tensor, output_size: []i64, scales_d: ?f64, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_nearest3d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                scales_d orelse std.math.nan, (scales_d == null),
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleNearest3dVec(
        self: *const Tensor, output_size: ?[]i64, scale_factors: []f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_nearest3d_vec(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                scale_factors.ptr, @intCast(scale_factors.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleTrilinear3d(
        self: *const Tensor, output_size: []i64, align_corners: bool, scales_d: ?f64, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_trilinear3d(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                if (align_corners)  1  else  0,
                scales_d orelse std.math.nan, (scales_d == null),
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleTrilinear3dBackward(
        grad_output: *const Tensor, output_size: []i64, input_size: []i64, align_corners: bool, scales_d: ?f64, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_trilinear3d_backward(@ptrCast(&c_tensors), grad_output.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                input_size.ptr, @intCast(input_size.len),
                if (align_corners)  1  else  0,
                scales_d orelse std.math.nan, (scales_d == null),
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleTrilinear3dBackwardGradInput(
        grad_input: *const Tensor, grad_output: *const Tensor, output_size: []i64, input_size: []i64, align_corners: bool, scales_d: ?f64, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_trilinear3d_backward_grad_input(@ptrCast(&c_tensors), grad_input.c_tensor,
                grad_output.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                input_size.ptr, @intCast(input_size.len),
                if (align_corners)  1  else  0,
                scales_d orelse std.math.nan, (scales_d == null),
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleTrilinear3dOut(
        self: *const Tensor, out: *const Tensor, output_size: []i64, align_corners: bool, scales_d: ?f64, scales_h: ?f64, scales_w: ?f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_trilinear3d_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                if (align_corners)  1  else  0,
                scales_d orelse std.math.nan, (scales_d == null),
                scales_h orelse std.math.nan, (scales_h == null),
                scales_w orelse std.math.nan, (scales_w == null));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn upsampleTrilinear3dVec(
        self: *const Tensor, output_size: ?[]i64, align_corners: bool, scale_factors: []f64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_upsample_trilinear3d_vec(@ptrCast(&c_tensors), self.c_tensor,
                output_size.ptr, @intCast(output_size.len),
                if (align_corners)  1  else  0,
                scale_factors.ptr, @intCast(scale_factors.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn valueSelectingReductionBackward(
        gradient: *const Tensor, dim_: i64, indices_: *const Tensor, sizes: []i64, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_value_selecting_reduction_backward(@ptrCast(&c_tensors), gradient.c_tensor,
                dim_,
                indices_.c_tensor,
                sizes.ptr, @intCast(sizes.len),
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn values(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_values(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn valuesCopy(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_values_copy(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn valuesCopyOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_values_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn vander(
        x: *const Tensor, n: ?i64, increasing: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_vander(@ptrCast(&c_tensors), x.c_tensor,
                n orelse 0, (n == null),
                if (increasing)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn variance(
        self: *const Tensor, unbiased: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_var(@ptrCast(&c_tensors), self.c_tensor,
                if (unbiased)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn varCorrection(
        self: *const Tensor, dim_: ?[]i64, correction: Scalar, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_var_correction(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                correction.into().c_scalar,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn varCorrectionOut(
        self: *const Tensor, out: *const Tensor, dim_: ?[]i64, correction: Scalar, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_var_correction_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                correction.into().c_scalar,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn varDim(
        self: *const Tensor, dim_: ?[]i64, unbiased: bool, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_var_dim(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (unbiased)  1  else  0,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn varMean(
        self: *const Tensor, unbiased: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_var_mean(@ptrCast(&c_tensors), self.c_tensor,
                if (unbiased)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn varMeanCorrection(
        self: *const Tensor, dim_: ?[]i64, correction: Scalar, keepdim: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_var_mean_correction(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                correction.into().c_scalar,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn varMeanCorrectionOut(
        self: *const Tensor, out0: *const Tensor, out1: *const Tensor, dim_: ?[]i64, correction: Scalar, keepdim: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_var_mean_correction_out(@ptrCast(&c_tensors), out0.c_tensor,
                out1.c_tensor,
                self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                correction.into().c_scalar,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn varMeanDim(
        self: *const Tensor, dim_: ?[]i64, unbiased: bool, keepdim: bool
    ) [2]Tensor {
        var c_tensors = [_]C_tensor{null} ** 2;
        __c.atg_var_mean_dim(@ptrCast(&c_tensors), self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (unbiased)  1  else  0,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return .{Tensor { .c_tensor = c_tensors[0] }, Tensor { .c_tensor = c_tensors[1] }};
    }

    pub fn varOut(
        self: *const Tensor, out: *const Tensor, dim_: ?[]i64, unbiased: bool, keepdim: bool
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_var_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dim_.ptr, @intCast(dim_.len),
                if (unbiased)  1  else  0,
                if (keepdim)  1  else  0);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn vdot(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_vdot(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn vdotOut(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_vdot_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn view(
        self: *const Tensor, size_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_view(@ptrCast(&c_tensors), self.c_tensor,
                size_.ptr, @intCast(size_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn viewAs(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_view_as(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn viewAsComplex(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_view_as_complex(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn viewAsComplexCopy(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_view_as_complex_copy(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn viewAsComplexCopyOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_view_as_complex_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn viewAsReal(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_view_as_real(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn viewAsRealCopy(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_view_as_real_copy(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn viewAsRealCopyOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_view_as_real_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn viewCopy(
        self: *const Tensor, size_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_view_copy(@ptrCast(&c_tensors), self.c_tensor,
                size_.ptr, @intCast(size_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn viewCopyDtype(
        self: *const Tensor, dtype: Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_view_copy_dtype(@ptrCast(&c_tensors), self.c_tensor,
                dtype.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn viewCopyDtypeOut(
        self: *const Tensor, out: *const Tensor, dtype: Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_view_copy_dtype_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                dtype.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn viewCopyOut(
        self: *const Tensor, out: *const Tensor, size_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_view_copy_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                size_.ptr, @intCast(size_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn viewDtype(
        self: *const Tensor, dtype: Kind
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_view_dtype(@ptrCast(&c_tensors), self.c_tensor,
                dtype.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn vsplit(
        self: *const Tensor, sections: i64
    ) []Tensor {
        const c_tensors = __c.atg_vsplit(self.c_tensor,
                sections);
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn vsplitArray(
        self: *const Tensor, indices_: []i64
    ) []Tensor {
        const c_tensors = __c.atg_vsplit_array(self.c_tensor,
                indices_.ptr, @intCast(indices_.len));
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn vstack(
        tensors: []*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_vstack(@ptrCast(&c_tensors), ptrList(tensors).ptr, @intCast(tensors.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn vstackOut(
        out: *const Tensor, tensors: []*const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_vstack_out(@ptrCast(&c_tensors), out.c_tensor,
                ptrList(tensors).ptr, @intCast(tensors.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn where(
        condition: *const Tensor
    ) []Tensor {
        const c_tensors = __c.atg_where(condition.c_tensor);
        var r__ = std.ArrayList(Tensor).init(torch.global_allocator);
        var idx: usize = 0;
        while (true) {
            const c__ = c_tensors[idx];
            if (c__ == null) break;
            r__.append(Tensor{ .c_tensor = c__ });
            idx += 1;
        }
        return r__;
    }

    pub fn whereScalar(
        condition: *const Tensor, self_scalar: Scalar, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_where_scalar(@ptrCast(&c_tensors), condition.c_tensor,
                self_scalar.into().c_scalar,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn whereScalarother(
        self: *const Tensor, condition: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_where_scalarother(@ptrCast(&c_tensors), condition.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn whereScalarself(
        condition: *const Tensor, self_scalar: Scalar, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_where_scalarself(@ptrCast(&c_tensors), condition.c_tensor,
                self_scalar.into().c_scalar,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn whereSelf(
        self: *const Tensor, condition: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_where_self(@ptrCast(&c_tensors), condition.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn whereSelfOut(
        self: *const Tensor, out: *const Tensor, condition: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_where_self_out(@ptrCast(&c_tensors), out.c_tensor,
                condition.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn xlogy(
        self: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_xlogy(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn xlogy_(
        self: *Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_xlogy_(@ptrCast(&c_tensors), self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn xlogyOutscalarOther(
        self: *const Tensor, out: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_xlogy_outscalar_other(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn xlogyOutscalarSelf(
        out: *const Tensor, self_scalar: Scalar, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_xlogy_outscalar_self(@ptrCast(&c_tensors), out.c_tensor,
                self_scalar.into().c_scalar,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn xlogyOuttensor(
        self: *const Tensor, out: *const Tensor, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_xlogy_outtensor(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn xlogyScalarOther(
        self: *const Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_xlogy_scalar_other(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn xlogyScalarOther_(
        self: *Tensor, other: Scalar
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_xlogy_scalar_other_(@ptrCast(&c_tensors), self.c_tensor,
                other.into().c_scalar);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn xlogyScalarSelf(
        self_scalar: Scalar, other: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_xlogy_scalar_self(@ptrCast(&c_tensors), self_scalar.into().c_scalar,
                other.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn zero(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_zero(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn zero_(
        self: *Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_zero_(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn zeroOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_zero_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn zeros(
        size_: []i64, options_: TensorOptions
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_zeros(@ptrCast(&c_tensors), size_.ptr, @intCast(size_.len),
                options_.kind.cInt(), options_.device.cInt());
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn zerosLike(
        self: *const Tensor, 
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_zeros_like(@ptrCast(&c_tensors), self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn zerosLikeOut(
        self: *const Tensor, out: *const Tensor
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_zeros_like_out(@ptrCast(&c_tensors), out.c_tensor,
                self.c_tensor);
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }

    pub fn zerosOut(
        out: *const Tensor, size_: []i64
    ) Tensor {
        var c_tensors = [_]C_tensor{null} ** 1;
        __c.atg_zeros_out(@ptrCast(&c_tensors), out.c_tensor,
                size_.ptr, @intCast(size_.len));
        torch.readAndCleanError();
        return Tensor { .c_tensor = c_tensors[0] };
    }
};
